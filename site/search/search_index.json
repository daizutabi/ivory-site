{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ivory Overview Ivory is a lightweight machine learning framework. Installation You can install Ivory from PyPI. $ pip install ivory","title":"Ivory"},{"location":"#ivory","text":"","title":"Ivory"},{"location":"#overview","text":"Ivory is a lightweight machine learning framework.","title":"Overview"},{"location":"#installation","text":"You can install Ivory from PyPI. $ pip install ivory","title":"Installation"},{"location":"tutorial/data/","text":"Data Create data As a simple toy example, we try to predict area of rectangles which have width and height , but they include some noises. First, define a function to create such data. import numpy as np import pandas as pd import ivory from ivory.utils import kfold_split def create_data(num_samples=1000): \"\"\"Returns a tuple of (input, target). Target has fold information.\"\"\" x = 4 * np.random.rand(num_samples, 2) + 1 x = x.astype(np.float32) noises = 0.1 * (np.random.rand(2, num_samples) - 0.5) df = pd.DataFrame(x, columns=[\"width\", \"height\"]) df[\"area\"] = (df.width + noises[0]) * (df.height + noises[1]) df.area = df.area.astype(np.float32) df[\"fold\"] = kfold_split(df.index, n_splits=5) return df[[\"width\", \"height\"]], df[[\"fold\", \"area\"]] [1] 2020-03-12 13:08:20 ( 6.00ms ) python3 ( 1.71s ) kfold_split function creates a fold-array. kfold_split(np.arange(10), n_splits=3) [2] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.72s ) array([2, 1, 0, 2, 0, 2, 1, 1, 0, 0], dtype=int8) To execute create_data funtion, you can use a dictionary as well as a YAML file. params = {'data': {'def': 'create_data'}} [3] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.72s ) The key of def means that the value is a funtion instead of a class. Apply this dictionary to ivory.instantiate function to get an instantiated object dictionary. data = ivory.instantiate(params)['data'] data[0].head() [4] 2020-03-12 13:08:20 ( 12.0ms ) python3 ( 1.73s ) width height 0 3.236384 1.675367 1 1.819559 1.545507 2 4.261151 4.065137 3 1.163016 2.549294 4 3.516565 1.181266 data[1].head() [5] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.74s ) fold area 0 2 5.401631 1 0 2.958865 2 1 17.326916 3 2 3.027853 4 3 3.998460 You can change the data size with additional key-value pairs (in this case, the minimun size is the number of fold 5). params = {'data': {'def': 'create_data', 'num_samples': 5}} ivory.instantiate(params)['data'][1] [6] 2020-03-12 13:08:20 ( 10.0ms ) python3 ( 1.75s ) fold area 0 1 6.070549 1 2 9.543874 2 0 6.234706 3 3 3.258180 4 4 9.950698 Dataset Ivory provides ivory.torch.Dataset for PyTorch. from ivory.torch import Dataset dataset = Dataset(input=data[0], target=data[1]) dataset [7] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.75s ) Dataset(num_samples=1000, input_shape=(2,), target_shape=(2,), mode='train') Ivory's Dataset is a subclass of PyTorch's Dataset import torch.utils.data isinstance(dataset, torch.utils.data.Dataset) [8] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.76s ) True Check an item. dataset[0] [9] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.76s ) (0, array([3.236384 , 1.6753671], dtype=float32), array([2. , 5.4016314], dtype=float32)) Indexing returns a tuple. The first is an index, the second is an input, and the last is a target. The target includes fold which is not a real target. But, it's okay because we don't use a Dataset directly. We can use more useful DataLoaders provided by Ivory. DataLoaders DataLoaders provides a data loader for both training and validation. This is the reason why our data include fold. from ivory.torch import DataLoaders dataloaders = DataLoaders(input=data[0], target=data[1], batch_size=3) dataloaders [10] 2020-03-12 13:08:20 ( 6.00ms ) python3 ( 1.77s ) DataLoaders(num_folds=5, num_samples=1000, input_shape=(2,), target_shape=(1,)) DataLoaders instance can detect the number of fold and remove the fold information from the target. You can get a pair of train and validation data loaders at once by indexing like a list . train_loader, val_loader = dataloaders[0] # for fold-0. [11] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.77s ) Here, the index is corresponding to a fold, in this case, ranging from 0 to 4 because the number of K-fold is 5. Check the data loader. train_loader [12] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.77s ) <torch.utils.data.dataloader.DataLoader at 0x196ed6d7ac8> The data loader is a pure PyTorch's DataLoader . Let's see the dataset that the data loader has. train_loader.dataset [13] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.78s ) Dataset(num_samples=800, input_shape=(2,), target_shape=(1,), mode='train') This is our Ivory's dataset ( ivory.torch.Dataset ). The number of samples decreases from 1000 to 800 because we use 5-fold splitting (80% reduction). Validation dataset shoud have the rest 20% samples. val_loader.dataset [14] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.78s ) Dataset(num_samples=200, input_shape=(2,), target_shape=(1,), mode='val') Now check iteration. next(iter(train_loader)) [15] 2020-03-12 13:08:20 ( 5.01ms ) python3 ( 1.79s ) [tensor([333, 198, 851]), tensor([[2.2426, 2.8620], [4.9297, 2.1651], [1.1057, 4.7007]]), tensor([[ 6.5396], [10.5526], [ 5.1436]])] next() returns a list of torch.Tensor . The first is an index, the second is an input, and the last is a target. Note that the target doesn't include fold any more. In default setting, train data loader shuffles its data. Validation data loader doesn't: next(iter(val_loader)) [16] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.79s ) [tensor([1, 5, 8]), tensor([[1.8196, 1.5455], [3.1627, 4.0857], [4.2921, 4.7499]]), tensor([[ 2.9589], [12.9208], [20.6691]])] DataLoaders via a YAML file As well as data, you can create your DataLoaders from a YAML file. File params_1a.yaml data: def: create_data num_samples: 60 dataloaders: class: ivory.torch.DataLoaders input: $.data.0 target: $.data.1 batch_size: 10 To create a DataLoaders , you need to give input and target to the DataLoaders initializer. Unlike a simple parameter ( num_samples or batch_size in this case), these can't be written in a YAML file directly. Instead, you can assign them using \" $ -notation \". In a YAML file, a value that starts with ' $. ' means an instance. In additon, if the value ends with ' .(digit) ', it is an element taken from a sequence by indexing. In the above case, ' $.data.0 ' is the first element of a tuple data created by the create_data function. Forthermote, you can use more direct form: inline unpacking . File params_1b.yaml input, target: def: create_data num_samples: 60 dataloaders: class: ivory.torch.DataLoaders input: $ target: $ batch_size: 10 Now, the output of create_data is unpacked to input and target . In $-notation, '(dot)+(instance name)' can be omitted if the key name is equal to an instance name. Next, instantiate them after loading the YAML file: import yaml with open('params_1b.yaml') as f: yml = yaml.safe_load(f) yml [17] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.80s ) {'input, target': {'def': 'create_data', 'num_samples': 60}, 'dataloaders': {'class': 'ivory.torch.DataLoaders', 'input': '$', 'target': '$', 'batch_size': 10}} params = ivory.instantiate(yml) params.keys() [18] 2020-03-12 13:08:20 ( 8.00ms ) python3 ( 1.80s ) dict_keys(['input', 'target', 'dataloaders']) params['input'].shape, params['target'].shape [19] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.81s ) ((60, 2), (60, 2)) params['dataloaders'] [20] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.81s ) DataLoaders(num_folds=5, num_samples=60, input_shape=(2,), target_shape=(1,)) Everythings works well!. Notice that the global name space of Python isn't affected by these instantiatitions. try: target except NameError: print(\"`target` doesn't exsist.\") [21] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.81s ) `target` doesn't exsist. In the next section, we will introduce model/optimizer/schduler combination for this dataset.","title":"Data"},{"location":"tutorial/data/#data","text":"","title":"Data"},{"location":"tutorial/data/#create-data","text":"As a simple toy example, we try to predict area of rectangles which have width and height , but they include some noises. First, define a function to create such data. import numpy as np import pandas as pd import ivory from ivory.utils import kfold_split def create_data(num_samples=1000): \"\"\"Returns a tuple of (input, target). Target has fold information.\"\"\" x = 4 * np.random.rand(num_samples, 2) + 1 x = x.astype(np.float32) noises = 0.1 * (np.random.rand(2, num_samples) - 0.5) df = pd.DataFrame(x, columns=[\"width\", \"height\"]) df[\"area\"] = (df.width + noises[0]) * (df.height + noises[1]) df.area = df.area.astype(np.float32) df[\"fold\"] = kfold_split(df.index, n_splits=5) return df[[\"width\", \"height\"]], df[[\"fold\", \"area\"]] [1] 2020-03-12 13:08:20 ( 6.00ms ) python3 ( 1.71s ) kfold_split function creates a fold-array. kfold_split(np.arange(10), n_splits=3) [2] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.72s ) array([2, 1, 0, 2, 0, 2, 1, 1, 0, 0], dtype=int8) To execute create_data funtion, you can use a dictionary as well as a YAML file. params = {'data': {'def': 'create_data'}} [3] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.72s ) The key of def means that the value is a funtion instead of a class. Apply this dictionary to ivory.instantiate function to get an instantiated object dictionary. data = ivory.instantiate(params)['data'] data[0].head() [4] 2020-03-12 13:08:20 ( 12.0ms ) python3 ( 1.73s ) width height 0 3.236384 1.675367 1 1.819559 1.545507 2 4.261151 4.065137 3 1.163016 2.549294 4 3.516565 1.181266 data[1].head() [5] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.74s ) fold area 0 2 5.401631 1 0 2.958865 2 1 17.326916 3 2 3.027853 4 3 3.998460 You can change the data size with additional key-value pairs (in this case, the minimun size is the number of fold 5). params = {'data': {'def': 'create_data', 'num_samples': 5}} ivory.instantiate(params)['data'][1] [6] 2020-03-12 13:08:20 ( 10.0ms ) python3 ( 1.75s ) fold area 0 1 6.070549 1 2 9.543874 2 0 6.234706 3 3 3.258180 4 4 9.950698","title":"Create data"},{"location":"tutorial/data/#dataset","text":"Ivory provides ivory.torch.Dataset for PyTorch. from ivory.torch import Dataset dataset = Dataset(input=data[0], target=data[1]) dataset [7] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.75s ) Dataset(num_samples=1000, input_shape=(2,), target_shape=(2,), mode='train') Ivory's Dataset is a subclass of PyTorch's Dataset import torch.utils.data isinstance(dataset, torch.utils.data.Dataset) [8] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.76s ) True Check an item. dataset[0] [9] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.76s ) (0, array([3.236384 , 1.6753671], dtype=float32), array([2. , 5.4016314], dtype=float32)) Indexing returns a tuple. The first is an index, the second is an input, and the last is a target. The target includes fold which is not a real target. But, it's okay because we don't use a Dataset directly. We can use more useful DataLoaders provided by Ivory.","title":"Dataset"},{"location":"tutorial/data/#dataloaders","text":"DataLoaders provides a data loader for both training and validation. This is the reason why our data include fold. from ivory.torch import DataLoaders dataloaders = DataLoaders(input=data[0], target=data[1], batch_size=3) dataloaders [10] 2020-03-12 13:08:20 ( 6.00ms ) python3 ( 1.77s ) DataLoaders(num_folds=5, num_samples=1000, input_shape=(2,), target_shape=(1,)) DataLoaders instance can detect the number of fold and remove the fold information from the target. You can get a pair of train and validation data loaders at once by indexing like a list . train_loader, val_loader = dataloaders[0] # for fold-0. [11] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.77s ) Here, the index is corresponding to a fold, in this case, ranging from 0 to 4 because the number of K-fold is 5. Check the data loader. train_loader [12] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.77s ) <torch.utils.data.dataloader.DataLoader at 0x196ed6d7ac8> The data loader is a pure PyTorch's DataLoader . Let's see the dataset that the data loader has. train_loader.dataset [13] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.78s ) Dataset(num_samples=800, input_shape=(2,), target_shape=(1,), mode='train') This is our Ivory's dataset ( ivory.torch.Dataset ). The number of samples decreases from 1000 to 800 because we use 5-fold splitting (80% reduction). Validation dataset shoud have the rest 20% samples. val_loader.dataset [14] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.78s ) Dataset(num_samples=200, input_shape=(2,), target_shape=(1,), mode='val') Now check iteration. next(iter(train_loader)) [15] 2020-03-12 13:08:20 ( 5.01ms ) python3 ( 1.79s ) [tensor([333, 198, 851]), tensor([[2.2426, 2.8620], [4.9297, 2.1651], [1.1057, 4.7007]]), tensor([[ 6.5396], [10.5526], [ 5.1436]])] next() returns a list of torch.Tensor . The first is an index, the second is an input, and the last is a target. Note that the target doesn't include fold any more. In default setting, train data loader shuffles its data. Validation data loader doesn't: next(iter(val_loader)) [16] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.79s ) [tensor([1, 5, 8]), tensor([[1.8196, 1.5455], [3.1627, 4.0857], [4.2921, 4.7499]]), tensor([[ 2.9589], [12.9208], [20.6691]])]","title":"DataLoaders"},{"location":"tutorial/data/#dataloaders-via-a-yaml-file","text":"As well as data, you can create your DataLoaders from a YAML file. File params_1a.yaml data: def: create_data num_samples: 60 dataloaders: class: ivory.torch.DataLoaders input: $.data.0 target: $.data.1 batch_size: 10 To create a DataLoaders , you need to give input and target to the DataLoaders initializer. Unlike a simple parameter ( num_samples or batch_size in this case), these can't be written in a YAML file directly. Instead, you can assign them using \" $ -notation \". In a YAML file, a value that starts with ' $. ' means an instance. In additon, if the value ends with ' .(digit) ', it is an element taken from a sequence by indexing. In the above case, ' $.data.0 ' is the first element of a tuple data created by the create_data function. Forthermote, you can use more direct form: inline unpacking . File params_1b.yaml input, target: def: create_data num_samples: 60 dataloaders: class: ivory.torch.DataLoaders input: $ target: $ batch_size: 10 Now, the output of create_data is unpacked to input and target . In $-notation, '(dot)+(instance name)' can be omitted if the key name is equal to an instance name. Next, instantiate them after loading the YAML file: import yaml with open('params_1b.yaml') as f: yml = yaml.safe_load(f) yml [17] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.80s ) {'input, target': {'def': 'create_data', 'num_samples': 60}, 'dataloaders': {'class': 'ivory.torch.DataLoaders', 'input': '$', 'target': '$', 'batch_size': 10}} params = ivory.instantiate(yml) params.keys() [18] 2020-03-12 13:08:20 ( 8.00ms ) python3 ( 1.80s ) dict_keys(['input', 'target', 'dataloaders']) params['input'].shape, params['target'].shape [19] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.81s ) ((60, 2), (60, 2)) params['dataloaders'] [20] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.81s ) DataLoaders(num_folds=5, num_samples=60, input_shape=(2,), target_shape=(1,)) Everythings works well!. Notice that the global name space of Python isn't affected by these instantiatitions. try: target except NameError: print(\"`target` doesn't exsist.\") [21] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.81s ) `target` doesn't exsist. In the next section, we will introduce model/optimizer/schduler combination for this dataset.","title":"DataLoaders via a YAML file"},{"location":"tutorial/experiment/","text":"Experiment The first start point in Ivory is ivory.Experiment . This master object controls a machine learning experiment under a certain condition. Let's create your first Experiment instance. In your favorite directory, write a YAML file like below named params_0.yaml : File params_0.yaml experiment: class: ivory.Experiment The class key notices that the experiment is an instance of ivory.Experiment . Then, in a jupyte notebook or a Python script under the same directory: import ivory experiment = ivory.create_experiment('params_0.yaml') experiment [1] 2020-03-12 13:08:18 ( 1.48s ) python3 ( 1.49s ) Experiment(name='ready', run_class='ivory.core.Run', shared=[], num_runs=0) You created an Experiment instance. It says that its name is \"ready\" and its run_class is ivory.core.Run . Next, you can start the Experiment . experiment.start() experiment [2] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.50s ) Experiment(name='2020/03/12 13:08:20', run_class='ivory.core.Run', shared=[], num_runs=0) The name was changed to the current time. An Experiment instance is like an environment. Actual interesting processes such as machine learning are done by Run s. So, create a Run . run = experiment.create_run() run [3] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.50s ) Run(name='#1', callbacks=[]) This Run instance was named '#1' because this is the first one for the Experiment instance. Do you want to start this run, too? Let's try it. run.start() [4] 2020-03-12 13:08:20 ( 107ms ) python3 ( 1.61s ) AttributeError: 'Run' object has no attribute 'trainer' AttributeError Traceback (most recent call last) <ipython-input-7-c930d12f52e8> in <module> ----> 1 run.start() ~\\Documents\\GitHub\\ivory\\ivory\\core\\run.py in start(self) 37 self.on_fit_start() 38 try: ---> 39 self.trainer.fit(self) 40 finally: 41 self.on_fit_end() Oops! The Run instance has the start method indeed, but it says there is no trainer . Of course, we need some data, a model, metrics, etc . for a particular machine learning problem. Also, we need to decide how to train the model. Following sections will explain this process step by step.","title":"Experiment"},{"location":"tutorial/experiment/#experiment","text":"The first start point in Ivory is ivory.Experiment . This master object controls a machine learning experiment under a certain condition. Let's create your first Experiment instance. In your favorite directory, write a YAML file like below named params_0.yaml : File params_0.yaml experiment: class: ivory.Experiment The class key notices that the experiment is an instance of ivory.Experiment . Then, in a jupyte notebook or a Python script under the same directory: import ivory experiment = ivory.create_experiment('params_0.yaml') experiment [1] 2020-03-12 13:08:18 ( 1.48s ) python3 ( 1.49s ) Experiment(name='ready', run_class='ivory.core.Run', shared=[], num_runs=0) You created an Experiment instance. It says that its name is \"ready\" and its run_class is ivory.core.Run . Next, you can start the Experiment . experiment.start() experiment [2] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.50s ) Experiment(name='2020/03/12 13:08:20', run_class='ivory.core.Run', shared=[], num_runs=0) The name was changed to the current time. An Experiment instance is like an environment. Actual interesting processes such as machine learning are done by Run s. So, create a Run . run = experiment.create_run() run [3] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.50s ) Run(name='#1', callbacks=[]) This Run instance was named '#1' because this is the first one for the Experiment instance. Do you want to start this run, too? Let's try it. run.start() [4] 2020-03-12 13:08:20 ( 107ms ) python3 ( 1.61s ) AttributeError: 'Run' object has no attribute 'trainer' AttributeError Traceback (most recent call last) <ipython-input-7-c930d12f52e8> in <module> ----> 1 run.start() ~\\Documents\\GitHub\\ivory\\ivory\\core\\run.py in start(self) 37 self.on_fit_start() 38 try: ---> 39 self.trainer.fit(self) 40 finally: 41 self.on_fit_end() Oops! The Run instance has the start method indeed, but it says there is no trainer . Of course, we need some data, a model, metrics, etc . for a particular machine learning problem. Also, we need to decide how to train the model. Following sections will explain this process step by step.","title":"Experiment"},{"location":"tutorial/metrics/","text":"Metrics Ivory has a ivory.callbacks.Metrics that doesn't depend on any specific library such as PyTorch or scikit-learn, etc. As the module name shows, a Metrics is a callback called from a Trainer , which is created by a Run . So at this stage, we cannot get an instance of Metrics . Instead, let's check the ivory.torch.Metrics that is a metrics class for PyTorch. A instance of this class will be called from ivory.torch.Trainer . File ivory/torch/metrics.py from typing import Any, Dict, Tuple from torch import Tensor import ivory.callbacks from ivory.torch.utils import cpu class Metrics(ivory.callbacks.Metrics): def evaluate(self, loss, output, target) -> Dict[str, float]: return {\"loss\": loss.item()} def train_evaluate(self, output, target) -> Tuple[Tensor, Dict[str, float]]: loss = self.criterion(output, target) output = output.detach() return loss, self.evaluate(loss, output, target) def val_evaluate(self, output, target) -> Tuple[Any, Dict[str, float]]: loss = self.criterion(output, target) output = output.detach() record = self.evaluate(loss, output, target) if output.device.type != \"cpu\": output = cpu(output) return output, record def on_current_record(self, run): self.current_record[\"lr\"] = run.optimizer.param_groups[0][\"lr\"] There four instance methods. train_evaluate and val_evaluate are functions that called from train and validation loops. evalute is a customizable function for metrics at a step. You can overwrite this function, for example: from typing import Dict import torch import ivory.torch class MyMetrics(ivory.torch.Metrics): def evaluate(self, loss, output, target) -> Dict[str, float]: mse = torch.mean((output - target) ** 2).item() return {\"loss\": loss.item(), \"mse\": mse} # Here, loss == mse [1] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.98s ) The last method on_current_record is called from on_epoch_end callback function. This gives you a chance to modify a current metrics for epoch (we call it a record .) In the above case, we add the learning rate of our optimizer in order to monitor it during a traing loop. A callback function of Ivoy takes one argument run which is corresponding to the current run. You can access all of the run information via this run instance.","title":"Metrics"},{"location":"tutorial/metrics/#metrics","text":"Ivory has a ivory.callbacks.Metrics that doesn't depend on any specific library such as PyTorch or scikit-learn, etc. As the module name shows, a Metrics is a callback called from a Trainer , which is created by a Run . So at this stage, we cannot get an instance of Metrics . Instead, let's check the ivory.torch.Metrics that is a metrics class for PyTorch. A instance of this class will be called from ivory.torch.Trainer . File ivory/torch/metrics.py from typing import Any, Dict, Tuple from torch import Tensor import ivory.callbacks from ivory.torch.utils import cpu class Metrics(ivory.callbacks.Metrics): def evaluate(self, loss, output, target) -> Dict[str, float]: return {\"loss\": loss.item()} def train_evaluate(self, output, target) -> Tuple[Tensor, Dict[str, float]]: loss = self.criterion(output, target) output = output.detach() return loss, self.evaluate(loss, output, target) def val_evaluate(self, output, target) -> Tuple[Any, Dict[str, float]]: loss = self.criterion(output, target) output = output.detach() record = self.evaluate(loss, output, target) if output.device.type != \"cpu\": output = cpu(output) return output, record def on_current_record(self, run): self.current_record[\"lr\"] = run.optimizer.param_groups[0][\"lr\"] There four instance methods. train_evaluate and val_evaluate are functions that called from train and validation loops. evalute is a customizable function for metrics at a step. You can overwrite this function, for example: from typing import Dict import torch import ivory.torch class MyMetrics(ivory.torch.Metrics): def evaluate(self, loss, output, target) -> Dict[str, float]: mse = torch.mean((output - target) ** 2).item() return {\"loss\": loss.item(), \"mse\": mse} # Here, loss == mse [1] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.98s ) The last method on_current_record is called from on_epoch_end callback function. This gives you a chance to modify a current metrics for epoch (we call it a record .) In the above case, we add the learning rate of our optimizer in order to monitor it during a traing loop. A callback function of Ivoy takes one argument run which is corresponding to the current run. You can access all of the run information via this run instance.","title":"Metrics"},{"location":"tutorial/model/","text":"Model We will train a model to predict a toy rectangle area problem. First, define a simple model with hidden_sizes as a hyper parameter (list of integers). import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] it = zip([2] + hidden_sizes, hidden_sizes + [1]) for in_features, out_features in it: layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) Model([3, 4]) # Just example [1] 2020-03-12 13:08:20 ( 7.00ms ) python3 ( 1.83s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=3, bias=True) (1): Linear(in_features=3, out_features=4, bias=True) (2): Linear(in_features=4, out_features=1, bias=True) ) ) To train this model, we need an optimizer and an optional schduler. Write them in a YAML file. File params_2.yaml input, target: def: create_data num_samples: 10000 dataloaders: class: ivory.torch.DataLoaders input: $ target: $ batch_size: 32 model: class: Model hidden_sizes: [10, 10] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.1 patience: 4 An PyTorch optimizer needs model's parameters as the first argument. We can give them by $-notation with attribute accessor ( $.model.parameters() ). The rest part of this YAML file is straight forward. Now, we have other hyper parameters such as lr (learning rate), factor , or patience . All of these hyper parameters are written in one YAML file. This allows us to manage them easily. Again, let's create instances: import yaml from ivory.utils import to_float with open('params_2.yaml') as f: yml = to_float(yaml.safe_load(f)) params = ivory.instantiate(yml) params.keys() [2] 2020-03-12 13:08:20 ( 13.0ms ) python3 ( 1.84s ) dict_keys(['input', 'target', 'dataloaders', 'model', 'optimizer', 'scheduler']) Here, a helper function ivory.utils.to_float converts an exponential expression such as \"1e-3\" to \"0.001\". Check the created instances params['model'] [3] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.84s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=10, bias=True) (1): Linear(in_features=10, out_features=10, bias=True) (2): Linear(in_features=10, out_features=1, bias=True) ) ) params['optimizer'] [4] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.85s ) SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) params['scheduler'] [5] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.85s ) <torch.optim.lr_scheduler.ReduceLROnPlateau at 0x196ed6f1948> Now, we have data and a model with an optimizer so we can start to train . dataloader, _ = params['dataloaders'][0] index, input_, target = next(iter(dataloader)) input_[:4] [6] 2020-03-12 13:08:20 ( 7.00ms ) python3 ( 1.86s ) tensor([[1.8756, 3.9146], [3.2288, 3.3474], [2.0733, 3.9388], [2.5353, 2.0516]]) output = params['model'](input_) output[:4] [7] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.86s ) tensor([[0.3522], [0.3329], [0.3460], [0.3234]], grad_fn=<SliceBackward>) Calculate the first loss loss = F.mse_loss(output, target) loss [8] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.87s ) tensor(128.2645, grad_fn=<MseLossBackward>) Backpropagate and update the weights. params['optimizer'].zero_grad() loss.backward() params['optimizer'].step() [9] 2020-03-12 13:08:20 ( 95.0ms ) python3 ( 1.96s ) Calculate the second loss after the first optimization step. output = params['model'](input_) loss = F.mse_loss(output, target) loss [10] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.97s ) tensor(126.6161, grad_fn=<MseLossBackward>) To train a model, we need metrics to estimate the model performance. In the next section, we will introduce Metrics class for it.","title":"Model"},{"location":"tutorial/model/#model","text":"We will train a model to predict a toy rectangle area problem. First, define a simple model with hidden_sizes as a hyper parameter (list of integers). import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] it = zip([2] + hidden_sizes, hidden_sizes + [1]) for in_features, out_features in it: layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) Model([3, 4]) # Just example [1] 2020-03-12 13:08:20 ( 7.00ms ) python3 ( 1.83s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=3, bias=True) (1): Linear(in_features=3, out_features=4, bias=True) (2): Linear(in_features=4, out_features=1, bias=True) ) ) To train this model, we need an optimizer and an optional schduler. Write them in a YAML file. File params_2.yaml input, target: def: create_data num_samples: 10000 dataloaders: class: ivory.torch.DataLoaders input: $ target: $ batch_size: 32 model: class: Model hidden_sizes: [10, 10] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.1 patience: 4 An PyTorch optimizer needs model's parameters as the first argument. We can give them by $-notation with attribute accessor ( $.model.parameters() ). The rest part of this YAML file is straight forward. Now, we have other hyper parameters such as lr (learning rate), factor , or patience . All of these hyper parameters are written in one YAML file. This allows us to manage them easily. Again, let's create instances: import yaml from ivory.utils import to_float with open('params_2.yaml') as f: yml = to_float(yaml.safe_load(f)) params = ivory.instantiate(yml) params.keys() [2] 2020-03-12 13:08:20 ( 13.0ms ) python3 ( 1.84s ) dict_keys(['input', 'target', 'dataloaders', 'model', 'optimizer', 'scheduler']) Here, a helper function ivory.utils.to_float converts an exponential expression such as \"1e-3\" to \"0.001\". Check the created instances params['model'] [3] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.84s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=10, bias=True) (1): Linear(in_features=10, out_features=10, bias=True) (2): Linear(in_features=10, out_features=1, bias=True) ) ) params['optimizer'] [4] 2020-03-12 13:08:20 ( 3.00ms ) python3 ( 1.85s ) SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) params['scheduler'] [5] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.85s ) <torch.optim.lr_scheduler.ReduceLROnPlateau at 0x196ed6f1948> Now, we have data and a model with an optimizer so we can start to train . dataloader, _ = params['dataloaders'][0] index, input_, target = next(iter(dataloader)) input_[:4] [6] 2020-03-12 13:08:20 ( 7.00ms ) python3 ( 1.86s ) tensor([[1.8756, 3.9146], [3.2288, 3.3474], [2.0733, 3.9388], [2.5353, 2.0516]]) output = params['model'](input_) output[:4] [7] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.86s ) tensor([[0.3522], [0.3329], [0.3460], [0.3234]], grad_fn=<SliceBackward>) Calculate the first loss loss = F.mse_loss(output, target) loss [8] 2020-03-12 13:08:20 ( 4.00ms ) python3 ( 1.87s ) tensor(128.2645, grad_fn=<MseLossBackward>) Backpropagate and update the weights. params['optimizer'].zero_grad() loss.backward() params['optimizer'].step() [9] 2020-03-12 13:08:20 ( 95.0ms ) python3 ( 1.96s ) Calculate the second loss after the first optimization step. output = params['model'](input_) loss = F.mse_loss(output, target) loss [10] 2020-03-12 13:08:20 ( 5.00ms ) python3 ( 1.97s ) tensor(126.6161, grad_fn=<MseLossBackward>) To train a model, we need metrics to estimate the model performance. In the next section, we will introduce Metrics class for it.","title":"Model"},{"location":"tutorial/run/","text":"Run Now we reached the time to invoke a Run . The latest params.yaml is like below: File params_3.yaml input, target: def: create_data num_samples: 10000 dataloaders: class: ivory.torch.DataLoaders input: $ target: $ batch_size: 32 model: class: Model hidden_sizes: [10, 10] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.1 patience: 4 metrics: class: ivory.torch.Metrics criterion: torch.nn.functional.mse_loss trainer: class: ivory.torch.Trainer max_epochs: 10 verbose: 2 experiment: class: ivory.Experiment run_class: ivory.torch.Run shared: [input] Umm... run isn't defined anywhere. Because a run instance is created by an experiment instance dynamically, we don't need define a run instance in a parameters file. Instead of it, experiment has a field of run_class in order to determine which Run class sould be created. First, create an experiment instance. import ivory experiment = ivory.create_experiment('params_3.yaml') experiment [1] 2020-03-12 13:09:24 ( 1.28s ) python3 ( 1.55s ) Experiment(name='ready', run_class='ivory.torch.Run', shared=['input', 'target'], num_runs=0) You created an Experiment instance. It says that its name is \"ready\" and its run_class is ivory.torch.Run , now. In addtion, the shared field becomes ['input', 'target'] . The experiment can detect that target should be also shared as well as input if input is shared. create_data and Model are in global space so we need define here again. import numpy as np import pandas as pd import torch.nn as nn import torch.nn.functional as F import ivory from ivory.utils import kfold_split def create_data(num_samples=1000): \"\"\"Returns a tuple of (input, target). Target has fold information.\"\"\" x = 4 * np.random.rand(num_samples, 2) + 1 x = x.astype(np.float32) noises = 0.1 * (np.random.rand(2, num_samples) - 0.5) df = pd.DataFrame(x, columns=[\"width\", \"height\"]) df[\"area\"] = (df.width + noises[0]) * (df.height + noises[1]) df.area = df.area.astype(np.float32) df[\"fold\"] = kfold_split(df.index, n_splits=5) return df[[\"width\", \"height\"]], df[[\"fold\", \"area\"]] class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] it = zip([2] + hidden_sizes, hidden_sizes + [1]) for in_features, out_features in it: layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) [2] 2020-03-12 13:09:25 ( 8.00ms ) python3 ( 1.55s ) Next, you can start the Experiment . experiment.start() experiment [3] 2020-03-12 13:09:25 ( 12.0ms ) python3 ( 1.57s ) Experiment(name='2020/03/12 13:09:25', run_class='ivory.torch.Run', shared=['input', 'target'], num_runs=0) Shared ojbects are stored at default field. ( experiment itself is also shared.) experiment.default.keys() [4] 2020-03-12 13:09:25 ( 4.00ms ) python3 ( 1.57s ) dict_keys(['input', 'target', 'experiment']) Create a Run . run = experiment.create_run() run [5] 2020-03-12 13:09:25 ( 8.00ms ) python3 ( 1.58s ) Run(name='#1', callbacks=[]) Run has params field that defines the hyper parameters space. run.params [6] 2020-03-12 13:09:25 ( 4.00ms ) python3 ( 1.58s ) {'input, target': {'def': 'create_data', 'num_samples': 10000}, 'dataloaders': {'class': 'ivory.torch.DataLoaders', 'input': '$', 'target': '$', 'batch_size': 32}, 'model': {'class': 'Model', 'hidden_sizes': [10, 10]}, 'optimizer': {'class': 'torch.optim.SGD', 'params': '$.model.parameters()', 'lr': 0.001}, 'scheduler': {'class': 'torch.optim.lr_scheduler.ReduceLROnPlateau', 'optimizer': '$', 'factor': 0.1, 'patience': 4}, 'metrics': {'class': 'ivory.torch.Metrics', 'criterion': 'torch.nn.functional.mse_loss'}, 'trainer': {'class': 'ivory.torch.Trainer', 'max_epochs': 10, 'verbose': 2}, 'experiment': {'class': 'ivory.Experiment', 'run_class': 'ivory.torch.Run', 'shared': ['input']}} All the first-level key in this params dictionary are set as run s attributes. Because Run is iterable, you can scan theses attributes: list(run) [7] 2020-03-12 13:09:25 ( 3.00ms ) python3 ( 1.58s ) ['input', 'target', 'dataloaders', 'model', 'optimizer', 'scheduler', 'metrics', 'trainer', 'experiment'] Take a look at model . run.model [8] 2020-03-12 13:09:25 ( 3.00ms ) python3 ( 1.59s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=10, bias=True) (1): Linear(in_features=10, out_features=10, bias=True) (2): Linear(in_features=10, out_features=1, bias=True) ) ) If you give a new dictionary to create_run method, you can get a different run with different hyper parameters. run2 = experiment.create_run({\"model.hidden_sizes\": [3, 4, 5]}) run2.model [9] 2020-03-12 13:09:25 ( 8.00ms ) python3 ( 1.60s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=3, bias=True) (1): Linear(in_features=3, out_features=4, bias=True) (2): Linear(in_features=4, out_features=5, bias=True) (3): Linear(in_features=5, out_features=1, bias=True) ) ) Yes. The layer structure are changed. run2 is a new Run instance which is independent of the first run . But, input and target are shared. run.input is run2.input, run.target is run2.target [10] 2020-03-12 13:09:25 ( 3.00ms ) python3 ( 1.60s ) (True, True) This time run can start successfully. run.start() [11] 2020-03-12 13:09:25 ( 2.69s ) python3 ( 4.29s ) [#1] epoch=000 loss=1.75e+01 val_loss=8.38e+00 lr=1.00e-03 * [#1] epoch=001 loss=6.84e+00 val_loss=4.74e+00 lr=1.00e-03 * [#1] epoch=002 loss=3.14e+00 val_loss=1.66e+00 lr=1.00e-03 * [#1] epoch=003 loss=1.16e+00 val_loss=7.73e-01 lr=1.00e-03 * [#1] epoch=004 loss=6.67e-01 val_loss=5.58e-01 lr=1.00e-03 * [#1] epoch=005 loss=5.45e-01 val_loss=4.98e-01 lr=1.00e-03 * [#1] epoch=006 loss=4.91e-01 val_loss=4.53e-01 lr=1.00e-03 * [#1] epoch=007 loss=4.27e-01 val_loss=3.63e-01 lr=1.00e-03 * [#1] epoch=008 loss=3.39e-01 val_loss=2.97e-01 lr=1.00e-03 * [#1] epoch=009 loss=2.91e-01 val_loss=2.70e-01 lr=1.00e-03 * After a run, you can check the metrics. run.metrics.best_epoch, run.metrics.best_score [12] 2020-03-12 13:09:28 ( 5.00ms ) python3 ( 4.30s ) (9, 0.26952525240088265) history = run.metrics.history history [13] 2020-03-12 13:09:28 ( 10.0ms ) python3 ( 4.31s ) loss val_loss lr epoch 0 17.524214 8.381061 0.001 1 6.840580 4.743402 0.001 2 3.141508 1.662693 0.001 3 1.156988 0.772618 0.001 4 0.666561 0.557686 0.001 5 0.545232 0.497878 0.001 6 0.490753 0.452959 0.001 7 0.427134 0.362795 0.001 8 0.339459 0.296865 0.001 9 0.290814 0.269525 0.001 import matplotlib.pyplot as plt plt.plot(history.index, history.loss, marker=\"o\", label=\"loss\") plt.plot(history.index, history.val_loss, marker=\"s\", label=\"val_loss\") plt.legend() plt.yscale(\"log\") [14] 2020-03-12 13:09:28 ( 443ms ) python3 ( 4.75s ) Best validation output of model is also stored. run.metrics.best_output.head() [15] 2020-03-12 13:09:28 ( 6.00ms ) python3 ( 4.75s ) output 9 16.024521 15 20.114882 18 11.211087 23 12.289846 31 1.775653 Here. the index is corresponding to that of the target. print(len(run.target)) run.target.head() [16] 2020-03-12 13:09:28 ( 6.00ms ) python3 ( 4.76s ) 10000 fold area 0 4 4.958039 1 2 2.230924 2 3 11.818921 3 3 1.877944 4 2 5.436651 df = run.target.join(run.metrics.best_output, how='inner') df.fold.unique() [17] 2020-03-12 13:09:28 ( 5.00ms ) python3 ( 4.77s ) array([0], dtype=int8) plt.scatter(df.area, df.output) [18] 2020-03-12 13:09:28 ( 88.0ms ) python3 ( 4.85s ) <matplotlib.collections.PathCollection at 0x19dea6e2508>","title":"Run"},{"location":"tutorial/run/#run","text":"Now we reached the time to invoke a Run . The latest params.yaml is like below: File params_3.yaml input, target: def: create_data num_samples: 10000 dataloaders: class: ivory.torch.DataLoaders input: $ target: $ batch_size: 32 model: class: Model hidden_sizes: [10, 10] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.1 patience: 4 metrics: class: ivory.torch.Metrics criterion: torch.nn.functional.mse_loss trainer: class: ivory.torch.Trainer max_epochs: 10 verbose: 2 experiment: class: ivory.Experiment run_class: ivory.torch.Run shared: [input] Umm... run isn't defined anywhere. Because a run instance is created by an experiment instance dynamically, we don't need define a run instance in a parameters file. Instead of it, experiment has a field of run_class in order to determine which Run class sould be created. First, create an experiment instance. import ivory experiment = ivory.create_experiment('params_3.yaml') experiment [1] 2020-03-12 13:09:24 ( 1.28s ) python3 ( 1.55s ) Experiment(name='ready', run_class='ivory.torch.Run', shared=['input', 'target'], num_runs=0) You created an Experiment instance. It says that its name is \"ready\" and its run_class is ivory.torch.Run , now. In addtion, the shared field becomes ['input', 'target'] . The experiment can detect that target should be also shared as well as input if input is shared. create_data and Model are in global space so we need define here again. import numpy as np import pandas as pd import torch.nn as nn import torch.nn.functional as F import ivory from ivory.utils import kfold_split def create_data(num_samples=1000): \"\"\"Returns a tuple of (input, target). Target has fold information.\"\"\" x = 4 * np.random.rand(num_samples, 2) + 1 x = x.astype(np.float32) noises = 0.1 * (np.random.rand(2, num_samples) - 0.5) df = pd.DataFrame(x, columns=[\"width\", \"height\"]) df[\"area\"] = (df.width + noises[0]) * (df.height + noises[1]) df.area = df.area.astype(np.float32) df[\"fold\"] = kfold_split(df.index, n_splits=5) return df[[\"width\", \"height\"]], df[[\"fold\", \"area\"]] class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] it = zip([2] + hidden_sizes, hidden_sizes + [1]) for in_features, out_features in it: layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) [2] 2020-03-12 13:09:25 ( 8.00ms ) python3 ( 1.55s ) Next, you can start the Experiment . experiment.start() experiment [3] 2020-03-12 13:09:25 ( 12.0ms ) python3 ( 1.57s ) Experiment(name='2020/03/12 13:09:25', run_class='ivory.torch.Run', shared=['input', 'target'], num_runs=0) Shared ojbects are stored at default field. ( experiment itself is also shared.) experiment.default.keys() [4] 2020-03-12 13:09:25 ( 4.00ms ) python3 ( 1.57s ) dict_keys(['input', 'target', 'experiment']) Create a Run . run = experiment.create_run() run [5] 2020-03-12 13:09:25 ( 8.00ms ) python3 ( 1.58s ) Run(name='#1', callbacks=[]) Run has params field that defines the hyper parameters space. run.params [6] 2020-03-12 13:09:25 ( 4.00ms ) python3 ( 1.58s ) {'input, target': {'def': 'create_data', 'num_samples': 10000}, 'dataloaders': {'class': 'ivory.torch.DataLoaders', 'input': '$', 'target': '$', 'batch_size': 32}, 'model': {'class': 'Model', 'hidden_sizes': [10, 10]}, 'optimizer': {'class': 'torch.optim.SGD', 'params': '$.model.parameters()', 'lr': 0.001}, 'scheduler': {'class': 'torch.optim.lr_scheduler.ReduceLROnPlateau', 'optimizer': '$', 'factor': 0.1, 'patience': 4}, 'metrics': {'class': 'ivory.torch.Metrics', 'criterion': 'torch.nn.functional.mse_loss'}, 'trainer': {'class': 'ivory.torch.Trainer', 'max_epochs': 10, 'verbose': 2}, 'experiment': {'class': 'ivory.Experiment', 'run_class': 'ivory.torch.Run', 'shared': ['input']}} All the first-level key in this params dictionary are set as run s attributes. Because Run is iterable, you can scan theses attributes: list(run) [7] 2020-03-12 13:09:25 ( 3.00ms ) python3 ( 1.58s ) ['input', 'target', 'dataloaders', 'model', 'optimizer', 'scheduler', 'metrics', 'trainer', 'experiment'] Take a look at model . run.model [8] 2020-03-12 13:09:25 ( 3.00ms ) python3 ( 1.59s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=10, bias=True) (1): Linear(in_features=10, out_features=10, bias=True) (2): Linear(in_features=10, out_features=1, bias=True) ) ) If you give a new dictionary to create_run method, you can get a different run with different hyper parameters. run2 = experiment.create_run({\"model.hidden_sizes\": [3, 4, 5]}) run2.model [9] 2020-03-12 13:09:25 ( 8.00ms ) python3 ( 1.60s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=3, bias=True) (1): Linear(in_features=3, out_features=4, bias=True) (2): Linear(in_features=4, out_features=5, bias=True) (3): Linear(in_features=5, out_features=1, bias=True) ) ) Yes. The layer structure are changed. run2 is a new Run instance which is independent of the first run . But, input and target are shared. run.input is run2.input, run.target is run2.target [10] 2020-03-12 13:09:25 ( 3.00ms ) python3 ( 1.60s ) (True, True) This time run can start successfully. run.start() [11] 2020-03-12 13:09:25 ( 2.69s ) python3 ( 4.29s ) [#1] epoch=000 loss=1.75e+01 val_loss=8.38e+00 lr=1.00e-03 * [#1] epoch=001 loss=6.84e+00 val_loss=4.74e+00 lr=1.00e-03 * [#1] epoch=002 loss=3.14e+00 val_loss=1.66e+00 lr=1.00e-03 * [#1] epoch=003 loss=1.16e+00 val_loss=7.73e-01 lr=1.00e-03 * [#1] epoch=004 loss=6.67e-01 val_loss=5.58e-01 lr=1.00e-03 * [#1] epoch=005 loss=5.45e-01 val_loss=4.98e-01 lr=1.00e-03 * [#1] epoch=006 loss=4.91e-01 val_loss=4.53e-01 lr=1.00e-03 * [#1] epoch=007 loss=4.27e-01 val_loss=3.63e-01 lr=1.00e-03 * [#1] epoch=008 loss=3.39e-01 val_loss=2.97e-01 lr=1.00e-03 * [#1] epoch=009 loss=2.91e-01 val_loss=2.70e-01 lr=1.00e-03 * After a run, you can check the metrics. run.metrics.best_epoch, run.metrics.best_score [12] 2020-03-12 13:09:28 ( 5.00ms ) python3 ( 4.30s ) (9, 0.26952525240088265) history = run.metrics.history history [13] 2020-03-12 13:09:28 ( 10.0ms ) python3 ( 4.31s ) loss val_loss lr epoch 0 17.524214 8.381061 0.001 1 6.840580 4.743402 0.001 2 3.141508 1.662693 0.001 3 1.156988 0.772618 0.001 4 0.666561 0.557686 0.001 5 0.545232 0.497878 0.001 6 0.490753 0.452959 0.001 7 0.427134 0.362795 0.001 8 0.339459 0.296865 0.001 9 0.290814 0.269525 0.001 import matplotlib.pyplot as plt plt.plot(history.index, history.loss, marker=\"o\", label=\"loss\") plt.plot(history.index, history.val_loss, marker=\"s\", label=\"val_loss\") plt.legend() plt.yscale(\"log\") [14] 2020-03-12 13:09:28 ( 443ms ) python3 ( 4.75s ) Best validation output of model is also stored. run.metrics.best_output.head() [15] 2020-03-12 13:09:28 ( 6.00ms ) python3 ( 4.75s ) output 9 16.024521 15 20.114882 18 11.211087 23 12.289846 31 1.775653 Here. the index is corresponding to that of the target. print(len(run.target)) run.target.head() [16] 2020-03-12 13:09:28 ( 6.00ms ) python3 ( 4.76s ) 10000 fold area 0 4 4.958039 1 2 2.230924 2 3 11.818921 3 3 1.877944 4 2 5.436651 df = run.target.join(run.metrics.best_output, how='inner') df.fold.unique() [17] 2020-03-12 13:09:28 ( 5.00ms ) python3 ( 4.77s ) array([0], dtype=int8) plt.scatter(df.area, df.output) [18] 2020-03-12 13:09:28 ( 88.0ms ) python3 ( 4.85s ) <matplotlib.collections.PathCollection at 0x19dea6e2508>","title":"Run"},{"location":"tutorial/trainer/","text":"Trainer Ivory's ivory.torch.Trainer instance manages train and validation loop. At this stage, just take a look at a simple version of the code. from dataclasses import dataclass import torch from torch.optim.lr_scheduler import ReduceLROnPlateau @dataclass class Trainer: fold: int = 0 epoch: int = -1 global_step: int = -1 max_epochs: int = 1000 def train_step(self, model, input): return model(input) def val_step(self, model, input): return model(input) def train(self, dataloader, metrics, model, optimizer): model.train() for index, input, target in dataloader: self.global_step += 1 output = self.train_step(model, input) loss = metrics.train_step(index, output, target) optimizer.zero_grad() loss.backward() optimizer.step() def val(self, dataloader, metrics, model): model.eval() with torch.no_grad(): for index, input, target in dataloader: output = self.val_step(model, input) metrics.val_step(index, output, target) def fit(self, run): train_loader, val_loader = run.dataloaders[self.fold] it = range(self.epoch + 1, self.epoch + self.max_epochs + 1) for self.epoch in it: run.on_epoch_start() run.on_train_start() self.train(train_loader, run.metrics, run.model, run.optimizer) run.on_train_end() run.on_val_start() self.val(val_loader, run.metrics, run.model) run.on_val_end() try: run.on_epoch_end() except StopIteration: break if run.scheduler: if isinstance(run.scheduler, ReduceLROnPlateau): run.scheduler.step(run.metrics.current_score) else: run.scheduler.step() [1] 2020-03-12 13:09:23 ( 239ms ) python3 ( 260ms )","title":"Trainer"},{"location":"tutorial/trainer/#trainer","text":"Ivory's ivory.torch.Trainer instance manages train and validation loop. At this stage, just take a look at a simple version of the code. from dataclasses import dataclass import torch from torch.optim.lr_scheduler import ReduceLROnPlateau @dataclass class Trainer: fold: int = 0 epoch: int = -1 global_step: int = -1 max_epochs: int = 1000 def train_step(self, model, input): return model(input) def val_step(self, model, input): return model(input) def train(self, dataloader, metrics, model, optimizer): model.train() for index, input, target in dataloader: self.global_step += 1 output = self.train_step(model, input) loss = metrics.train_step(index, output, target) optimizer.zero_grad() loss.backward() optimizer.step() def val(self, dataloader, metrics, model): model.eval() with torch.no_grad(): for index, input, target in dataloader: output = self.val_step(model, input) metrics.val_step(index, output, target) def fit(self, run): train_loader, val_loader = run.dataloaders[self.fold] it = range(self.epoch + 1, self.epoch + self.max_epochs + 1) for self.epoch in it: run.on_epoch_start() run.on_train_start() self.train(train_loader, run.metrics, run.model, run.optimizer) run.on_train_end() run.on_val_start() self.val(val_loader, run.metrics, run.model) run.on_val_end() try: run.on_epoch_end() except StopIteration: break if run.scheduler: if isinstance(run.scheduler, ReduceLROnPlateau): run.scheduler.step(run.metrics.current_score) else: run.scheduler.step() [1] 2020-03-12 13:09:23 ( 239ms ) python3 ( 260ms )","title":"Trainer"}]}