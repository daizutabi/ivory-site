{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ivory Ivory is a lightweight framework for machine learning. It integrates model design, hyperparmeter tuning, and tracking. Ivory uses Optuna for hyperparmeter tuning and MLflow Tracking for tracking. The relationship of these libraries is like below: Ivory's Experiment = Optuna's Study = MLflow's Experiment Ivory's Run = Optuna's Trial = MLflow's Run Using Ivory, you can obtain the both tuning and tracking workflow at one place. Another key feature of Ivory is its model design. You can write down all of your model structure and tuning/tracking process in one YAML file. It allows us to understand the whole process at a glance. Setup Install Ivory from PyPI. $ pip install ivory","title":"Ivory"},{"location":"#ivory","text":"Ivory is a lightweight framework for machine learning. It integrates model design, hyperparmeter tuning, and tracking. Ivory uses Optuna for hyperparmeter tuning and MLflow Tracking for tracking. The relationship of these libraries is like below: Ivory's Experiment = Optuna's Study = MLflow's Experiment Ivory's Run = Optuna's Trial = MLflow's Run Using Ivory, you can obtain the both tuning and tracking workflow at one place. Another key feature of Ivory is its model design. You can write down all of your model structure and tuning/tracking process in one YAML file. It allows us to understand the whole process at a glance.","title":"Ivory"},{"location":"#setup","text":"Install Ivory from PyPI. $ pip install ivory","title":"Setup"},{"location":"tutorial/client_experiment_run/","text":"Client, Experiment, and Run A Client creates and manages an experiment and runs of the experiment. import os import ivory from ivory.core.client import create_client root = os.path.dirname(ivory.__file__) path = os.path.join(root, \"../tests/params.yaml\") client = create_client(path) client [1] 2020-03-30 21:59:42 ( 1.64s ) python3 ( 1.65s ) Client(name='params', num_objects=3) A client instance is a dict-like object. Let's see its contents. for key, value in client.items(): print(f\"{key}={repr(value)}\") [2] 2020-03-30 21:59:43 ( 6.00ms ) python3 ( 1.66s ) tracker=Tracker(tracking_uri='file:///C:/Users/daizu/Documents/GitHub/ivory/tests/mlruns', artifact_location=None) tuner=Tuner(storage='sqlite://', load_if_exists=True) experiment=Experiment(id='1', name='example', num_objects=4) A client has an automatically created experiment. The experiment is also a dict-like object. Let's see its values. for key, value in client.experiment.items(): print(f\"{key}={repr(value)}\") [3] 2020-03-30 21:59:43 ( 8.00ms ) python3 ( 1.67s ) data=Data(mode='train', num_samples=1000) objective=<ivory.core.objective.Objective object at 0x000001C0B3D9E208> tracker=Tracker(tracking_uri='file:///C:/Users/daizu/Documents/GitHub/ivory/tests/mlruns', artifact_location=None) tuner=Tuner(storage='sqlite://', load_if_exists=True) tracker and tuner are copied from the client. The experiment provides data to runs that will be created by the client. Let's check the source. import inspect data = client.experiment.data print(inspect.getsource(data.__class__)) [4] 2020-03-30 21:59:43 ( 6.00ms ) python3 ( 1.67s ) class Data(ivory.core.data.Data): num_samples: int = 1000 def init(self): self.input, self.target = create_data(self.num_samples) self.target = self.target.reshape((-1, 1)) self.index = np.arange(len(self.input)) self.fold = kfold_split(self.input, n_splits=5) def get(self, index=None): if index is None: return [self.index, self.input] else: return [self.index[index], self.input[index], self.target[index]] You can find that the init function initializes its data and get function returns a subset of the data. data.init() data.input[:4] [5] 2020-03-30 21:59:43 ( 5.00ms ) python3 ( 1.68s ) array([[2.638187 , 2.3710504], [4.9634385, 2.998295 ], [1.1058048, 2.9046001], [2.3231244, 4.9858737]], dtype=float32) Now, create a run. run = client.create_run() run [6] 2020-03-30 21:59:43 ( 258ms ) python3 ( 1.94s ) Run(id='c93c4171cad54c6fac75228c0bfd9db7', num_objects=11) Again, a run is a dict-like object. for key, value in run.items(): print(f\"{key}={repr(value)}\") [7] 2020-03-30 21:59:44 ( 10.0ms ) python3 ( 1.95s ) dataloaders=DataLoaders(dataset=example.Dataset(dummy=5), fold=0, batch_size=10) model=Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=100, bias=True) (1): Linear(in_features=100, out_features=100, bias=True) (2): Linear(in_features=100, out_features=1, bias=True) ) ) optimizer=SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) scheduler=<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x000001C0B6282848> results=<ivory.torch.results.Results object at 0x000001C0B62829C8> metrics=Metrics() monitor=Monitor(metric='val_loss', mode='min') early_stopping=EarlyStopping(patience=8) trainer=Trainer(epoch=-1, max_epochs=10, global_step=-1, verbose=2, gpu=False, precision=32, amp_level='O1') data=Data(mode='train', num_samples=1000) tracking=<ivory.callbacks.tracking.Tracking object at 0x000001C0B6282B48> To start the run, just call start method. run.start() [8] 2020-03-30 21:59:44 ( 1.61s ) python3 ( 3.55s ) [] epoch=0 loss=12.06 val_loss=5.307 lr=0.001 [] epoch=1 loss=5.285 val_loss=3.778 lr=0.001 [] epoch=2 loss=3.28 val_loss=1.971 lr=0.001 [] epoch=3 loss=1.913 val_loss=1.045 lr=0.001 [] epoch=4 loss=1.114 val_loss=0.7286 lr=0.001 [] epoch=5 loss=0.7734 val_loss=0.5511 lr=0.001 [] epoch=6 loss=0.6729 val_loss=0.4973 lr=0.001 [] epoch=7 loss=0.5802 val_loss=0.5506 lr=0.001 [] epoch=8 loss=0.5461 val_loss=0.621 lr=0.001 [] epoch=9 loss=0.4493 val_loss=0.3547 lr=0.001 Check the contents of the run again. for key, value in run.items(): print(f\"{key}={repr(value)}\") [9] 2020-03-30 21:59:45 ( 17.0ms ) python3 ( 3.57s ) dataloaders=DataLoaders(dataset=example.Dataset(dummy=5), fold=0, batch_size=10) model=Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=100, bias=True) (1): Linear(in_features=100, out_features=100, bias=True) (2): Linear(in_features=100, out_features=1, bias=True) ) ) optimizer=SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) scheduler=<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x000001C0B6282848> results=<ivory.torch.results.Results object at 0x000001C0B62829C8> metrics=Metrics(loss=0.4493, val_loss=0.3547, lr=0.001) monitor=Monitor(metric='val_loss', mode='min') early_stopping=EarlyStopping(patience=8) trainer=Trainer(epoch=9, max_epochs=10, global_step=799, verbose=2, gpu=False, precision=32, amp_level='O1') data=Data(mode='train', num_samples=1000) tracking=<ivory.callbacks.tracking.Tracking object at 0x000001C0B6282B48> on_fit_start=Callback(['trainer', 'tracking']) on_epoch_start=Callback(['metrics']) on_train_start=Callback(['results', 'metrics', 'trainer']) on_train_end=Callback(['results', 'metrics']) on_val_start=Callback(['results', 'metrics', 'trainer']) on_val_end=Callback(['results', 'metrics']) on_epoch_end=Callback(['metrics', 'monitor', 'early_stopping', 'trainer', 'tracking']) on_fit_end=Callback(['tracking']) on_test_start=Callback(['results', 'trainer']) on_test_end=Callback(['results', 'tracking']) on_XXX_start and on_XXX_end methods were dynamically added. They are callback functions invoked by the started run instance. The repr of them shows that which objects will be called in order.","title":"Client, Experiment, and Run"},{"location":"tutorial/client_experiment_run/#client-experiment-and-run","text":"A Client creates and manages an experiment and runs of the experiment. import os import ivory from ivory.core.client import create_client root = os.path.dirname(ivory.__file__) path = os.path.join(root, \"../tests/params.yaml\") client = create_client(path) client [1] 2020-03-30 21:59:42 ( 1.64s ) python3 ( 1.65s ) Client(name='params', num_objects=3) A client instance is a dict-like object. Let's see its contents. for key, value in client.items(): print(f\"{key}={repr(value)}\") [2] 2020-03-30 21:59:43 ( 6.00ms ) python3 ( 1.66s ) tracker=Tracker(tracking_uri='file:///C:/Users/daizu/Documents/GitHub/ivory/tests/mlruns', artifact_location=None) tuner=Tuner(storage='sqlite://', load_if_exists=True) experiment=Experiment(id='1', name='example', num_objects=4) A client has an automatically created experiment. The experiment is also a dict-like object. Let's see its values. for key, value in client.experiment.items(): print(f\"{key}={repr(value)}\") [3] 2020-03-30 21:59:43 ( 8.00ms ) python3 ( 1.67s ) data=Data(mode='train', num_samples=1000) objective=<ivory.core.objective.Objective object at 0x000001C0B3D9E208> tracker=Tracker(tracking_uri='file:///C:/Users/daizu/Documents/GitHub/ivory/tests/mlruns', artifact_location=None) tuner=Tuner(storage='sqlite://', load_if_exists=True) tracker and tuner are copied from the client. The experiment provides data to runs that will be created by the client. Let's check the source. import inspect data = client.experiment.data print(inspect.getsource(data.__class__)) [4] 2020-03-30 21:59:43 ( 6.00ms ) python3 ( 1.67s ) class Data(ivory.core.data.Data): num_samples: int = 1000 def init(self): self.input, self.target = create_data(self.num_samples) self.target = self.target.reshape((-1, 1)) self.index = np.arange(len(self.input)) self.fold = kfold_split(self.input, n_splits=5) def get(self, index=None): if index is None: return [self.index, self.input] else: return [self.index[index], self.input[index], self.target[index]] You can find that the init function initializes its data and get function returns a subset of the data. data.init() data.input[:4] [5] 2020-03-30 21:59:43 ( 5.00ms ) python3 ( 1.68s ) array([[2.638187 , 2.3710504], [4.9634385, 2.998295 ], [1.1058048, 2.9046001], [2.3231244, 4.9858737]], dtype=float32) Now, create a run. run = client.create_run() run [6] 2020-03-30 21:59:43 ( 258ms ) python3 ( 1.94s ) Run(id='c93c4171cad54c6fac75228c0bfd9db7', num_objects=11) Again, a run is a dict-like object. for key, value in run.items(): print(f\"{key}={repr(value)}\") [7] 2020-03-30 21:59:44 ( 10.0ms ) python3 ( 1.95s ) dataloaders=DataLoaders(dataset=example.Dataset(dummy=5), fold=0, batch_size=10) model=Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=100, bias=True) (1): Linear(in_features=100, out_features=100, bias=True) (2): Linear(in_features=100, out_features=1, bias=True) ) ) optimizer=SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) scheduler=<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x000001C0B6282848> results=<ivory.torch.results.Results object at 0x000001C0B62829C8> metrics=Metrics() monitor=Monitor(metric='val_loss', mode='min') early_stopping=EarlyStopping(patience=8) trainer=Trainer(epoch=-1, max_epochs=10, global_step=-1, verbose=2, gpu=False, precision=32, amp_level='O1') data=Data(mode='train', num_samples=1000) tracking=<ivory.callbacks.tracking.Tracking object at 0x000001C0B6282B48> To start the run, just call start method. run.start() [8] 2020-03-30 21:59:44 ( 1.61s ) python3 ( 3.55s ) [] epoch=0 loss=12.06 val_loss=5.307 lr=0.001 [] epoch=1 loss=5.285 val_loss=3.778 lr=0.001 [] epoch=2 loss=3.28 val_loss=1.971 lr=0.001 [] epoch=3 loss=1.913 val_loss=1.045 lr=0.001 [] epoch=4 loss=1.114 val_loss=0.7286 lr=0.001 [] epoch=5 loss=0.7734 val_loss=0.5511 lr=0.001 [] epoch=6 loss=0.6729 val_loss=0.4973 lr=0.001 [] epoch=7 loss=0.5802 val_loss=0.5506 lr=0.001 [] epoch=8 loss=0.5461 val_loss=0.621 lr=0.001 [] epoch=9 loss=0.4493 val_loss=0.3547 lr=0.001 Check the contents of the run again. for key, value in run.items(): print(f\"{key}={repr(value)}\") [9] 2020-03-30 21:59:45 ( 17.0ms ) python3 ( 3.57s ) dataloaders=DataLoaders(dataset=example.Dataset(dummy=5), fold=0, batch_size=10) model=Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=100, bias=True) (1): Linear(in_features=100, out_features=100, bias=True) (2): Linear(in_features=100, out_features=1, bias=True) ) ) optimizer=SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) scheduler=<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x000001C0B6282848> results=<ivory.torch.results.Results object at 0x000001C0B62829C8> metrics=Metrics(loss=0.4493, val_loss=0.3547, lr=0.001) monitor=Monitor(metric='val_loss', mode='min') early_stopping=EarlyStopping(patience=8) trainer=Trainer(epoch=9, max_epochs=10, global_step=799, verbose=2, gpu=False, precision=32, amp_level='O1') data=Data(mode='train', num_samples=1000) tracking=<ivory.callbacks.tracking.Tracking object at 0x000001C0B6282B48> on_fit_start=Callback(['trainer', 'tracking']) on_epoch_start=Callback(['metrics']) on_train_start=Callback(['results', 'metrics', 'trainer']) on_train_end=Callback(['results', 'metrics']) on_val_start=Callback(['results', 'metrics', 'trainer']) on_val_end=Callback(['results', 'metrics']) on_epoch_end=Callback(['metrics', 'monitor', 'early_stopping', 'trainer', 'tracking']) on_fit_end=Callback(['tracking']) on_test_start=Callback(['results', 'trainer']) on_test_end=Callback(['results', 'tracking']) on_XXX_start and on_XXX_end methods were dynamically added. They are callback functions invoked by the started run instance. The repr of them shows that which objects will be called in order.","title":"Client, Experiment, and Run"},{"location":"tutorial/overview/","text":"Overview In this tutorial, as a simple toy example, we try to predict area of rectangles which have width and height , but they include some noises. Using Ivory, we can write some codes like below: File example.py from dataclasses import dataclass import numpy as np import torch.nn as nn import torch.nn.functional as F import ivory.core.data import ivory.torch.data from ivory.utils import kfold_split def create_data(num_samples=1000): xy = 4 * np.random.rand(num_samples, 2) + 1 xy = xy.astype(np.float32) dx = 0.1 * (np.random.rand(num_samples) - 0.5) dy = 0.1 * (np.random.rand(num_samples) - 0.5) z = ((xy[:, 0] + dx) * (xy[:, 1] + dy)).astype(np.float32) return xy, z @dataclass class Data(ivory.core.data.Data): num_samples: int = 1000 def init(self): self.input, self.target = create_data(self.num_samples) self.target = self.target.reshape((-1, 1)) self.index = np.arange(len(self.input)) self.fold = kfold_split(self.input, n_splits=5) def get(self, index=None): if index is None: return [self.index, self.input] else: return [self.index[index], self.input[index], self.target[index]] @dataclass(repr=False) class Dataset(ivory.torch.data.Dataset): dummy: int = 10 def __len__(self): return len(self.data[0]) def get(self, index): return [x[index] for x in self.data] class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] for in_features, out_features in zip([2] + hidden_sizes, hidden_sizes + [1]): layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) def suggest_lr(trial): trial.suggest_loguniform(\"lr\", 1e-4, 1e-1) def suggest_hidden_sizes(trial, max_num_layers=3): num_layers = trial.suggest_int(\"num_layers\", 2, max_num_layers) for k in range(num_layers): trial.suggest_int(f\"hidden_sizes.{k}\", 10, 30) Then define a learning process in on YAML file: File params.yaml client: tracker: tuner: experiment: name: example data: class: example.Data num_samples: 1000 objective: lr: example.suggest_lr hidden_sizes: def: example.suggest_hidden_sizes max_num_layers: 3 pruner: class: optuna.pruners.MedianPruner run: library: torch dataloaders: dataset: def: example.Dataset dummy: 5 batch_size: 10 fold: 0 model: class: example.Model hidden_sizes: [100, 100] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 results: metrics: criterion: def: torch.nn.functional.mse_loss monitor: metric: val_loss early_stopping: patience: 8 trainer: max_epochs: 10 verbose: 2 In the following sections, we will see in detail step by step.","title":"Overview"},{"location":"tutorial/overview/#overview","text":"In this tutorial, as a simple toy example, we try to predict area of rectangles which have width and height , but they include some noises. Using Ivory, we can write some codes like below: File example.py from dataclasses import dataclass import numpy as np import torch.nn as nn import torch.nn.functional as F import ivory.core.data import ivory.torch.data from ivory.utils import kfold_split def create_data(num_samples=1000): xy = 4 * np.random.rand(num_samples, 2) + 1 xy = xy.astype(np.float32) dx = 0.1 * (np.random.rand(num_samples) - 0.5) dy = 0.1 * (np.random.rand(num_samples) - 0.5) z = ((xy[:, 0] + dx) * (xy[:, 1] + dy)).astype(np.float32) return xy, z @dataclass class Data(ivory.core.data.Data): num_samples: int = 1000 def init(self): self.input, self.target = create_data(self.num_samples) self.target = self.target.reshape((-1, 1)) self.index = np.arange(len(self.input)) self.fold = kfold_split(self.input, n_splits=5) def get(self, index=None): if index is None: return [self.index, self.input] else: return [self.index[index], self.input[index], self.target[index]] @dataclass(repr=False) class Dataset(ivory.torch.data.Dataset): dummy: int = 10 def __len__(self): return len(self.data[0]) def get(self, index): return [x[index] for x in self.data] class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] for in_features, out_features in zip([2] + hidden_sizes, hidden_sizes + [1]): layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) def suggest_lr(trial): trial.suggest_loguniform(\"lr\", 1e-4, 1e-1) def suggest_hidden_sizes(trial, max_num_layers=3): num_layers = trial.suggest_int(\"num_layers\", 2, max_num_layers) for k in range(num_layers): trial.suggest_int(f\"hidden_sizes.{k}\", 10, 30) Then define a learning process in on YAML file: File params.yaml client: tracker: tuner: experiment: name: example data: class: example.Data num_samples: 1000 objective: lr: example.suggest_lr hidden_sizes: def: example.suggest_hidden_sizes max_num_layers: 3 pruner: class: optuna.pruners.MedianPruner run: library: torch dataloaders: dataset: def: example.Dataset dummy: 5 batch_size: 10 fold: 0 model: class: example.Model hidden_sizes: [100, 100] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 results: metrics: criterion: def: torch.nn.functional.mse_loss monitor: metric: val_loss early_stopping: patience: 8 trainer: max_epochs: 10 verbose: 2 In the following sections, we will see in detail step by step.","title":"Overview"},{"location":"tutorial_old/data/","text":"Skipped.","title":"Data"},{"location":"tutorial_old/metrics/","text":"Skipped.","title":"Metrics"},{"location":"tutorial_old/model/","text":"Skipped.","title":"Model"}]}