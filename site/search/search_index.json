{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ivory Overview Ivory is a lightweight machine learning framework. Installation You can install Ivory from PyPI. $ pip install ivory","title":"Ivory"},{"location":"#ivory","text":"","title":"Ivory"},{"location":"#overview","text":"Ivory is a lightweight machine learning framework.","title":"Overview"},{"location":"#installation","text":"You can install Ivory from PyPI. $ pip install ivory","title":"Installation"},{"location":"tutorial/data/","text":"Data Create data As a simple toy example, we try to predict area of rectangles which have width and height , but they include some noises. First, define a function to create such data. import numpy as np import pandas as pd import ivory from ivory.utils import kfold_split def create_data(num_samples=1000): \"\"\"Returns a tuple of (input, target). Target has fold information.\"\"\" x = 4 * np.random.rand(num_samples, 2) + 1 x = x.astype(np.float32) noises = 0.1 * (np.random.rand(2, num_samples) - 0.5) df = pd.DataFrame(x, columns=[\"width\", \"height\"]) df[\"area\"] = (df.width + noises[0]) * (df.height + noises[1]) df.area = df.area.astype(np.float32) df[\"fold\"] = kfold_split(df.index, n_splits=5) return df[[\"width\", \"height\"]], df[[\"fold\", \"area\"]] [1] 2020-03-12 09:48:17 ( 7.00ms ) python3 ( 1.69s ) kfold_split function creates a fold-array. kfold_split(np.arange(10), n_splits=3) [2] 2020-03-12 09:48:17 ( 4.00ms ) python3 ( 1.69s ) array([2, 1, 0, 2, 0, 2, 1, 1, 0, 0], dtype=int8) To execute create_data funtion, you can use a dictionary as well as a YAML file. params = {'data': {'def': 'create_data'}} [3] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.70s ) The key of def means that the value is a funtion instead of a class. Apply this dictionary to ivory.instantiate function to get an instantiated object dictionary. data = ivory.instantiate(params)['data'] data[0].head() [4] 2020-03-12 09:48:17 ( 11.0ms ) python3 ( 1.71s ) width height 0 4.511972 3.982252 1 3.250920 1.691386 2 1.991131 4.563490 3 3.670630 4.498393 4 3.581269 2.286469 data[1].head() [5] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.71s ) fold area 0 2 18.085651 1 0 5.471332 2 1 9.248866 3 2 16.546057 4 3 7.932877 You can change the data size with additional key-value pairs (in this case, the minimun size is the number of fold 5). params = {'data': {'def': 'create_data', 'num_samples': 5}} ivory.instantiate(params)['data'][1] [6] 2020-03-12 09:48:17 ( 8.00ms ) python3 ( 1.72s ) fold area 0 1 16.474211 1 2 14.433804 2 0 17.797640 3 3 2.849218 4 4 3.960932 Dataset Ivory provides ivory.torch.Dataset for PyTorch. from ivory.torch import Dataset dataset = Dataset(input=data[0], target=data[1]) dataset [7] 2020-03-12 09:48:17 ( 4.00ms ) python3 ( 1.73s ) Dataset(num_samples=1000, input_shape=(2,), target_shape=(2,), mode='train') Ivory's Dataset is a subclass of PyTorch's Dataset import torch.utils.data isinstance(dataset, torch.utils.data.Dataset) [8] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.73s ) True Check an item. dataset[0] [9] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.73s ) (0, array([4.511972, 3.982252], dtype=float32), array([ 2. , 18.085651], dtype=float32)) Indexing returns a tuple. The first is an index, the second is an input, and the last is a target. The target includes fold which is not a real target. But, it's okay because we don't use a Dataset directly. We can use more useful DataLoaders provided by Ivory. DataLoaders DataLoaders provides a data loader for both training and validation. This is the reason why our data include fold. from ivory.torch import DataLoaders dataloaders = DataLoaders(input=data[0], target=data[1], batch_size=3) dataloaders [10] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.74s ) DataLoaders(num_folds=5, num_samples=1000, input_shape=(2,), target_shape=(1,)) DataLoaders instance can detect the number of fold and remove the fold information from the target. You can get a pair of train and validation data loaders at once by indexing like a list . train_loader, val_loader = dataloaders[0] # for fold-0. [11] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.74s ) Here, the index is corresponding to a fold, in this case, ranging from 0 to 4 because the number of K-fold is 5. Check the data loader. train_loader [12] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.75s ) <torch.utils.data.dataloader.DataLoader at 0x22e684ba348> The data loader is a pure PyTorch's DataLoader . Let's see the dataset that the data loader has. train_loader.dataset [13] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.75s ) Dataset(num_samples=800, input_shape=(2,), target_shape=(1,), mode='train') This is our Ivory's dataset ( ivory.torch.Dataset ). The number of samples decreases from 1000 to 800 because we use 5-fold splitting (80% reduction). Validation dataset shoud have the rest 20% samples. val_loader.dataset [14] 2020-03-12 09:48:17 ( 4.00ms ) python3 ( 1.75s ) Dataset(num_samples=200, input_shape=(2,), target_shape=(1,), mode='val') Now check iteration. next(iter(train_loader)) [15] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.76s ) [tensor([314, 180, 439]), tensor([[4.5884, 4.6709], [1.4973, 1.0977], [3.9966, 1.6659]]), tensor([[21.7108], [ 1.6970], [ 6.6598]])] next() returns a list of torch.Tensor . The first is an index, the second is an input, and the last is a target. Note that the target doesn't include fold any more. In default setting, train data loader shuffles its data. Validation data loader doesn't: next(iter(val_loader)) [16] 2020-03-12 09:48:17 ( 4.00ms ) python3 ( 1.76s ) [tensor([1, 5, 8]), tensor([[3.2509, 1.6914], [1.0459, 4.2624], [1.2749, 3.0739]]), tensor([[5.4713], [4.5953], [3.9403]])] DataLoaders via a YAML file As well as data, you can create your DataLoaders from a YAML file. File params_1a.yaml data: def: create_data num_samples: 60 dataloaders: class: ivory.torch.DataLoaders input: $.data.0 target: $.data.1 batch_size: 10 To create a DataLoaders , you need to give input and target to the DataLoaders initializer. Unlike a simple parameter ( num_samples or batch_size in this case), these can't be written in a YAML file directly. Instead, you can assign them using \" $ -notation \". In a YAML file, a value that starts with ' $. ' means an instance. In additon, if the value ends with ' .(digit) ', it is an element taken from a sequence by indexing. In the above case, ' $.data.0 ' is the first element of a tuple data created by the create_data function. Forthermote, you can use more direct form: inline unpacking . File params_1b.yaml input, target: def: create_data num_samples: 60 dataloaders: class: ivory.torch.DataLoaders input: $ target: $ batch_size: 10 Now, the output of create_data is unpacked to input and target . In $-notation, '(dot)+(instance name)' can be omitted if the key name is equal to an instance name. Next, instantiate them after loading the YAML file: import yaml with open('params_1b.yaml') as f: yml = yaml.safe_load(f) yml [17] 2020-03-12 09:48:17 ( 6.00ms ) python3 ( 1.77s ) {'input, target': {'def': 'create_data', 'num_samples': 60}, 'dataloaders': {'class': 'ivory.torch.DataLoaders', 'input': '$', 'target': '$', 'batch_size': 10}} params = ivory.instantiate(yml) params.keys() [18] 2020-03-12 09:48:17 ( 8.00ms ) python3 ( 1.78s ) dict_keys(['input', 'target', 'dataloaders']) params['input'].shape, params['target'].shape [19] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.78s ) ((60, 2), (60, 2)) params['dataloaders'] [20] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.78s ) DataLoaders(num_folds=5, num_samples=60, input_shape=(2,), target_shape=(1,)) Everythings works well!. Notice that the global name space of Python isn't affected by these instantiatitions. try: target except NameError: print(\"`target` doesn't exsist.\") [21] 2020-03-12 09:48:17 ( 4.00ms ) python3 ( 1.79s ) `target` doesn't exsist. In the next section, we will introduce model/optimizer/schduler combination for this dataset.","title":"Data"},{"location":"tutorial/data/#data","text":"","title":"Data"},{"location":"tutorial/data/#create-data","text":"As a simple toy example, we try to predict area of rectangles which have width and height , but they include some noises. First, define a function to create such data. import numpy as np import pandas as pd import ivory from ivory.utils import kfold_split def create_data(num_samples=1000): \"\"\"Returns a tuple of (input, target). Target has fold information.\"\"\" x = 4 * np.random.rand(num_samples, 2) + 1 x = x.astype(np.float32) noises = 0.1 * (np.random.rand(2, num_samples) - 0.5) df = pd.DataFrame(x, columns=[\"width\", \"height\"]) df[\"area\"] = (df.width + noises[0]) * (df.height + noises[1]) df.area = df.area.astype(np.float32) df[\"fold\"] = kfold_split(df.index, n_splits=5) return df[[\"width\", \"height\"]], df[[\"fold\", \"area\"]] [1] 2020-03-12 09:48:17 ( 7.00ms ) python3 ( 1.69s ) kfold_split function creates a fold-array. kfold_split(np.arange(10), n_splits=3) [2] 2020-03-12 09:48:17 ( 4.00ms ) python3 ( 1.69s ) array([2, 1, 0, 2, 0, 2, 1, 1, 0, 0], dtype=int8) To execute create_data funtion, you can use a dictionary as well as a YAML file. params = {'data': {'def': 'create_data'}} [3] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.70s ) The key of def means that the value is a funtion instead of a class. Apply this dictionary to ivory.instantiate function to get an instantiated object dictionary. data = ivory.instantiate(params)['data'] data[0].head() [4] 2020-03-12 09:48:17 ( 11.0ms ) python3 ( 1.71s ) width height 0 4.511972 3.982252 1 3.250920 1.691386 2 1.991131 4.563490 3 3.670630 4.498393 4 3.581269 2.286469 data[1].head() [5] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.71s ) fold area 0 2 18.085651 1 0 5.471332 2 1 9.248866 3 2 16.546057 4 3 7.932877 You can change the data size with additional key-value pairs (in this case, the minimun size is the number of fold 5). params = {'data': {'def': 'create_data', 'num_samples': 5}} ivory.instantiate(params)['data'][1] [6] 2020-03-12 09:48:17 ( 8.00ms ) python3 ( 1.72s ) fold area 0 1 16.474211 1 2 14.433804 2 0 17.797640 3 3 2.849218 4 4 3.960932","title":"Create data"},{"location":"tutorial/data/#dataset","text":"Ivory provides ivory.torch.Dataset for PyTorch. from ivory.torch import Dataset dataset = Dataset(input=data[0], target=data[1]) dataset [7] 2020-03-12 09:48:17 ( 4.00ms ) python3 ( 1.73s ) Dataset(num_samples=1000, input_shape=(2,), target_shape=(2,), mode='train') Ivory's Dataset is a subclass of PyTorch's Dataset import torch.utils.data isinstance(dataset, torch.utils.data.Dataset) [8] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.73s ) True Check an item. dataset[0] [9] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.73s ) (0, array([4.511972, 3.982252], dtype=float32), array([ 2. , 18.085651], dtype=float32)) Indexing returns a tuple. The first is an index, the second is an input, and the last is a target. The target includes fold which is not a real target. But, it's okay because we don't use a Dataset directly. We can use more useful DataLoaders provided by Ivory.","title":"Dataset"},{"location":"tutorial/data/#dataloaders","text":"DataLoaders provides a data loader for both training and validation. This is the reason why our data include fold. from ivory.torch import DataLoaders dataloaders = DataLoaders(input=data[0], target=data[1], batch_size=3) dataloaders [10] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.74s ) DataLoaders(num_folds=5, num_samples=1000, input_shape=(2,), target_shape=(1,)) DataLoaders instance can detect the number of fold and remove the fold information from the target. You can get a pair of train and validation data loaders at once by indexing like a list . train_loader, val_loader = dataloaders[0] # for fold-0. [11] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.74s ) Here, the index is corresponding to a fold, in this case, ranging from 0 to 4 because the number of K-fold is 5. Check the data loader. train_loader [12] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.75s ) <torch.utils.data.dataloader.DataLoader at 0x22e684ba348> The data loader is a pure PyTorch's DataLoader . Let's see the dataset that the data loader has. train_loader.dataset [13] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.75s ) Dataset(num_samples=800, input_shape=(2,), target_shape=(1,), mode='train') This is our Ivory's dataset ( ivory.torch.Dataset ). The number of samples decreases from 1000 to 800 because we use 5-fold splitting (80% reduction). Validation dataset shoud have the rest 20% samples. val_loader.dataset [14] 2020-03-12 09:48:17 ( 4.00ms ) python3 ( 1.75s ) Dataset(num_samples=200, input_shape=(2,), target_shape=(1,), mode='val') Now check iteration. next(iter(train_loader)) [15] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.76s ) [tensor([314, 180, 439]), tensor([[4.5884, 4.6709], [1.4973, 1.0977], [3.9966, 1.6659]]), tensor([[21.7108], [ 1.6970], [ 6.6598]])] next() returns a list of torch.Tensor . The first is an index, the second is an input, and the last is a target. Note that the target doesn't include fold any more. In default setting, train data loader shuffles its data. Validation data loader doesn't: next(iter(val_loader)) [16] 2020-03-12 09:48:17 ( 4.00ms ) python3 ( 1.76s ) [tensor([1, 5, 8]), tensor([[3.2509, 1.6914], [1.0459, 4.2624], [1.2749, 3.0739]]), tensor([[5.4713], [4.5953], [3.9403]])]","title":"DataLoaders"},{"location":"tutorial/data/#dataloaders-via-a-yaml-file","text":"As well as data, you can create your DataLoaders from a YAML file. File params_1a.yaml data: def: create_data num_samples: 60 dataloaders: class: ivory.torch.DataLoaders input: $.data.0 target: $.data.1 batch_size: 10 To create a DataLoaders , you need to give input and target to the DataLoaders initializer. Unlike a simple parameter ( num_samples or batch_size in this case), these can't be written in a YAML file directly. Instead, you can assign them using \" $ -notation \". In a YAML file, a value that starts with ' $. ' means an instance. In additon, if the value ends with ' .(digit) ', it is an element taken from a sequence by indexing. In the above case, ' $.data.0 ' is the first element of a tuple data created by the create_data function. Forthermote, you can use more direct form: inline unpacking . File params_1b.yaml input, target: def: create_data num_samples: 60 dataloaders: class: ivory.torch.DataLoaders input: $ target: $ batch_size: 10 Now, the output of create_data is unpacked to input and target . In $-notation, '(dot)+(instance name)' can be omitted if the key name is equal to an instance name. Next, instantiate them after loading the YAML file: import yaml with open('params_1b.yaml') as f: yml = yaml.safe_load(f) yml [17] 2020-03-12 09:48:17 ( 6.00ms ) python3 ( 1.77s ) {'input, target': {'def': 'create_data', 'num_samples': 60}, 'dataloaders': {'class': 'ivory.torch.DataLoaders', 'input': '$', 'target': '$', 'batch_size': 10}} params = ivory.instantiate(yml) params.keys() [18] 2020-03-12 09:48:17 ( 8.00ms ) python3 ( 1.78s ) dict_keys(['input', 'target', 'dataloaders']) params['input'].shape, params['target'].shape [19] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.78s ) ((60, 2), (60, 2)) params['dataloaders'] [20] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.78s ) DataLoaders(num_folds=5, num_samples=60, input_shape=(2,), target_shape=(1,)) Everythings works well!. Notice that the global name space of Python isn't affected by these instantiatitions. try: target except NameError: print(\"`target` doesn't exsist.\") [21] 2020-03-12 09:48:17 ( 4.00ms ) python3 ( 1.79s ) `target` doesn't exsist. In the next section, we will introduce model/optimizer/schduler combination for this dataset.","title":"DataLoaders via a YAML file"},{"location":"tutorial/experiment/","text":"Experiment The first start point in Ivory is ivory.Experiment . This master object controls a machine learning experiment under a certain condition. Let's create your first Experiment instance. In your favorite directory, write a YAML file like below named params_0.yaml : File params_0.yaml experiment: class: ivory.Experiment The class key notices that the experiment is an instance of ivory.Experiment . Then, in a jupyte notebook or a Python script under the same directory: import ivory experiment = ivory.create_experiment('params_0.yaml') experiment [1] 2020-03-12 09:48:15 ( 1.46s ) python3 ( 1.46s ) Experiment(name='ready', run_class='ivory.core.Run', shared=[], num_runs=0) You created an Experiment instance. It says that its name is \"ready\" and its run_class is ivory.core.Run . Next, you can start the Experiment . experiment.start() experiment [2] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.47s ) Experiment(name='2020/03/12 09:48:17', run_class='ivory.core.Run', shared=[], num_runs=0) The name was changed to the current time. An Experiment instance is like an environment. Actual interesting processes such as machine learning are done by Run s. So, create a Run . run = experiment.create_run() run [3] 2020-03-12 09:48:17 ( 6.00ms ) python3 ( 1.47s ) Run(name='#1', callbacks=[]) This Run instance was named '#1' because this is the first one for the Experiment instance. Do you want to start this run? Let's try it. run.start() [4] 2020-03-12 09:48:17 ( 108ms ) python3 ( 1.58s ) AttributeError: 'Run' object has no attribute 'trainer' AttributeError Traceback (most recent call last) <ipython-input-7-c930d12f52e8> in <module> ----> 1 run.start() ~\\Documents\\GitHub\\ivory\\ivory\\core\\run.py in start(self) 37 self.on_fit_start() 38 try: ---> 39 self.trainer.fit(self) 40 finally: 41 self.on_fit_end() Oops! The Run instance has the method indeed, but it says there is no trainer . Of course, we need some data, a model, metrics, etc . for a particular machine learning problem. Also, we need to decide how to train the model. Following sections will explain this process step by step.","title":"Experiment"},{"location":"tutorial/experiment/#experiment","text":"The first start point in Ivory is ivory.Experiment . This master object controls a machine learning experiment under a certain condition. Let's create your first Experiment instance. In your favorite directory, write a YAML file like below named params_0.yaml : File params_0.yaml experiment: class: ivory.Experiment The class key notices that the experiment is an instance of ivory.Experiment . Then, in a jupyte notebook or a Python script under the same directory: import ivory experiment = ivory.create_experiment('params_0.yaml') experiment [1] 2020-03-12 09:48:15 ( 1.46s ) python3 ( 1.46s ) Experiment(name='ready', run_class='ivory.core.Run', shared=[], num_runs=0) You created an Experiment instance. It says that its name is \"ready\" and its run_class is ivory.core.Run . Next, you can start the Experiment . experiment.start() experiment [2] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.47s ) Experiment(name='2020/03/12 09:48:17', run_class='ivory.core.Run', shared=[], num_runs=0) The name was changed to the current time. An Experiment instance is like an environment. Actual interesting processes such as machine learning are done by Run s. So, create a Run . run = experiment.create_run() run [3] 2020-03-12 09:48:17 ( 6.00ms ) python3 ( 1.47s ) Run(name='#1', callbacks=[]) This Run instance was named '#1' because this is the first one for the Experiment instance. Do you want to start this run? Let's try it. run.start() [4] 2020-03-12 09:48:17 ( 108ms ) python3 ( 1.58s ) AttributeError: 'Run' object has no attribute 'trainer' AttributeError Traceback (most recent call last) <ipython-input-7-c930d12f52e8> in <module> ----> 1 run.start() ~\\Documents\\GitHub\\ivory\\ivory\\core\\run.py in start(self) 37 self.on_fit_start() 38 try: ---> 39 self.trainer.fit(self) 40 finally: 41 self.on_fit_end() Oops! The Run instance has the method indeed, but it says there is no trainer . Of course, we need some data, a model, metrics, etc . for a particular machine learning problem. Also, we need to decide how to train the model. Following sections will explain this process step by step.","title":"Experiment"},{"location":"tutorial/metrics/","text":"Metrics Ivory has a ivory.callbacks.Metrics that doesn't depend on any specific library such as PyTorch or scikit-learn, etc. As the module name shows, a Metrics is a callback called from a Trainer , which is created by a Run . So at this stage, we cannot get an instance of Metrics . Instead, let's check the ivory.torch.Metrics that is a metrics class for PyTorch. A instance of this class will be called from ivory.torch.Trainer . File ivory/torch/metrics.py from typing import Any, Dict, Tuple from torch import Tensor import ivory.callbacks from ivory.torch.utils import cpu class Metrics(ivory.callbacks.Metrics): def evaluate(self, loss, output, target) -> Dict[str, float]: return {\"loss\": loss.item()} def train_evaluate(self, output, target) -> Tuple[Tensor, Dict[str, float]]: loss = self.criterion(output, target) output = output.detach() return loss, self.evaluate(loss, output, target) def val_evaluate(self, output, target) -> Tuple[Any, Dict[str, float]]: loss = self.criterion(output, target) output = output.detach() record = self.evaluate(loss, output, target) if output.device.type != \"cpu\": output = cpu(output) return output, record def on_current_record(self, run): self.current_record[\"lr\"] = run.optimizer.param_groups[0][\"lr\"] There four instance methods. train_evaluate and val_evaluate are functions that called from train and validation loops. evalute is a customizable function for metrics at a step. You can overwrite this function, for example: from typing import Dict import torch import ivory.torch class MyMetrics(ivory.torch.Metrics): def evaluate(self, loss, output, target) -> Dict[str, float]: mse = torch.mean((output - target) ** 2).item() return {\"loss\": loss.item(), \"mse\": mse} # Here, loss == mse [1] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.95s ) The last method on_current_record is called from on_epoch_end callback function. This gives you a chance to modify a current metrics for epoch (we call it a record .) In the above case, we add the learning rate of our optimizer in order to monitor it during a traing loop. A callback function of Ivoy takes one argument run which is corresponding to the current run. You can access all of the run information via this run instance.","title":"Metrics"},{"location":"tutorial/metrics/#metrics","text":"Ivory has a ivory.callbacks.Metrics that doesn't depend on any specific library such as PyTorch or scikit-learn, etc. As the module name shows, a Metrics is a callback called from a Trainer , which is created by a Run . So at this stage, we cannot get an instance of Metrics . Instead, let's check the ivory.torch.Metrics that is a metrics class for PyTorch. A instance of this class will be called from ivory.torch.Trainer . File ivory/torch/metrics.py from typing import Any, Dict, Tuple from torch import Tensor import ivory.callbacks from ivory.torch.utils import cpu class Metrics(ivory.callbacks.Metrics): def evaluate(self, loss, output, target) -> Dict[str, float]: return {\"loss\": loss.item()} def train_evaluate(self, output, target) -> Tuple[Tensor, Dict[str, float]]: loss = self.criterion(output, target) output = output.detach() return loss, self.evaluate(loss, output, target) def val_evaluate(self, output, target) -> Tuple[Any, Dict[str, float]]: loss = self.criterion(output, target) output = output.detach() record = self.evaluate(loss, output, target) if output.device.type != \"cpu\": output = cpu(output) return output, record def on_current_record(self, run): self.current_record[\"lr\"] = run.optimizer.param_groups[0][\"lr\"] There four instance methods. train_evaluate and val_evaluate are functions that called from train and validation loops. evalute is a customizable function for metrics at a step. You can overwrite this function, for example: from typing import Dict import torch import ivory.torch class MyMetrics(ivory.torch.Metrics): def evaluate(self, loss, output, target) -> Dict[str, float]: mse = torch.mean((output - target) ** 2).item() return {\"loss\": loss.item(), \"mse\": mse} # Here, loss == mse [1] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.95s ) The last method on_current_record is called from on_epoch_end callback function. This gives you a chance to modify a current metrics for epoch (we call it a record .) In the above case, we add the learning rate of our optimizer in order to monitor it during a traing loop. A callback function of Ivoy takes one argument run which is corresponding to the current run. You can access all of the run information via this run instance.","title":"Metrics"},{"location":"tutorial/model/","text":"Model We will train a model to predict a toy rectangle area problem. First, define a simple model with hidden_sizes as a hyper parameter (list of integers). import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] it = zip([2] + hidden_sizes, hidden_sizes + [1]) for in_features, out_features in it: layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) Model([3, 4]) # Just example [1] 2020-03-12 09:48:17 ( 7.00ms ) python3 ( 1.80s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=3, bias=True) (1): Linear(in_features=3, out_features=4, bias=True) (2): Linear(in_features=4, out_features=1, bias=True) ) ) To train this model, we need an optimizer and an optional schduler. Write them in a YAML file. File params_c.yaml input, target: def: create_data num_samples: 10000 dataloaders: class: ivory.torch.DataLoaders input: $ target: $ batch_size: 32 model: class: Model hidden_sizes: [10, 10] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.1 patience: 4 An PyTorch optimizer needs model's parameters as the first argument. We can give them by $-notation with attribute accessor ( $.model.parameters() ). The rest part of this YAML file is straight forward. Now, we have other hyper parameters such as lr (learning rate), factor , or patience . All of these hyper parameters are written in one YAML file. This allows us to manage them easily. Again, let's create instances: import yaml from ivory.utils import to_float with open('params_c.yaml') as f: yml = to_float(yaml.safe_load(f)) params = ivory.instantiate(yml) params.keys() [2] 2020-03-12 09:48:17 ( 13.0ms ) python3 ( 1.81s ) dict_keys(['input', 'target', 'dataloaders', 'model', 'optimizer', 'scheduler']) Here, a helper function ivory.utils.to_float converts an exponential expression such as \"1e-3\" to \"0.001\". Check the created instances params['model'] [3] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.81s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=10, bias=True) (1): Linear(in_features=10, out_features=10, bias=True) (2): Linear(in_features=10, out_features=1, bias=True) ) ) params['optimizer'] [4] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.81s ) SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) params['scheduler'] [5] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.82s ) <torch.optim.lr_scheduler.ReduceLROnPlateau at 0x22e686333c8> Now, we have data and a model with an optimizer so we can start to train . dataloader, _ = params['dataloaders'][0] index, input_, target = next(iter(dataloader)) input_[:4] [6] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.82s ) tensor([[3.2481, 4.8582], [4.5864, 1.1889], [3.3863, 1.3553], [2.9476, 3.0847]]) output = params['model'](input_) output[:4] [7] 2020-03-12 09:48:17 ( 6.00ms ) python3 ( 1.83s ) tensor([[-0.1175], [-0.2202], [-0.2132], [-0.1453]], grad_fn=<SliceBackward>) Calculate the first loss loss = F.mse_loss(output, target) loss [8] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.83s ) tensor(114.4701, grad_fn=<MseLossBackward>) Backpropagate and update the weights. params['optimizer'].zero_grad() loss.backward() params['optimizer'].step() [9] 2020-03-12 09:48:17 ( 99.0ms ) python3 ( 1.93s ) Calculate the second loss after the first optimization step. output = params['model'](input_) loss = F.mse_loss(output, target) loss [10] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.94s ) tensor(111.4255, grad_fn=<MseLossBackward>) To train a model, we need metrics to estimate the model performance. In the next section, we will introduce Metrics class for it.","title":"Model"},{"location":"tutorial/model/#model","text":"We will train a model to predict a toy rectangle area problem. First, define a simple model with hidden_sizes as a hyper parameter (list of integers). import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] it = zip([2] + hidden_sizes, hidden_sizes + [1]) for in_features, out_features in it: layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) Model([3, 4]) # Just example [1] 2020-03-12 09:48:17 ( 7.00ms ) python3 ( 1.80s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=3, bias=True) (1): Linear(in_features=3, out_features=4, bias=True) (2): Linear(in_features=4, out_features=1, bias=True) ) ) To train this model, we need an optimizer and an optional schduler. Write them in a YAML file. File params_c.yaml input, target: def: create_data num_samples: 10000 dataloaders: class: ivory.torch.DataLoaders input: $ target: $ batch_size: 32 model: class: Model hidden_sizes: [10, 10] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.1 patience: 4 An PyTorch optimizer needs model's parameters as the first argument. We can give them by $-notation with attribute accessor ( $.model.parameters() ). The rest part of this YAML file is straight forward. Now, we have other hyper parameters such as lr (learning rate), factor , or patience . All of these hyper parameters are written in one YAML file. This allows us to manage them easily. Again, let's create instances: import yaml from ivory.utils import to_float with open('params_c.yaml') as f: yml = to_float(yaml.safe_load(f)) params = ivory.instantiate(yml) params.keys() [2] 2020-03-12 09:48:17 ( 13.0ms ) python3 ( 1.81s ) dict_keys(['input', 'target', 'dataloaders', 'model', 'optimizer', 'scheduler']) Here, a helper function ivory.utils.to_float converts an exponential expression such as \"1e-3\" to \"0.001\". Check the created instances params['model'] [3] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.81s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=10, bias=True) (1): Linear(in_features=10, out_features=10, bias=True) (2): Linear(in_features=10, out_features=1, bias=True) ) ) params['optimizer'] [4] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.81s ) SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) params['scheduler'] [5] 2020-03-12 09:48:17 ( 3.00ms ) python3 ( 1.82s ) <torch.optim.lr_scheduler.ReduceLROnPlateau at 0x22e686333c8> Now, we have data and a model with an optimizer so we can start to train . dataloader, _ = params['dataloaders'][0] index, input_, target = next(iter(dataloader)) input_[:4] [6] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.82s ) tensor([[3.2481, 4.8582], [4.5864, 1.1889], [3.3863, 1.3553], [2.9476, 3.0847]]) output = params['model'](input_) output[:4] [7] 2020-03-12 09:48:17 ( 6.00ms ) python3 ( 1.83s ) tensor([[-0.1175], [-0.2202], [-0.2132], [-0.1453]], grad_fn=<SliceBackward>) Calculate the first loss loss = F.mse_loss(output, target) loss [8] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.83s ) tensor(114.4701, grad_fn=<MseLossBackward>) Backpropagate and update the weights. params['optimizer'].zero_grad() loss.backward() params['optimizer'].step() [9] 2020-03-12 09:48:17 ( 99.0ms ) python3 ( 1.93s ) Calculate the second loss after the first optimization step. output = params['model'](input_) loss = F.mse_loss(output, target) loss [10] 2020-03-12 09:48:17 ( 5.00ms ) python3 ( 1.94s ) tensor(111.4255, grad_fn=<MseLossBackward>) To train a model, we need metrics to estimate the model performance. In the next section, we will introduce Metrics class for it.","title":"Model"},{"location":"tutorial/trainer/","text":"Trainer Ivory's ivory.torch.Trainer instance manages train and validation loop. File ivory/torch/trainer.py from dataclasses import dataclass from typing import Optional import torch from torch.optim.lr_scheduler import ReduceLROnPlateau from tqdm import tqdm from ivory.torch.utils import cuda try: from apex import amp except ImportError: pass @dataclass class Trainer: fold: int = 0 epoch: int = -1 global_step: int = -1 max_epochs: int = 1000 gpu: bool = False amp_level: Optional[str] = None verbose: int = 1 def train_step(self, model, input): return model(input) def val_step(self, model, input): return model(input) def train(self, dataloader, metrics, model, optimizer): model.train() if self.verbose == 1: lr = optimizer.param_groups[0][\"lr\"] dataloader = tqdm(dataloader, desc=f\"LR{lr:.1e}\", leave=False) for index, input, target in dataloader: self.global_step += 1 if self.gpu: input = cuda(input) target = cuda(target) output = self.train_step(model, input) loss = metrics.train_step(index, output, target) optimizer.zero_grad() if self.gpu and self.amp_level: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward() else: loss.backward() optimizer.step() def val(self, dataloader, metrics, model): model.eval() if self.verbose == 1: dataloader = tqdm(dataloader, desc=\"-Validate\", leave=False) with torch.no_grad(): for index, input, target in dataloader: if self.gpu: input = cuda(input) target = cuda(target) output = self.val_step(model, input) metrics.val_step(index, output, target) def fit(self, run): train_loader, val_loader = run.dataloaders[self.fold] if self.gpu: run.model.cuda() if self.amp_level: run.model, run.optimizer = amp.initialize( run.model, run.optimizer, opt_level=self.amp_level ) it = range(self.epoch + 1, self.epoch + self.max_epochs + 1) for self.epoch in tqdm(it) if self.verbose == 1 else it: run.on_epoch_start() run.on_train_start() self.train(train_loader, run.metrics, run.model, run.optimizer) run.on_train_end() run.on_val_start() self.val(val_loader, run.metrics, run.model) run.on_val_end() try: run.on_epoch_end() except StopIteration: break finally: if self.verbose: latest = run.metrics.latest tqdm.write(f\"[{run.name}] epoch={self.epoch:03d} {latest}\") if run.scheduler: if isinstance(run.scheduler, ReduceLROnPlateau): run.scheduler.step(run.metrics.current_score) else: run.scheduler.step() def state_dict(self): return { \"fold\": self.fold, \"epoch\": self.epoch, \"global_step\": self.global_step, } def load_state_dict(self, state_dict): self.fold = state_dict[\"fold\"] self.epoch = state_dict[\"epoch\"] self.global_step = state_dict[\"global_step\"]","title":"Trainer"},{"location":"tutorial/trainer/#trainer","text":"Ivory's ivory.torch.Trainer instance manages train and validation loop. File ivory/torch/trainer.py from dataclasses import dataclass from typing import Optional import torch from torch.optim.lr_scheduler import ReduceLROnPlateau from tqdm import tqdm from ivory.torch.utils import cuda try: from apex import amp except ImportError: pass @dataclass class Trainer: fold: int = 0 epoch: int = -1 global_step: int = -1 max_epochs: int = 1000 gpu: bool = False amp_level: Optional[str] = None verbose: int = 1 def train_step(self, model, input): return model(input) def val_step(self, model, input): return model(input) def train(self, dataloader, metrics, model, optimizer): model.train() if self.verbose == 1: lr = optimizer.param_groups[0][\"lr\"] dataloader = tqdm(dataloader, desc=f\"LR{lr:.1e}\", leave=False) for index, input, target in dataloader: self.global_step += 1 if self.gpu: input = cuda(input) target = cuda(target) output = self.train_step(model, input) loss = metrics.train_step(index, output, target) optimizer.zero_grad() if self.gpu and self.amp_level: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward() else: loss.backward() optimizer.step() def val(self, dataloader, metrics, model): model.eval() if self.verbose == 1: dataloader = tqdm(dataloader, desc=\"-Validate\", leave=False) with torch.no_grad(): for index, input, target in dataloader: if self.gpu: input = cuda(input) target = cuda(target) output = self.val_step(model, input) metrics.val_step(index, output, target) def fit(self, run): train_loader, val_loader = run.dataloaders[self.fold] if self.gpu: run.model.cuda() if self.amp_level: run.model, run.optimizer = amp.initialize( run.model, run.optimizer, opt_level=self.amp_level ) it = range(self.epoch + 1, self.epoch + self.max_epochs + 1) for self.epoch in tqdm(it) if self.verbose == 1 else it: run.on_epoch_start() run.on_train_start() self.train(train_loader, run.metrics, run.model, run.optimizer) run.on_train_end() run.on_val_start() self.val(val_loader, run.metrics, run.model) run.on_val_end() try: run.on_epoch_end() except StopIteration: break finally: if self.verbose: latest = run.metrics.latest tqdm.write(f\"[{run.name}] epoch={self.epoch:03d} {latest}\") if run.scheduler: if isinstance(run.scheduler, ReduceLROnPlateau): run.scheduler.step(run.metrics.current_score) else: run.scheduler.step() def state_dict(self): return { \"fold\": self.fold, \"epoch\": self.epoch, \"global_step\": self.global_step, } def load_state_dict(self, state_dict): self.fold = state_dict[\"fold\"] self.epoch = state_dict[\"epoch\"] self.global_step = state_dict[\"global_step\"]","title":"Trainer"}]}