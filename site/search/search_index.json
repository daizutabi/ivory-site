{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ivory Documentation Ivory is a lightweight framework for machine learning. It integrates model design, tracking, and hyperparmeter tuning. Ivory uses MLflow Tracking for tracking and Optuna for hyperparmeter tuning. Using Ivory, you can tackle both tracking and tuning workflow at one place. Another key feature of Ivory is its workflow design. You can write down all of your workflow such as model structure or tracking/tuning process in one YAML file. It allows us to understand the whole process at a glance. Ivory is library-agnostic. You can use it with any machine learning library. Get started using the Quickstart. Quickstart Or take a look at the code below. import numpy as np from ivory.callbacks.results import Results from ivory.core.data import Data, Dataset, Datasets from ivory.core.run import Run from ivory.sklearn.estimator import Estimator from ivory.sklearn.metrics import Metrics data = Data() data.index = np.arange(30) data.input = np.arange(60).reshape(30, -1) data.target = np.sum(data.input, axis=1) data.fold = data.index % 4 datasets = Datasets(data, Dataset, fold=0) estimator = Estimator( model='sklearn.ensemble.RandomForestRegressor', n_estimators=10, max_depth=5, ) run = Run( name='first example', datasets=datasets, estimator=estimator, results=Results(), metrics=Metrics() ) run.start() [2] 2020-06-20 15:23:32 ( 417ms ) python3 ( 431ms ) [first example] mse=14.7 import matplotlib.pyplot as plt plt.scatter(run.results.val.target, run.results.val.output) [3] 2020-06-20 15:23:33 ( 242ms ) python3 ( 673ms ) <matplotlib.collections.PathCollection at 0x140448ad408>","title":"Ivory Documentation"},{"location":"#ivory-documentation","text":"Ivory is a lightweight framework for machine learning. It integrates model design, tracking, and hyperparmeter tuning. Ivory uses MLflow Tracking for tracking and Optuna for hyperparmeter tuning. Using Ivory, you can tackle both tracking and tuning workflow at one place. Another key feature of Ivory is its workflow design. You can write down all of your workflow such as model structure or tracking/tuning process in one YAML file. It allows us to understand the whole process at a glance. Ivory is library-agnostic. You can use it with any machine learning library. Get started using the Quickstart. Quickstart Or take a look at the code below. import numpy as np from ivory.callbacks.results import Results from ivory.core.data import Data, Dataset, Datasets from ivory.core.run import Run from ivory.sklearn.estimator import Estimator from ivory.sklearn.metrics import Metrics data = Data() data.index = np.arange(30) data.input = np.arange(60).reshape(30, -1) data.target = np.sum(data.input, axis=1) data.fold = data.index % 4 datasets = Datasets(data, Dataset, fold=0) estimator = Estimator( model='sklearn.ensemble.RandomForestRegressor', n_estimators=10, max_depth=5, ) run = Run( name='first example', datasets=datasets, estimator=estimator, results=Results(), metrics=Metrics() ) run.start() [2] 2020-06-20 15:23:32 ( 417ms ) python3 ( 431ms ) [first example] mse=14.7 import matplotlib.pyplot as plt plt.scatter(run.results.val.target, run.results.val.output) [3] 2020-06-20 15:23:33 ( 242ms ) python3 ( 673ms ) <matplotlib.collections.PathCollection at 0x140448ad408>","title":"Ivory Documentation"},{"location":"quickstart/","text":"Quickstart Installation Install Ivory using pip . $ pip install ivory Ivory Client Ivory has the Client class that manages the workflow of machine learning. Let's create your first Client instance. In this quickstart, we are working with examples under the examples directory. Pass examples to the first argument of ivory.create_client() : import ivory client = ivory.create_client(\"examples\") client [3] 2020-06-20 15:23:33 ( 677ms ) python3 ( 1.37s ) Client(num_instances=2) The representation of the client shows that it has two instances. These instances can be accessed by index notation or dot notation . client[0] # or client['tracker'], or client.tracker [4] 2020-06-20 15:23:34 ( 3.00ms ) python3 ( 1.37s ) Tracker(tracking_uri='file:///C:/Users/daizu/Documents/github/ivory/examples/mlruns', artifact_location=None) The first instance is a Tracker instance that connects Ivory to MLFlow Tracking . Because a Client instance is an iterable, you can get all of the instances by applying list() to it. list(client) [5] 2020-06-20 15:23:34 ( 3.00ms ) python3 ( 1.37s ) ['tracker', 'tuner'] The second instance is named tuner . client.tuner [6] 2020-06-20 15:23:34 ( 3.00ms ) python3 ( 1.37s ) Tuner(storage='sqlite://', sampler=None, pruner=None, load_if_exists=True) A Tuner instance connects Ivory to Optuna: A hyperparameter optimization framework . We can customize these instances with a YAML file named client.yml under the working directory. In our case, the file just contains the minimal settings. File 1 client.yml client: tracker: tuner: Note If you don't need any customization, the YAML file for client is not required. If there is no file for client, Ivory creates a default client with a tracker and tuner. (So, the above file is unnecessary.) If you don't need a tracker and/or tuner, for example in debugging, use ivory.create_client(tracker=False, tuner=False) . Create NumPy data In this quickstart, we try to predict rectangles area from their width and height using PyTorch . First, prepare the data as NumPy arrays. In rectangle/data.py under the working directory, create_data() is defined. The ivory.create_client() automatically inserts the working directory to sys.path , so that we can import the module regardless of the current directory. Let's check the create_data() code and an example output: File 2 rectangle/data.py from dataclasses import dataclass import numpy as np import ivory.core.data from ivory.utils.fold import kfold_split def create_data(num_samples=1000): xy = 4 * np.random.rand(num_samples, 2) + 1 xy = xy.astype(np.float32) dx = 0.1 * (np.random.rand(num_samples) - 0.5) dy = 0.1 * (np.random.rand(num_samples) - 0.5) z = ((xy[:, 0] + dx) * (xy[:, 1] + dy)).astype(np.float32) return xy, z @dataclass(repr=False) class Data(ivory.core.data.Data): n_splits: int = 4 DATA = create_data(1000) # Shared by each run. def init(self): # Called from self.__post_init__() self.input, self.target = self.DATA self.index = np.arange(len(self.input)) # Extra fold for test data. self.fold = kfold_split(self.input, n_splits=self.n_splits + 1) # Creating dummy test data just for demonstration. is_test = self.fold == self.n_splits # Use an extra fold. self.fold[is_test] = -1 # -1 for test data. self.target = self.target.copy() # n_splits may be different among runs. self.target[is_test] = np.nan # Delete target for test data. self.target = self.target.reshape(-1, 1) # (sample, class) def transform(mode, input, target): return input, target.reshape(-1) import rectangle.data xy, z = rectangle.data.create_data(4) xy [8] 2020-06-20 15:23:34 ( 4.00ms ) python3 ( 1.38s ) array([[2.2804623, 1.3581246], [1.453418 , 3.905157 ], [4.4610925, 2.598797 ], [4.1255593, 3.9046824]], dtype=float32) z [9] 2020-06-20 15:23:34 ( 4.00ms ) python3 ( 1.39s ) array([ 3.1574094, 5.7998157, 11.530496 , 16.19652 ], dtype=float32) ivory.utils.fold.kfold_split() creates a fold array. import numpy as np from ivory.utils.fold import kfold_split kfold_split(np.arange(10), n_splits=3) [10] 2020-06-20 15:23:34 ( 4.00ms ) python3 ( 1.39s ) array([2, 1, 0, 2, 0, 2, 1, 1, 0, 0], dtype=int8) Set of Data Classes Ivory defines a set of base classes for data ( Data , Dataset , Datasets , and DataLoaders ) that user's custom classes can inherit. But now, we use the Data only. Now, we can get a rectangle.data.Data instance. data = rectangle.data.Data() data [11] 2020-06-20 15:23:34 ( 4.00ms ) python3 ( 1.40s ) Data(train_size=800, test_size=200) data.get(0) # get data of index = 0. [12] 2020-06-20 15:23:34 ( 5.00ms ) python3 ( 1.40s ) (0, array([2.0772183, 4.9461417], dtype=float32), array([10.418284], dtype=float32)) The returned value is a tuple of (index, input, target). Ivory always keeps data index so that we can know where a sample comes from. Define a model We use a simple MLP model. Note that the number of hidden layers and the size of each hidden layer are customizable. File 3 rectangle/torch.py import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] for in_features, out_features in zip([2] + hidden_sizes, hidden_sizes + [1]): layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) Parameter file for Run Ivory configures a run using a YAML file. Here is a full example. File 4 torch.yaml library: torch datasets: data: class: rectangle.data.Data n_splits: 4 dataset: fold: 0 model: class: rectangle.torch.Model hidden_sizes: [20, 30] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 results: metrics: monitor: metric: val_loss early_stopping: patience: 10 trainer: loss: mse batch_size: 10 epochs: 10 shuffle: true verbose: 2 Let's create a run calling the Client.create_run() . run = client.create_run('torch') run [14] 2020-06-20 15:23:34 ( 298ms ) python3 ( 1.97s ) [I 200620 15:23:34 tracker:48] A new experiment created with name: 'torch' Run(id='7b9e0effe2c84d1b90a4def042be41a1', name='run#0', num_instances=12) Note Client.create_run(<name>) creates an experiment named <name> if it hasn't existed yet. By clicking an icon ( ) in the above cell, you can see the log. Or you can directly create an experiment then make the experiment create a run: experiment = client . create_experiment ( 'torch' ) run = experiment . create_run () A Run instance have an attribute params that holds the parameters for the run. import yaml print(yaml.dump(run.params, sort_keys=False)) [15] 2020-06-20 15:23:34 ( 7.00ms ) python3 ( 1.98s ) run: datasets: data: class: rectangle.data.Data n_splits: 4 dataset: def: ivory.torch.data.Dataset fold: 0 class: ivory.core.data.Datasets model: class: rectangle.torch.Model hidden_sizes: - 20 - 30 optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 0.001 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 results: class: ivory.torch.results.Results metrics: class: ivory.torch.metrics.Metrics monitor: metric: val_loss class: ivory.callbacks.monitor.Monitor early_stopping: patience: 10 class: ivory.callbacks.early_stopping.EarlyStopping trainer: loss: mse batch_size: 10 epochs: 10 shuffle: true verbose: 2 class: ivory.torch.trainer.Trainer class: ivory.torch.run.Run name: run#0 id: 7b9e0effe2c84d1b90a4def042be41a1 experiment: name: torch class: ivory.core.base.Experiment id: '1' This is similar to the YAML file we read before, but has been slightly changed. Run and experiment keys are inserted. Run name is assigned by Ivory Client. Experiment ID and Run ID are assigned by MLFlow Tracking. Default classes are specified, for example the ivory.torch.trainer.Trainer class for a trainer instance. The Client.create_run() can take keyword arguments to modify these parameters: run = client.create_run( 'torch', fold=3, hidden_sizes=[40, 50, 60], ) print('[datasets]') print(yaml.dump(run.params['run']['datasets'], sort_keys=False)) print('[model]') print(yaml.dump(run.params['run']['model'], sort_keys=False)) [16] 2020-06-20 15:23:34 ( 43.0ms ) python3 ( 2.02s ) [datasets] data: class: rectangle.data.Data n_splits: 4 dataset: def: ivory.torch.data.Dataset fold: 3 class: ivory.core.data.Datasets [model] class: rectangle.torch.Model hidden_sizes: - 40 - 50 - 60 Train a model Once you got a run instance, then all you need is to start it. run = client.create_run('torch') # Back to the default settings. run.start() [17] 2020-06-20 15:23:34 ( 1.28s ) python3 ( 3.30s ) [epoch#0] loss=21.26 val_loss=7.393 lr=0.001 best [epoch#1] loss=7.776 val_loss=6.682 lr=0.001 best [epoch#2] loss=6.895 val_loss=5.736 lr=0.001 best [epoch#3] loss=6.053 val_loss=5.007 lr=0.001 best [epoch#4] loss=5.333 val_loss=4.328 lr=0.001 best [epoch#5] loss=4.529 val_loss=3.47 lr=0.001 best [epoch#6] loss=3.624 val_loss=2.702 lr=0.001 best [epoch#7] loss=2.863 val_loss=2.34 lr=0.001 best [epoch#8] loss=2.1 val_loss=1.446 lr=0.001 best [epoch#9] loss=1.599 val_loss=1.051 lr=0.001 best The history of metrics is saved as the history attribute of a run.metrics instance. run.metrics.history [18] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.31s ) Dict(['loss', 'val_loss', 'lr']) run.metrics.history.val_loss [19] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.31s ) {0: 7.392936539649964, 1: 6.681512886285782, 2: 5.736435055732727, 3: 5.0073208689689634, 4: 4.32755663394928, 5: 3.4701678693294524, 6: 2.7017099022865296, 7: 2.3395130217075346, 8: 1.4459084510803222, 9: 1.0509242355823516} Also the model output and target are automatically collected in a run.results instance. run.results [20] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.32s ) Results(['train', 'val']) run.results.val.output[:5] [21] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.32s ) array([[9.240469 ], [9.394586 ], [8.627342 ], [6.3360777], [4.2040167]], dtype=float32) run.results.val.target[:5] [22] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.32s ) array([[9.582911 ], [8.077265 ], [8.156037 ], [5.7045836], [3.401937 ]], dtype=float32) Test a model Testing a model is as simple as training. Just call Run.start('test') instead of a (default) 'train' argument. run.start('test') run.results [23] 2020-06-20 15:23:36 ( 38.0ms ) python3 ( 3.36s ) Results(['train', 'val', 'test']) As you can see, test results were added. run.results.test.output[:5] [24] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.37s ) array([[16.236954], [ 7.707018], [11.219795], [13.589512], [15.816204]], dtype=float32) Off course the target values for the test data are np.nan . run.results.test.target[:5] [25] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.37s ) array([[nan], [nan], [nan], [nan], [nan]], dtype=float32) Task for multiple runs Ivory implements a special run type called Task that controls multiple nested runs. A task is useful for parameter search or cross validation. task = client.create_task('torch') task [26] 2020-06-20 15:23:36 ( 49.0ms ) python3 ( 3.42s ) Task(id='d7765754a1774d4da508252590cd0ac9', name='task#0', num_instances=3) The Task class has two functions to generate multiple runs: Task.prodcut() and Task.chain() . These two functions have the same functionality as itertools of Python starndard library. Let's try to perform cross validation. runs = task.product(fold=range(4), verbose=0, epochs=3) runs [27] 2020-06-20 15:23:36 ( 3.00ms ) python3 ( 3.42s ) <generator object Task.product at 0x000001404B6F95C8> Like itertools 's functions, Task.prodcut() and Task.chain() return a generator, which yields runs that are configured by different parameters you specify. In this case, this generator will yield 4 runs with a fold number ranging from 0 to 3 for each. A task instance doesn't start any training by itself. In addition, you can pass fixed parameters to update the original parameters in the YAML file. Then start 4 runs by a for loop including run.start('both') . Here 'both' means successive test after training. for run in runs: run.start('both') [28] 2020-06-20 15:23:36 ( 2.14s ) python3 ( 5.56s ) [run#3] epochs=3 fold=0 [run#4] epochs=3 fold=1 [run#5] epochs=3 fold=2 [run#6] epochs=3 fold=3 Collect runs Our client has a Tracker instance. It stores the state of runs in background using MLFlow Tracking. The Client provides several functions to access the stored runs. For example, Client.search_run_ids() returns a generator that yields Run ID assigned by MLFlow Tracking. # A helper function. def print_run_info(run_ids): for run_id in run_ids: print(run_id[:5], client.get_run_name(run_id)) [29] 2020-06-20 15:23:38 ( 3.00ms ) python3 ( 5.56s ) run_ids = client.search_run_ids('torch') # Yields all runs of `torch`. print_run_info(run_ids) [30] 2020-06-20 15:23:38 ( 81.7ms ) python3 ( 5.64s ) 05ac5 run#6 d9d3d run#5 88e6b run#4 b9b09 run#3 d7765 task#0 63398 run#2 02178 run#1 7b9e0 run#0 For filtering, add key-value pairs. # If `exclude_parent` is True, parent runs are excluded. run_ids = client.search_run_ids('torch', fold=0, exclude_parent=True) print_run_info(run_ids) [31] 2020-06-20 15:23:38 ( 162ms ) python3 ( 5.80s ) b9b09 run#3 63398 run#2 7b9e0 run#0 # If `parent_run_id` is specified, nested runs with the parent are returned. run_ids = client.search_run_ids('torch', parent_run_id=task.id) print_run_info(run_ids) [32] 2020-06-20 15:23:38 ( 45.0ms ) python3 ( 5.85s ) 05ac5 run#6 d9d3d run#5 88e6b run#4 b9b09 run#3 Client.get_run_id() and Client.get_run_ids() fetch Run ID from run name, more strictly, a key-value pair of (run class name in lower case, run number). run_ids = [client.get_run_id('torch', run=0), client.get_run_id('torch', task=0)] print_run_info(run_ids) [33] 2020-06-20 15:23:38 ( 53.0ms ) python3 ( 5.90s ) 7b9e0 run#0 d7765 task#0 run_ids = client.get_run_ids('torch', run=range(2, 4)) print_run_info(run_ids) [34] 2020-06-20 15:23:38 ( 54.0ms ) python3 ( 5.96s ) 63398 run#2 b9b09 run#3 Load runs and results A Client instance can load runs. First select Run ID(s) to load. We want to perform cross validation here, so that we need a run collection created by the task#0 . In this case, we can use Client.get_nested_run_ids() . Why don't we use Client.search_run_ids() as we did above? Because we don't have an easy way to get a very long Run ID after we restart a Python session and lose the Task instance. On the other hand, a run name is easy to manage and write. # Assume that we restarted a session so we have no run instances now. run_ids = list(client.get_nested_run_ids('torch', task=0)) print_run_info(run_ids) [35] 2020-06-20 15:23:38 ( 76.7ms ) python3 ( 6.03s ) 05ac5 run#6 d9d3d run#5 88e6b run#4 b9b09 run#3 Let's load the latest run. run = client.load_run(run_ids[0]) run [36] 2020-06-20 15:23:38 ( 47.0ms ) python3 ( 6.08s ) Run(id='05ac5e6f875e41688e74f9f89108a55a', name='run#6', num_instances=11) Note that the Client.load_run() doesn't require an experiment name because Run ID is UUID . As you expected, the fold number is 3. run.datasets.fold [37] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.08s ) 3 By loading a run, we obtain the pretrained model. run.model.eval() [38] 2020-06-20 15:23:39 ( 5.00ms ) python3 ( 6.09s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=20, bias=True) (1): Linear(in_features=20, out_features=30, bias=True) (2): Linear(in_features=30, out_features=1, bias=True) ) ) import torch index, input, target = run.datasets.val[:5] with torch.no_grad(): output = run.model(torch.tensor(input)) print('[output]') print(output.numpy()) print('[target]') print(target) [39] 2020-06-20 15:23:39 ( 9.00ms ) python3 ( 6.10s ) [output] [[ 5.89771 ] [11.323543] [13.761651] [ 8.328902] [14.471639]] [target] [[ 2.709134] [10.005989] [15.843957] [ 5.110198] [17.774628]] If you don't need a whole run instance, Client.load_instance() is a better choice to save time and memory. results = client.load_instance(run_ids[0], 'results') results [40] 2020-06-20 15:23:39 ( 27.0ms ) python3 ( 6.12s ) Results(['train', 'val', 'test']) for mode, result in results.items(): print(mode, result.output.shape) [41] 2020-06-20 15:23:39 ( 8.00ms ) python3 ( 6.13s ) train (600, 1) val (200, 1) test (200, 1) For cross validation, we need 4 runs. In order to load multiple run's results at the same time, the Ivory Client provides a convenient function. results = client.load_results(run_ids, verbose=False) # No progress bar. results [42] 2020-06-20 15:23:39 ( 92.8ms ) python3 ( 6.23s ) Results(['val', 'test']) for mode, result in results.items(): print(mode, result.output.shape) [43] 2020-06-20 15:23:39 ( 6.00ms ) python3 ( 6.23s ) val (800, 1) test (800, 1) Note Client.load_results() drops train data for saving memory. The lengths of the validation and test data are both 800 (200 times 4). But be careful about the test data. The length of unique samples should be 200 (one fold size). import numpy as np len(np.unique(results.val.index)), len(np.unique(results.test.index)) [44] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.24s ) (800, 200) Usually, duplicated samples in test data are averaged for ensembling. Results.mean() performs this mean reduction and returns a newly created Rusults instance. reduced_results = results.mean() for mode, result in reduced_results.items(): print(mode, result.output.shape) [45] 2020-06-20 15:23:39 ( 14.0ms ) python3 ( 6.25s ) val (800, 1) test (200, 1) Compare these two results. index = results.test.index index_0 = index[0] x = results.test.output[index == index_0] print('[results]') print(x) print(\"-> mean:\", np.mean(x)) index = reduced_results.test.index x = reduced_results.test.output[index == index_0] print('[reduced_results]') print(x) [46] 2020-06-20 15:23:39 ( 10.0ms ) python3 ( 6.26s ) [results] [[14.397777] [13.893448] [13.043155] [13.440302]] -> mean: 13.69367 [reduced_results] [[13.69367]] For convenience, The Client.load_results() has a reduction keyword argument. results = client.load_results(run_ids, reduction='mean', verbose=False) results [47] 2020-06-20 15:23:39 ( 84.8ms ) python3 ( 6.34s ) Results(['val', 'test']) for mode, result in results.items(): print(mode, result.output.shape) [48] 2020-06-20 15:23:39 ( 6.00ms ) python3 ( 6.35s ) val (800, 1) test (200, 1) The cross validation (CV) score can be calculated as follows: true = results.val.target pred = results.val.output np.mean(np.sqrt((true - pred) ** 2)) # Use any function for your metric. [49] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.35s ) 2.1015716 And we got prediction for the test data using 4 MLP models. results.test.output[:5] [50] 2020-06-20 15:23:39 ( 5.00ms ) python3 ( 6.36s ) array([[13.69367 ], [ 9.086484], [10.563665], [12.643805], [13.991818]], dtype=float32) Summary In this quickstart, we learned how to use the Ivory library to perform machine learning workflow. For more details see the Tutorial.","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#installation","text":"Install Ivory using pip . $ pip install ivory","title":"Installation"},{"location":"quickstart/#ivory-client","text":"Ivory has the Client class that manages the workflow of machine learning. Let's create your first Client instance. In this quickstart, we are working with examples under the examples directory. Pass examples to the first argument of ivory.create_client() : import ivory client = ivory.create_client(\"examples\") client [3] 2020-06-20 15:23:33 ( 677ms ) python3 ( 1.37s ) Client(num_instances=2) The representation of the client shows that it has two instances. These instances can be accessed by index notation or dot notation . client[0] # or client['tracker'], or client.tracker [4] 2020-06-20 15:23:34 ( 3.00ms ) python3 ( 1.37s ) Tracker(tracking_uri='file:///C:/Users/daizu/Documents/github/ivory/examples/mlruns', artifact_location=None) The first instance is a Tracker instance that connects Ivory to MLFlow Tracking . Because a Client instance is an iterable, you can get all of the instances by applying list() to it. list(client) [5] 2020-06-20 15:23:34 ( 3.00ms ) python3 ( 1.37s ) ['tracker', 'tuner'] The second instance is named tuner . client.tuner [6] 2020-06-20 15:23:34 ( 3.00ms ) python3 ( 1.37s ) Tuner(storage='sqlite://', sampler=None, pruner=None, load_if_exists=True) A Tuner instance connects Ivory to Optuna: A hyperparameter optimization framework . We can customize these instances with a YAML file named client.yml under the working directory. In our case, the file just contains the minimal settings. File 1 client.yml client: tracker: tuner: Note If you don't need any customization, the YAML file for client is not required. If there is no file for client, Ivory creates a default client with a tracker and tuner. (So, the above file is unnecessary.) If you don't need a tracker and/or tuner, for example in debugging, use ivory.create_client(tracker=False, tuner=False) .","title":"Ivory Client"},{"location":"quickstart/#create-numpy-data","text":"In this quickstart, we try to predict rectangles area from their width and height using PyTorch . First, prepare the data as NumPy arrays. In rectangle/data.py under the working directory, create_data() is defined. The ivory.create_client() automatically inserts the working directory to sys.path , so that we can import the module regardless of the current directory. Let's check the create_data() code and an example output: File 2 rectangle/data.py from dataclasses import dataclass import numpy as np import ivory.core.data from ivory.utils.fold import kfold_split def create_data(num_samples=1000): xy = 4 * np.random.rand(num_samples, 2) + 1 xy = xy.astype(np.float32) dx = 0.1 * (np.random.rand(num_samples) - 0.5) dy = 0.1 * (np.random.rand(num_samples) - 0.5) z = ((xy[:, 0] + dx) * (xy[:, 1] + dy)).astype(np.float32) return xy, z @dataclass(repr=False) class Data(ivory.core.data.Data): n_splits: int = 4 DATA = create_data(1000) # Shared by each run. def init(self): # Called from self.__post_init__() self.input, self.target = self.DATA self.index = np.arange(len(self.input)) # Extra fold for test data. self.fold = kfold_split(self.input, n_splits=self.n_splits + 1) # Creating dummy test data just for demonstration. is_test = self.fold == self.n_splits # Use an extra fold. self.fold[is_test] = -1 # -1 for test data. self.target = self.target.copy() # n_splits may be different among runs. self.target[is_test] = np.nan # Delete target for test data. self.target = self.target.reshape(-1, 1) # (sample, class) def transform(mode, input, target): return input, target.reshape(-1) import rectangle.data xy, z = rectangle.data.create_data(4) xy [8] 2020-06-20 15:23:34 ( 4.00ms ) python3 ( 1.38s ) array([[2.2804623, 1.3581246], [1.453418 , 3.905157 ], [4.4610925, 2.598797 ], [4.1255593, 3.9046824]], dtype=float32) z [9] 2020-06-20 15:23:34 ( 4.00ms ) python3 ( 1.39s ) array([ 3.1574094, 5.7998157, 11.530496 , 16.19652 ], dtype=float32) ivory.utils.fold.kfold_split() creates a fold array. import numpy as np from ivory.utils.fold import kfold_split kfold_split(np.arange(10), n_splits=3) [10] 2020-06-20 15:23:34 ( 4.00ms ) python3 ( 1.39s ) array([2, 1, 0, 2, 0, 2, 1, 1, 0, 0], dtype=int8)","title":"Create NumPy data"},{"location":"quickstart/#set-of-data-classes","text":"Ivory defines a set of base classes for data ( Data , Dataset , Datasets , and DataLoaders ) that user's custom classes can inherit. But now, we use the Data only. Now, we can get a rectangle.data.Data instance. data = rectangle.data.Data() data [11] 2020-06-20 15:23:34 ( 4.00ms ) python3 ( 1.40s ) Data(train_size=800, test_size=200) data.get(0) # get data of index = 0. [12] 2020-06-20 15:23:34 ( 5.00ms ) python3 ( 1.40s ) (0, array([2.0772183, 4.9461417], dtype=float32), array([10.418284], dtype=float32)) The returned value is a tuple of (index, input, target). Ivory always keeps data index so that we can know where a sample comes from.","title":"Set of Data Classes"},{"location":"quickstart/#define-a-model","text":"We use a simple MLP model. Note that the number of hidden layers and the size of each hidden layer are customizable. File 3 rectangle/torch.py import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] for in_features, out_features in zip([2] + hidden_sizes, hidden_sizes + [1]): layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x)","title":"Define a model"},{"location":"quickstart/#parameter-file-for-run","text":"Ivory configures a run using a YAML file. Here is a full example. File 4 torch.yaml library: torch datasets: data: class: rectangle.data.Data n_splits: 4 dataset: fold: 0 model: class: rectangle.torch.Model hidden_sizes: [20, 30] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 results: metrics: monitor: metric: val_loss early_stopping: patience: 10 trainer: loss: mse batch_size: 10 epochs: 10 shuffle: true verbose: 2 Let's create a run calling the Client.create_run() . run = client.create_run('torch') run [14] 2020-06-20 15:23:34 ( 298ms ) python3 ( 1.97s ) [I 200620 15:23:34 tracker:48] A new experiment created with name: 'torch' Run(id='7b9e0effe2c84d1b90a4def042be41a1', name='run#0', num_instances=12) Note Client.create_run(<name>) creates an experiment named <name> if it hasn't existed yet. By clicking an icon ( ) in the above cell, you can see the log. Or you can directly create an experiment then make the experiment create a run: experiment = client . create_experiment ( 'torch' ) run = experiment . create_run () A Run instance have an attribute params that holds the parameters for the run. import yaml print(yaml.dump(run.params, sort_keys=False)) [15] 2020-06-20 15:23:34 ( 7.00ms ) python3 ( 1.98s ) run: datasets: data: class: rectangle.data.Data n_splits: 4 dataset: def: ivory.torch.data.Dataset fold: 0 class: ivory.core.data.Datasets model: class: rectangle.torch.Model hidden_sizes: - 20 - 30 optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 0.001 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 results: class: ivory.torch.results.Results metrics: class: ivory.torch.metrics.Metrics monitor: metric: val_loss class: ivory.callbacks.monitor.Monitor early_stopping: patience: 10 class: ivory.callbacks.early_stopping.EarlyStopping trainer: loss: mse batch_size: 10 epochs: 10 shuffle: true verbose: 2 class: ivory.torch.trainer.Trainer class: ivory.torch.run.Run name: run#0 id: 7b9e0effe2c84d1b90a4def042be41a1 experiment: name: torch class: ivory.core.base.Experiment id: '1' This is similar to the YAML file we read before, but has been slightly changed. Run and experiment keys are inserted. Run name is assigned by Ivory Client. Experiment ID and Run ID are assigned by MLFlow Tracking. Default classes are specified, for example the ivory.torch.trainer.Trainer class for a trainer instance. The Client.create_run() can take keyword arguments to modify these parameters: run = client.create_run( 'torch', fold=3, hidden_sizes=[40, 50, 60], ) print('[datasets]') print(yaml.dump(run.params['run']['datasets'], sort_keys=False)) print('[model]') print(yaml.dump(run.params['run']['model'], sort_keys=False)) [16] 2020-06-20 15:23:34 ( 43.0ms ) python3 ( 2.02s ) [datasets] data: class: rectangle.data.Data n_splits: 4 dataset: def: ivory.torch.data.Dataset fold: 3 class: ivory.core.data.Datasets [model] class: rectangle.torch.Model hidden_sizes: - 40 - 50 - 60","title":"Parameter file for Run"},{"location":"quickstart/#train-a-model","text":"Once you got a run instance, then all you need is to start it. run = client.create_run('torch') # Back to the default settings. run.start() [17] 2020-06-20 15:23:34 ( 1.28s ) python3 ( 3.30s ) [epoch#0] loss=21.26 val_loss=7.393 lr=0.001 best [epoch#1] loss=7.776 val_loss=6.682 lr=0.001 best [epoch#2] loss=6.895 val_loss=5.736 lr=0.001 best [epoch#3] loss=6.053 val_loss=5.007 lr=0.001 best [epoch#4] loss=5.333 val_loss=4.328 lr=0.001 best [epoch#5] loss=4.529 val_loss=3.47 lr=0.001 best [epoch#6] loss=3.624 val_loss=2.702 lr=0.001 best [epoch#7] loss=2.863 val_loss=2.34 lr=0.001 best [epoch#8] loss=2.1 val_loss=1.446 lr=0.001 best [epoch#9] loss=1.599 val_loss=1.051 lr=0.001 best The history of metrics is saved as the history attribute of a run.metrics instance. run.metrics.history [18] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.31s ) Dict(['loss', 'val_loss', 'lr']) run.metrics.history.val_loss [19] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.31s ) {0: 7.392936539649964, 1: 6.681512886285782, 2: 5.736435055732727, 3: 5.0073208689689634, 4: 4.32755663394928, 5: 3.4701678693294524, 6: 2.7017099022865296, 7: 2.3395130217075346, 8: 1.4459084510803222, 9: 1.0509242355823516} Also the model output and target are automatically collected in a run.results instance. run.results [20] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.32s ) Results(['train', 'val']) run.results.val.output[:5] [21] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.32s ) array([[9.240469 ], [9.394586 ], [8.627342 ], [6.3360777], [4.2040167]], dtype=float32) run.results.val.target[:5] [22] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.32s ) array([[9.582911 ], [8.077265 ], [8.156037 ], [5.7045836], [3.401937 ]], dtype=float32)","title":"Train a model"},{"location":"quickstart/#test-a-model","text":"Testing a model is as simple as training. Just call Run.start('test') instead of a (default) 'train' argument. run.start('test') run.results [23] 2020-06-20 15:23:36 ( 38.0ms ) python3 ( 3.36s ) Results(['train', 'val', 'test']) As you can see, test results were added. run.results.test.output[:5] [24] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.37s ) array([[16.236954], [ 7.707018], [11.219795], [13.589512], [15.816204]], dtype=float32) Off course the target values for the test data are np.nan . run.results.test.target[:5] [25] 2020-06-20 15:23:36 ( 4.00ms ) python3 ( 3.37s ) array([[nan], [nan], [nan], [nan], [nan]], dtype=float32)","title":"Test a model"},{"location":"quickstart/#task-for-multiple-runs","text":"Ivory implements a special run type called Task that controls multiple nested runs. A task is useful for parameter search or cross validation. task = client.create_task('torch') task [26] 2020-06-20 15:23:36 ( 49.0ms ) python3 ( 3.42s ) Task(id='d7765754a1774d4da508252590cd0ac9', name='task#0', num_instances=3) The Task class has two functions to generate multiple runs: Task.prodcut() and Task.chain() . These two functions have the same functionality as itertools of Python starndard library. Let's try to perform cross validation. runs = task.product(fold=range(4), verbose=0, epochs=3) runs [27] 2020-06-20 15:23:36 ( 3.00ms ) python3 ( 3.42s ) <generator object Task.product at 0x000001404B6F95C8> Like itertools 's functions, Task.prodcut() and Task.chain() return a generator, which yields runs that are configured by different parameters you specify. In this case, this generator will yield 4 runs with a fold number ranging from 0 to 3 for each. A task instance doesn't start any training by itself. In addition, you can pass fixed parameters to update the original parameters in the YAML file. Then start 4 runs by a for loop including run.start('both') . Here 'both' means successive test after training. for run in runs: run.start('both') [28] 2020-06-20 15:23:36 ( 2.14s ) python3 ( 5.56s ) [run#3] epochs=3 fold=0 [run#4] epochs=3 fold=1 [run#5] epochs=3 fold=2 [run#6] epochs=3 fold=3","title":"Task for multiple runs"},{"location":"quickstart/#collect-runs","text":"Our client has a Tracker instance. It stores the state of runs in background using MLFlow Tracking. The Client provides several functions to access the stored runs. For example, Client.search_run_ids() returns a generator that yields Run ID assigned by MLFlow Tracking. # A helper function. def print_run_info(run_ids): for run_id in run_ids: print(run_id[:5], client.get_run_name(run_id)) [29] 2020-06-20 15:23:38 ( 3.00ms ) python3 ( 5.56s ) run_ids = client.search_run_ids('torch') # Yields all runs of `torch`. print_run_info(run_ids) [30] 2020-06-20 15:23:38 ( 81.7ms ) python3 ( 5.64s ) 05ac5 run#6 d9d3d run#5 88e6b run#4 b9b09 run#3 d7765 task#0 63398 run#2 02178 run#1 7b9e0 run#0 For filtering, add key-value pairs. # If `exclude_parent` is True, parent runs are excluded. run_ids = client.search_run_ids('torch', fold=0, exclude_parent=True) print_run_info(run_ids) [31] 2020-06-20 15:23:38 ( 162ms ) python3 ( 5.80s ) b9b09 run#3 63398 run#2 7b9e0 run#0 # If `parent_run_id` is specified, nested runs with the parent are returned. run_ids = client.search_run_ids('torch', parent_run_id=task.id) print_run_info(run_ids) [32] 2020-06-20 15:23:38 ( 45.0ms ) python3 ( 5.85s ) 05ac5 run#6 d9d3d run#5 88e6b run#4 b9b09 run#3 Client.get_run_id() and Client.get_run_ids() fetch Run ID from run name, more strictly, a key-value pair of (run class name in lower case, run number). run_ids = [client.get_run_id('torch', run=0), client.get_run_id('torch', task=0)] print_run_info(run_ids) [33] 2020-06-20 15:23:38 ( 53.0ms ) python3 ( 5.90s ) 7b9e0 run#0 d7765 task#0 run_ids = client.get_run_ids('torch', run=range(2, 4)) print_run_info(run_ids) [34] 2020-06-20 15:23:38 ( 54.0ms ) python3 ( 5.96s ) 63398 run#2 b9b09 run#3","title":"Collect runs"},{"location":"quickstart/#load-runs-and-results","text":"A Client instance can load runs. First select Run ID(s) to load. We want to perform cross validation here, so that we need a run collection created by the task#0 . In this case, we can use Client.get_nested_run_ids() . Why don't we use Client.search_run_ids() as we did above? Because we don't have an easy way to get a very long Run ID after we restart a Python session and lose the Task instance. On the other hand, a run name is easy to manage and write. # Assume that we restarted a session so we have no run instances now. run_ids = list(client.get_nested_run_ids('torch', task=0)) print_run_info(run_ids) [35] 2020-06-20 15:23:38 ( 76.7ms ) python3 ( 6.03s ) 05ac5 run#6 d9d3d run#5 88e6b run#4 b9b09 run#3 Let's load the latest run. run = client.load_run(run_ids[0]) run [36] 2020-06-20 15:23:38 ( 47.0ms ) python3 ( 6.08s ) Run(id='05ac5e6f875e41688e74f9f89108a55a', name='run#6', num_instances=11) Note that the Client.load_run() doesn't require an experiment name because Run ID is UUID . As you expected, the fold number is 3. run.datasets.fold [37] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.08s ) 3 By loading a run, we obtain the pretrained model. run.model.eval() [38] 2020-06-20 15:23:39 ( 5.00ms ) python3 ( 6.09s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=20, bias=True) (1): Linear(in_features=20, out_features=30, bias=True) (2): Linear(in_features=30, out_features=1, bias=True) ) ) import torch index, input, target = run.datasets.val[:5] with torch.no_grad(): output = run.model(torch.tensor(input)) print('[output]') print(output.numpy()) print('[target]') print(target) [39] 2020-06-20 15:23:39 ( 9.00ms ) python3 ( 6.10s ) [output] [[ 5.89771 ] [11.323543] [13.761651] [ 8.328902] [14.471639]] [target] [[ 2.709134] [10.005989] [15.843957] [ 5.110198] [17.774628]] If you don't need a whole run instance, Client.load_instance() is a better choice to save time and memory. results = client.load_instance(run_ids[0], 'results') results [40] 2020-06-20 15:23:39 ( 27.0ms ) python3 ( 6.12s ) Results(['train', 'val', 'test']) for mode, result in results.items(): print(mode, result.output.shape) [41] 2020-06-20 15:23:39 ( 8.00ms ) python3 ( 6.13s ) train (600, 1) val (200, 1) test (200, 1) For cross validation, we need 4 runs. In order to load multiple run's results at the same time, the Ivory Client provides a convenient function. results = client.load_results(run_ids, verbose=False) # No progress bar. results [42] 2020-06-20 15:23:39 ( 92.8ms ) python3 ( 6.23s ) Results(['val', 'test']) for mode, result in results.items(): print(mode, result.output.shape) [43] 2020-06-20 15:23:39 ( 6.00ms ) python3 ( 6.23s ) val (800, 1) test (800, 1) Note Client.load_results() drops train data for saving memory. The lengths of the validation and test data are both 800 (200 times 4). But be careful about the test data. The length of unique samples should be 200 (one fold size). import numpy as np len(np.unique(results.val.index)), len(np.unique(results.test.index)) [44] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.24s ) (800, 200) Usually, duplicated samples in test data are averaged for ensembling. Results.mean() performs this mean reduction and returns a newly created Rusults instance. reduced_results = results.mean() for mode, result in reduced_results.items(): print(mode, result.output.shape) [45] 2020-06-20 15:23:39 ( 14.0ms ) python3 ( 6.25s ) val (800, 1) test (200, 1) Compare these two results. index = results.test.index index_0 = index[0] x = results.test.output[index == index_0] print('[results]') print(x) print(\"-> mean:\", np.mean(x)) index = reduced_results.test.index x = reduced_results.test.output[index == index_0] print('[reduced_results]') print(x) [46] 2020-06-20 15:23:39 ( 10.0ms ) python3 ( 6.26s ) [results] [[14.397777] [13.893448] [13.043155] [13.440302]] -> mean: 13.69367 [reduced_results] [[13.69367]] For convenience, The Client.load_results() has a reduction keyword argument. results = client.load_results(run_ids, reduction='mean', verbose=False) results [47] 2020-06-20 15:23:39 ( 84.8ms ) python3 ( 6.34s ) Results(['val', 'test']) for mode, result in results.items(): print(mode, result.output.shape) [48] 2020-06-20 15:23:39 ( 6.00ms ) python3 ( 6.35s ) val (800, 1) test (200, 1) The cross validation (CV) score can be calculated as follows: true = results.val.target pred = results.val.output np.mean(np.sqrt((true - pred) ** 2)) # Use any function for your metric. [49] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.35s ) 2.1015716 And we got prediction for the test data using 4 MLP models. results.test.output[:5] [50] 2020-06-20 15:23:39 ( 5.00ms ) python3 ( 6.36s ) array([[13.69367 ], [ 9.086484], [10.563665], [12.643805], [13.991818]], dtype=float32)","title":"Load runs and results"},{"location":"quickstart/#summary","text":"In this quickstart, we learned how to use the Ivory library to perform machine learning workflow. For more details see the Tutorial.","title":"Summary"},{"location":"api/ivory.callbacks.early_stopping/","text":"MODULE IVORY.CALLBACKS . EARLY_STOPPING </> Early stopping when a monitored metric has stopped improving. Classes EarlyStopping \u2014 Early stops a training loop when a monitored metric has stopped improving. </> dataclass ivory.callbacks.early_stopping . EarlyStopping ( patience ) </> Bases ivory.core.state.State Early stops a training loop when a monitored metric has stopped improving. Parameters patience (int) \u2014 Number of epochs with no improvement after which training will be stopped. Attributes patience (int) \u2014 Number of epochs with no improvement after which training will be stopped. wait (int) \u2014 Number of continuous epochs with no imporovement. Raises EarlyStopped \u2014 When ealry stopping occurs.","title":"ivory.callbacks.early_stopping"},{"location":"api/ivory.callbacks.early_stopping/#ivorycallbacksearly_stopping","text":"</> Early stopping when a monitored metric has stopped improving. Classes EarlyStopping \u2014 Early stops a training loop when a monitored metric has stopped improving. </> dataclass","title":"ivory.callbacks.early_stopping"},{"location":"api/ivory.callbacks.early_stopping/#ivorycallbacksearly_stoppingearlystopping","text":"</> Bases ivory.core.state.State Early stops a training loop when a monitored metric has stopped improving. Parameters patience (int) \u2014 Number of epochs with no improvement after which training will be stopped. Attributes patience (int) \u2014 Number of epochs with no improvement after which training will be stopped. wait (int) \u2014 Number of continuous epochs with no imporovement. Raises EarlyStopped \u2014 When ealry stopping occurs.","title":"ivory.callbacks.early_stopping.EarlyStopping"},{"location":"api/ivory.callbacks/","text":"PACKAGE IVORY. CALLBACKS </> MODULE IVORY.CALLBACKS . EARLY_STOPPING </> Early stopping when a monitored metric has stopped improving. Classes EarlyStopping \u2014 Early stops a training loop when a monitored metric has stopped improving. </> MODULE IVORY.CALLBACKS . METRICS </> Metrics to record scores while training. Classes Metrics \u2014 Metrics object. </> BatchMetrics \u2014 Metrics object. </> MODULE IVORY.CALLBACKS . PRUNING </> Pruning class to prune unpromising trials. Classes Pruning \u2014 Callback to prune unpromising trials. </> MODULE IVORY.CALLBACKS . RESULTS </> A container to store training, validation and test results. Classes Results \u2014 Results callback stores training, validation and test results. </> BatchResults \u2014 Results callback stores training, validation and test results. </> Functions concatenate ( iterable , callback , modes , reduction ) ( Results ) \u2014 Returns a concatenated Results. </>","title":"ivory.callbacks"},{"location":"api/ivory.callbacks/#ivorycallbacks","text":"</> MODULE","title":"ivory.callbacks"},{"location":"api/ivory.callbacks/#ivorycallbacksearly_stopping","text":"</> Early stopping when a monitored metric has stopped improving. Classes EarlyStopping \u2014 Early stops a training loop when a monitored metric has stopped improving. </> MODULE","title":"ivory.callbacks.early_stopping"},{"location":"api/ivory.callbacks/#ivorycallbacksmetrics","text":"</> Metrics to record scores while training. Classes Metrics \u2014 Metrics object. </> BatchMetrics \u2014 Metrics object. </> MODULE","title":"ivory.callbacks.metrics"},{"location":"api/ivory.callbacks/#ivorycallbackspruning","text":"</> Pruning class to prune unpromising trials. Classes Pruning \u2014 Callback to prune unpromising trials. </> MODULE","title":"ivory.callbacks.pruning"},{"location":"api/ivory.callbacks/#ivorycallbacksresults","text":"</> A container to store training, validation and test results. Classes Results \u2014 Results callback stores training, validation and test results. </> BatchResults \u2014 Results callback stores training, validation and test results. </> Functions concatenate ( iterable , callback , modes , reduction ) ( Results ) \u2014 Returns a concatenated Results. </>","title":"ivory.callbacks.results"},{"location":"api/ivory.callbacks.metrics/","text":"MODULE IVORY.CALLBACKS . METRICS </> Metrics to record scores while training. Classes Metrics \u2014 Metrics object. </> BatchMetrics \u2014 Metrics object. </> class ivory.callbacks.metrics . Metrics ( **kwargs ) </> Bases ivory.core.collections.Dict ivory.core.state.State Metrics object. Parameters **kwargs \u2014 Attributes epoch \u2014 history \u2014 metrics_fn \u2014 Methods metrics_dict ( run ) (dict(str: any)) \u2014 Returns an extra custom metrics dictionary. </> method metrics_dict ( run ) \u2192 dict(str: any) </> Returns an extra custom metrics dictionary. Parameters run ( Run ) \u2014 class ivory.callbacks.metrics . BatchMetrics ( **kwargs ) </> Bases ivory.callbacks.metrics.Metrics ivory.core.collections.Dict ivory.core.state.State Metrics object. Parameters **kwargs \u2014 Attributes epoch \u2014 history \u2014 losses (list of float) \u2014 metrics_fn \u2014 Methods metrics_dict ( run ) (dict(str: any)) \u2014 Returns an extra custom metrics dictionary. </> method metrics_dict ( run ) \u2192 dict(str: any) </> Returns an extra custom metrics dictionary. Parameters run ( Run ) \u2014","title":"ivory.callbacks.metrics"},{"location":"api/ivory.callbacks.metrics/#ivorycallbacksmetrics","text":"</> Metrics to record scores while training. Classes Metrics \u2014 Metrics object. </> BatchMetrics \u2014 Metrics object. </> class","title":"ivory.callbacks.metrics"},{"location":"api/ivory.callbacks.metrics/#ivorycallbacksmetricsmetrics","text":"</> Bases ivory.core.collections.Dict ivory.core.state.State Metrics object. Parameters **kwargs \u2014 Attributes epoch \u2014 history \u2014 metrics_fn \u2014 Methods metrics_dict ( run ) (dict(str: any)) \u2014 Returns an extra custom metrics dictionary. </> method","title":"ivory.callbacks.metrics.Metrics"},{"location":"api/ivory.callbacks.metrics/#ivorycallbacksmetricsmetricsmetrics_dict","text":"</> Returns an extra custom metrics dictionary. Parameters run ( Run ) \u2014 class","title":"ivory.callbacks.metrics.Metrics.metrics_dict"},{"location":"api/ivory.callbacks.metrics/#ivorycallbacksmetricsbatchmetrics","text":"</> Bases ivory.callbacks.metrics.Metrics ivory.core.collections.Dict ivory.core.state.State Metrics object. Parameters **kwargs \u2014 Attributes epoch \u2014 history \u2014 losses (list of float) \u2014 metrics_fn \u2014 Methods metrics_dict ( run ) (dict(str: any)) \u2014 Returns an extra custom metrics dictionary. </> method","title":"ivory.callbacks.metrics.BatchMetrics"},{"location":"api/ivory.callbacks.metrics/#ivorycallbacksmetricsmetricsmetrics_dict_1","text":"</> Returns an extra custom metrics dictionary. Parameters run ( Run ) \u2014","title":"ivory.callbacks.metrics.Metrics.metrics_dict"},{"location":"api/ivory.callbacks.pruning/","text":"MODULE IVORY.CALLBACKS . PRUNING </> Pruning class to prune unpromising trials. Classes Pruning \u2014 Callback to prune unpromising trials. </> dataclass ivory.callbacks.pruning . Pruning ( trial=None , metric='' ) </> Callback to prune unpromising trials. Parameters trial (Trial, optional) \u2014 A Trial corresponding to the current evaluation of the objective function. metric (str, optional) \u2014 An evaluation metric for pruning, e.g., val_loss Attributes metric (str) \u2014 An evaluation metric for pruning, e.g., val_loss trial (Trial, optional) \u2014 A Trial corresponding to the current evaluation of the objective function.","title":"ivory.callbacks.pruning"},{"location":"api/ivory.callbacks.pruning/#ivorycallbackspruning","text":"</> Pruning class to prune unpromising trials. Classes Pruning \u2014 Callback to prune unpromising trials. </> dataclass","title":"ivory.callbacks.pruning"},{"location":"api/ivory.callbacks.pruning/#ivorycallbackspruningpruning","text":"</> Callback to prune unpromising trials. Parameters trial (Trial, optional) \u2014 A Trial corresponding to the current evaluation of the objective function. metric (str, optional) \u2014 An evaluation metric for pruning, e.g., val_loss Attributes metric (str) \u2014 An evaluation metric for pruning, e.g., val_loss trial (Trial, optional) \u2014 A Trial corresponding to the current evaluation of the objective function.","title":"ivory.callbacks.pruning.Pruning"},{"location":"api/ivory.callbacks.results/","text":"MODULE IVORY.CALLBACKS . RESULTS </> A container to store training, validation and test results. Classes Results \u2014 Results callback stores training, validation and test results. </> BatchResults \u2014 Results callback stores training, validation and test results. </> Functions concatenate ( iterable , callback , modes , reduction ) ( Results ) \u2014 Returns a concatenated Results. </> class ivory.callbacks.results . Results ( ) </> Bases ivory.core.collections.Dict ivory.core.state.State Results callback stores training, validation and test results. Each result is ivory.core.collections.Dict type that has index , output , and target array. To get target array of validation, use target = results.val.target Attributes index \u2014 output \u2014 target \u2014 test (Dict) \u2014 Test results. train (Dict) \u2014 Train results. val (Dict) \u2014 Validation results. Methods mean ( ) ( Results ) \u2014 Returns a reduced Results instance aveaged by index . </> method mean ( ) \u2192 Results </> Returns a reduced Results instance aveaged by index . class ivory.callbacks.results . BatchResults ( ) </> Bases ivory.callbacks.results.Results ivory.core.collections.Dict ivory.core.state.State Results callback stores training, validation and test results. Each result is ivory.core.collections.Dict type that has index , output , and target array. To get target array of validation, use target = results.val.target Attributes index \u2014 indexes \u2014 output \u2014 outputs \u2014 target \u2014 targets \u2014 test (Dict) \u2014 Test results. train (Dict) \u2014 Train results. val (Dict) \u2014 Validation results. Methods mean ( ) ( Results ) \u2014 Returns a reduced Results instance aveaged by index . </> method mean ( ) \u2192 Results </> Returns a reduced Results instance aveaged by index . function ivory.callbacks.results . concatenate ( iterable , callback=None , modes=('val', 'test') , reduction='none' ) \u2192 Results </> Returns a concatenated Results. Parameters iterable (iterable of Results) \u2014 Iterable of Results instance. callback (callable, optional) \u2014 Called for each Results . Must take ( mode , index , output , target ) arguments and return a tuple of ('index', output , target ). modes (iterable of str) \u2014 Specify modes to concatenate. reduction (str, optional) \u2014 Reduction. none or mean .","title":"ivory.callbacks.results"},{"location":"api/ivory.callbacks.results/#ivorycallbacksresults","text":"</> A container to store training, validation and test results. Classes Results \u2014 Results callback stores training, validation and test results. </> BatchResults \u2014 Results callback stores training, validation and test results. </> Functions concatenate ( iterable , callback , modes , reduction ) ( Results ) \u2014 Returns a concatenated Results. </> class","title":"ivory.callbacks.results"},{"location":"api/ivory.callbacks.results/#ivorycallbacksresultsresults","text":"</> Bases ivory.core.collections.Dict ivory.core.state.State Results callback stores training, validation and test results. Each result is ivory.core.collections.Dict type that has index , output , and target array. To get target array of validation, use target = results.val.target Attributes index \u2014 output \u2014 target \u2014 test (Dict) \u2014 Test results. train (Dict) \u2014 Train results. val (Dict) \u2014 Validation results. Methods mean ( ) ( Results ) \u2014 Returns a reduced Results instance aveaged by index . </> method","title":"ivory.callbacks.results.Results"},{"location":"api/ivory.callbacks.results/#ivorycallbacksresultsresultsmean","text":"</> Returns a reduced Results instance aveaged by index . class","title":"ivory.callbacks.results.Results.mean"},{"location":"api/ivory.callbacks.results/#ivorycallbacksresultsbatchresults","text":"</> Bases ivory.callbacks.results.Results ivory.core.collections.Dict ivory.core.state.State Results callback stores training, validation and test results. Each result is ivory.core.collections.Dict type that has index , output , and target array. To get target array of validation, use target = results.val.target Attributes index \u2014 indexes \u2014 output \u2014 outputs \u2014 target \u2014 targets \u2014 test (Dict) \u2014 Test results. train (Dict) \u2014 Train results. val (Dict) \u2014 Validation results. Methods mean ( ) ( Results ) \u2014 Returns a reduced Results instance aveaged by index . </> method","title":"ivory.callbacks.results.BatchResults"},{"location":"api/ivory.callbacks.results/#ivorycallbacksresultsresultsmean_1","text":"</> Returns a reduced Results instance aveaged by index . function","title":"ivory.callbacks.results.Results.mean"},{"location":"api/ivory.callbacks.results/#ivorycallbacksresultsconcatenate","text":"</> Returns a concatenated Results. Parameters iterable (iterable of Results) \u2014 Iterable of Results instance. callback (callable, optional) \u2014 Called for each Results . Must take ( mode , index , output , target ) arguments and return a tuple of ('index', output , target ). modes (iterable of str) \u2014 Specify modes to concatenate. reduction (str, optional) \u2014 Reduction. none or mean .","title":"ivory.callbacks.results.concatenate"},{"location":"api/ivory.core.base/","text":"MODULE IVORY.CORE . BASE </> This module provides base classes for Ivory. Classes Base \u2014 Base class for an entity class such as Client , Experiment , and Run . </> Creator \u2014 Creator class to create Run instances. </> Callback \u2014 Callback class for the Ivory callback system. </> CallbackCaller \u2014 Callback caller class. </> Experiment \u2014 Experimet class is one of the main classes of Ivory library. </> class ivory.core.base . Base ( params=None , **instances ) </> Bases ivory.core.collections.Dict Base class for an entity class such as Client , Experiment , and Run . Parameters params (dict, optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. class ivory.core.base . Creator ( params=None , **instances ) </> Bases ivory.core.base.Base ivory.core.collections.Dict Creator class to create Run instances. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. Methods create_instance ( instance_name , args , name , **kwargs ) \u2014 Creates an member instance of a Run according to arguments. </> create_params ( args , name , **kwargs ) (dict, dict) \u2014 Returns a tuple of (parameter dictionary, update dictionary). </> create_run ( args , name , **kwargs ) (Run) \u2014 Creates a Run instance according to arguments. </> method create_params ( args=None , name='run' , **kwargs ) \u2192 (dict, dict) </> Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Examples Use args for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The params is the full parameter dictionary, while the update is a part of params , i.e., update = {'hidden_sizes.0': 100, 'fold': 3} . method create_run ( args=None , name='run' , **kwargs ) </> Creates a Run instance according to arguments. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns (Run) Created Run instance. The parameter for this instance is the returned dictionary from the create_params() function. method create_instance ( instance_name , args=None , name='run' , **kwargs ) </> Creates an member instance of a Run according to arguments. Parameters instance_name (str) \u2014 Name of a member instance to create. args (dict, optional) \u2014 Update dictionary. name (optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns Created instance. The parameter for this instance is the returned directory from the create_params() function. class ivory.core.base . Callback ( caller , methods ) </> Callback class for the Ivory callback system. Parameters caller ( CallbackCaller ) \u2014 methods (dict(str: callable)) \u2014 Attributes caller \u2014 methods \u2014 class ivory.core.base . CallbackCaller ( params=None , **instances ) </> Bases ivory.core.base.Creator ivory.core.base.Base ivory.core.collections.Dict Callback caller class. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. Methods create_callbacks ( ) \u2014 Creates callback functions and store them in a dictionary. </> create_instance ( instance_name , args , name , **kwargs ) \u2014 Creates an member instance of a Run according to arguments. </> create_params ( args , name , **kwargs ) (dict, dict) \u2014 Returns a tuple of (parameter dictionary, update dictionary). </> create_run ( args , name , **kwargs ) (Run) \u2014 Creates a Run instance according to arguments. </> method create_params ( args=None , name='run' , **kwargs ) \u2192 (dict, dict) </> Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Examples Use args for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The params is the full parameter dictionary, while the update is a part of params , i.e., update = {'hidden_sizes.0': 100, 'fold': 3} . method create_run ( args=None , name='run' , **kwargs ) </> Creates a Run instance according to arguments. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns (Run) Created Run instance. The parameter for this instance is the returned dictionary from the create_params() function. method create_instance ( instance_name , args=None , name='run' , **kwargs ) </> Creates an member instance of a Run according to arguments. Parameters instance_name (str) \u2014 Name of a member instance to create. args (dict, optional) \u2014 Update dictionary. name (optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns Created instance. The parameter for this instance is the returned directory from the create_params() function. method create_callbacks ( ) </> Creates callback functions and store them in a dictionary. class ivory.core.base . Experiment ( params=None , **instances ) </> Bases ivory.core.base.Creator ivory.core.base.Base ivory.core.collections.Dict Experimet class is one of the main classes of Ivory library. Basically, one experiment is corresponding to one YAML parameter file that is held in an Experiment instance as a parameter dictionary. This parameter dictionary defines the default parameter values to create Run instances. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. See Also The base class ivory.core.base.Creator defines some functions to create a Run instance or its member instance. Methods create_instance ( instance_name , args , name , **kwargs ) \u2014 Creates an member instance of a Run according to arguments. </> create_params ( args , name , **kwargs ) (dict, dict) \u2014 Returns a tuple of (parameter dictionary, update dictionary). </> create_run ( args , name , **kwargs ) (Run) \u2014 Creates a Run instance according to arguments. </> create_study ( args , **suggests ) \u2014 Creates a Study instance for hyperparameter tuning. </> create_task ( ) \u2014 Creates a Task instance for multiple runs. </> set_tracker ( tracker ) \u2014 Sets a Tracker instance for tracking. </> method create_params ( args=None , name='run' , **kwargs ) \u2192 (dict, dict) </> Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Examples Use args for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The params is the full parameter dictionary, while the update is a part of params , i.e., update = {'hidden_sizes.0': 100, 'fold': 3} . method create_run ( args=None , name='run' , **kwargs ) </> Creates a Run instance according to arguments. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns (Run) Created Run instance. The parameter for this instance is the returned dictionary from the create_params() function. method create_instance ( instance_name , args=None , name='run' , **kwargs ) </> Creates an member instance of a Run according to arguments. Parameters instance_name (str) \u2014 Name of a member instance to create. args (dict, optional) \u2014 Update dictionary. name (optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns Created instance. The parameter for this instance is the returned directory from the create_params() function. method set_tracker ( tracker ) </> Sets a Tracker instance for tracking. Parameters tracker (Tracker) \u2014 Tracker instance. method create_task ( ) </> Creates a Task instance for multiple runs. See Also For more details, see client.create_task() Multiple Runs in Tutorial. method create_study ( args=None , **suggests ) </> Creates a Study instance for hyperparameter tuning. Parameters args (optional) \u2014 **suggests \u2014 See Also For more details, see client.create_study() Hyperparameter Tuning in Tutorial","title":"ivory.core.base"},{"location":"api/ivory.core.base/#ivorycorebase","text":"</> This module provides base classes for Ivory. Classes Base \u2014 Base class for an entity class such as Client , Experiment , and Run . </> Creator \u2014 Creator class to create Run instances. </> Callback \u2014 Callback class for the Ivory callback system. </> CallbackCaller \u2014 Callback caller class. </> Experiment \u2014 Experimet class is one of the main classes of Ivory library. </> class","title":"ivory.core.base"},{"location":"api/ivory.core.base/#ivorycorebasebase","text":"</> Bases ivory.core.collections.Dict Base class for an entity class such as Client , Experiment , and Run . Parameters params (dict, optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. class","title":"ivory.core.base.Base"},{"location":"api/ivory.core.base/#ivorycorebasecreator","text":"</> Bases ivory.core.base.Base ivory.core.collections.Dict Creator class to create Run instances. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. Methods create_instance ( instance_name , args , name , **kwargs ) \u2014 Creates an member instance of a Run according to arguments. </> create_params ( args , name , **kwargs ) (dict, dict) \u2014 Returns a tuple of (parameter dictionary, update dictionary). </> create_run ( args , name , **kwargs ) (Run) \u2014 Creates a Run instance according to arguments. </> method","title":"ivory.core.base.Creator"},{"location":"api/ivory.core.base/#ivorycorebasecreatorcreate_params","text":"</> Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Examples Use args for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The params is the full parameter dictionary, while the update is a part of params , i.e., update = {'hidden_sizes.0': 100, 'fold': 3} . method","title":"ivory.core.base.Creator.create_params"},{"location":"api/ivory.core.base/#ivorycorebasecreatorcreate_run","text":"</> Creates a Run instance according to arguments. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns (Run) Created Run instance. The parameter for this instance is the returned dictionary from the create_params() function. method","title":"ivory.core.base.Creator.create_run"},{"location":"api/ivory.core.base/#ivorycorebasecreatorcreate_instance","text":"</> Creates an member instance of a Run according to arguments. Parameters instance_name (str) \u2014 Name of a member instance to create. args (dict, optional) \u2014 Update dictionary. name (optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns Created instance. The parameter for this instance is the returned directory from the create_params() function. class","title":"ivory.core.base.Creator.create_instance"},{"location":"api/ivory.core.base/#ivorycorebasecallback","text":"</> Callback class for the Ivory callback system. Parameters caller ( CallbackCaller ) \u2014 methods (dict(str: callable)) \u2014 Attributes caller \u2014 methods \u2014 class","title":"ivory.core.base.Callback"},{"location":"api/ivory.core.base/#ivorycorebasecallbackcaller","text":"</> Bases ivory.core.base.Creator ivory.core.base.Base ivory.core.collections.Dict Callback caller class. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. Methods create_callbacks ( ) \u2014 Creates callback functions and store them in a dictionary. </> create_instance ( instance_name , args , name , **kwargs ) \u2014 Creates an member instance of a Run according to arguments. </> create_params ( args , name , **kwargs ) (dict, dict) \u2014 Returns a tuple of (parameter dictionary, update dictionary). </> create_run ( args , name , **kwargs ) (Run) \u2014 Creates a Run instance according to arguments. </> method","title":"ivory.core.base.CallbackCaller"},{"location":"api/ivory.core.base/#ivorycorebasecreatorcreate_params_1","text":"</> Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Examples Use args for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The params is the full parameter dictionary, while the update is a part of params , i.e., update = {'hidden_sizes.0': 100, 'fold': 3} . method","title":"ivory.core.base.Creator.create_params"},{"location":"api/ivory.core.base/#ivorycorebasecreatorcreate_run_1","text":"</> Creates a Run instance according to arguments. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns (Run) Created Run instance. The parameter for this instance is the returned dictionary from the create_params() function. method","title":"ivory.core.base.Creator.create_run"},{"location":"api/ivory.core.base/#ivorycorebasecreatorcreate_instance_1","text":"</> Creates an member instance of a Run according to arguments. Parameters instance_name (str) \u2014 Name of a member instance to create. args (dict, optional) \u2014 Update dictionary. name (optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns Created instance. The parameter for this instance is the returned directory from the create_params() function. method","title":"ivory.core.base.Creator.create_instance"},{"location":"api/ivory.core.base/#ivorycorebasecallbackcallercreate_callbacks","text":"</> Creates callback functions and store them in a dictionary. class","title":"ivory.core.base.CallbackCaller.create_callbacks"},{"location":"api/ivory.core.base/#ivorycorebaseexperiment","text":"</> Bases ivory.core.base.Creator ivory.core.base.Base ivory.core.collections.Dict Experimet class is one of the main classes of Ivory library. Basically, one experiment is corresponding to one YAML parameter file that is held in an Experiment instance as a parameter dictionary. This parameter dictionary defines the default parameter values to create Run instances. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. See Also The base class ivory.core.base.Creator defines some functions to create a Run instance or its member instance. Methods create_instance ( instance_name , args , name , **kwargs ) \u2014 Creates an member instance of a Run according to arguments. </> create_params ( args , name , **kwargs ) (dict, dict) \u2014 Returns a tuple of (parameter dictionary, update dictionary). </> create_run ( args , name , **kwargs ) (Run) \u2014 Creates a Run instance according to arguments. </> create_study ( args , **suggests ) \u2014 Creates a Study instance for hyperparameter tuning. </> create_task ( ) \u2014 Creates a Task instance for multiple runs. </> set_tracker ( tracker ) \u2014 Sets a Tracker instance for tracking. </> method","title":"ivory.core.base.Experiment"},{"location":"api/ivory.core.base/#ivorycorebasecreatorcreate_params_2","text":"</> Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Examples Use args for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The params is the full parameter dictionary, while the update is a part of params , i.e., update = {'hidden_sizes.0': 100, 'fold': 3} . method","title":"ivory.core.base.Creator.create_params"},{"location":"api/ivory.core.base/#ivorycorebasecreatorcreate_run_2","text":"</> Creates a Run instance according to arguments. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns (Run) Created Run instance. The parameter for this instance is the returned dictionary from the create_params() function. method","title":"ivory.core.base.Creator.create_run"},{"location":"api/ivory.core.base/#ivorycorebasecreatorcreate_instance_2","text":"</> Creates an member instance of a Run according to arguments. Parameters instance_name (str) \u2014 Name of a member instance to create. args (dict, optional) \u2014 Update dictionary. name (optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns Created instance. The parameter for this instance is the returned directory from the create_params() function. method","title":"ivory.core.base.Creator.create_instance"},{"location":"api/ivory.core.base/#ivorycorebaseexperimentset_tracker","text":"</> Sets a Tracker instance for tracking. Parameters tracker (Tracker) \u2014 Tracker instance. method","title":"ivory.core.base.Experiment.set_tracker"},{"location":"api/ivory.core.base/#ivorycorebaseexperimentcreate_task","text":"</> Creates a Task instance for multiple runs. See Also For more details, see client.create_task() Multiple Runs in Tutorial. method","title":"ivory.core.base.Experiment.create_task"},{"location":"api/ivory.core.base/#ivorycorebaseexperimentcreate_study","text":"</> Creates a Study instance for hyperparameter tuning. Parameters args (optional) \u2014 **suggests \u2014 See Also For more details, see client.create_study() Hyperparameter Tuning in Tutorial","title":"ivory.core.base.Experiment.create_study"},{"location":"api/ivory.core.client/","text":"MODULE IVORY.CORE . CLIENT </> This module provides the Ivory Client class that is one of the main classes of Ivory library. To create an Client instance: import ivory client = ivory.create_client() Here, the current directory becomes the working directory in which experiment YAML files exist. If you want to refer other directory, use: client = ivory.create_client('path/to/working_directory') Classes Client \u2014 The Ivory Client class. </> Functions create_client ( directory , name , tracker , tuner ) ( Client ) \u2014 Creates an Ivory Client instance. </> class ivory.core.client . Client ( params=None , **objects ) </> Bases ivory.core.base.Base ivory.core.collections.Dict The Ivory Client class. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **objects \u2014 Attributes dict \u2014 experiments (dict(str: Experiment )) \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. tracker (Tracker) \u2014 A Tracker instance for tracking run process. tuner (Tuner) \u2014 A Tuner instance for hyperparameter tuning. Methods create_experiment ( name , *args , **kwargs ) ( Experiment ) \u2014 Creates an Experiment according to the YAML file specified by name . </> create_run ( name , args , **kwargs ) ( Run ) \u2014 Creates a Run . </> create_study ( name , args , run_number , **suggests ) ( Study ) \u2014 Creates a Study instance for hyperparameter tuning. </> create_task ( name , run_number ) ( Task ) \u2014 Creates a Task instance for multiple runs. </> get_nested_run_ids ( name , **kwargs ) (str) \u2014 Returns an iterator that yields nested Run IDs of parent runs. </> get_parent_run_id ( name , **kwargs ) (str) \u2014 Returns a parent Run ID of a nested run. </> get_run_id ( name , **kwargs ) (str) \u2014 Returns a Run ID. </> get_run_ids ( name , **kwargs ) (str) \u2014 Returns an iterator that yields Run IDs. </> get_run_name ( run_id ) (str) \u2014 Returns a run name ( run#XXX , task#XXX , etc .) for Run ID. </> get_run_name_tuple ( run_id ) (str, int) \u2014 Returns a run name as a tuple of (run class name, run number). </> load_instance ( run_id , instance_name , mode ) (any) \u2014 Returns a member of a Run created using parameters loaded from MLFlow Tracking. </> load_params ( run_id ) (dict(str: any)) \u2014 Returns a parameter dictionary loaded from MLFlow Tracking. </> load_results ( run_ids , callback , reduction , verbose ) ( Results ) \u2014 Loads results from multiple runs and concatenates them. </> load_run ( run_id , mode ) ( Run ) \u2014 Returns a Run instance created using parameters loaded from MLFlow Tracking. </> load_run_by_name ( name , mode , **kwargs ) ( Run ) \u2014 Returns a Run instance created using parameters loaded from MLFlow Tracking. </> remove_deleted_runs ( name ) (int) \u2014 Removes deleted runs from a local file system. </> search_nested_run_ids ( name , **query ) (str) \u2014 Returns an iterator that yields matching nested Run IDs. </> search_parent_run_ids ( name , **query ) (str) \u2014 Returns an iterator that yields matching parent Run IDs. </> search_run_ids ( name , run_name , parent_run_id , parent_only , nested_only , exclude_parent , best_score_limit , **query ) (str) \u2014 Returns an iterator that yields matching Run IDs. </> set_parent_run_id ( name , **kwargs ) \u2014 Sets parent Run ID to runs. </> set_terminated ( name , status , **kwargs ) \u2014 Sets runs' status to terminated. </> set_terminated_all ( name ) \u2014 Sets all runs' status to terminated. </> method create_experiment ( name , *args , **kwargs ) \u2192 Experiment </> Creates an Experiment according to the YAML file specified by name . Parameters name (str) \u2014 Experiment name. *args \u2014 Additional parameter files. **kwargs \u2014 Additional parameter files. A YAML file named <name>.yml or <name>.yaml should exist under the working directory. Any additionanl parameter files are added through *args and/or **kwargs . Examples Positional argument style : experiment = client.create_experiment('example', 'study') In this case, study.yml is like this, including the instance name study : study: tuner: pruner: class: optuna.pruners.MedianPruner objective: lr: example.suggest_lr Keyword argument style : experiment = client.create_experiment('example', study='study') In this case, study.yml is like this, omitting the instance name study : tuner: pruner: class: optuna.pruners.MedianPruner objective: lr: example.suggest_lr method create_run ( name , args=None , **kwargs ) \u2192 Run </> Creates a Run . Parameters name (str) \u2014 Experiment name. args (dict, optional) \u2014 Parameter dictionary to update the default values of Experiment . **kwargs \u2014 Additional parameters. Examples To update a fold number: run = client.create_run('example', fold=3) If a parameter name includes dots: run = client.create_run('example', {'model.class': 'your.new.Model'}) method create_task ( name , run_number=None ) \u2192 Task </> Creates a Task instance for multiple runs. Parameters name (str) \u2014 Experiment name. run_number (int, optional) \u2014 If specified, load an existing task instead of creating a new one. See Also Multiple Runs in Tutorial method create_study ( name , args=None , run_number=None , **suggests ) \u2192 Study </> Creates a Study instance for hyperparameter tuning. Parameters name (str) \u2014 Experiment name. args (str or dict) \u2014 Suggest name (str) or parametric optimization (dict). run_number (int, optional) \u2014 If specified, load an existing study instead of creating a new one. **suggests \u2014 Parametric optimization. Examples To use a suggest function: study = client.create_study('example', 'lr') For parametric optimization: study = client.create_study('example', lr=(1e-5, 1e-3)) If a parameter name includes dots: study = client.create_study('example', {'hidden_sizes.0': range(5, 20)}) See Also Hyperparameter Tuning in Tutorial method get_run_id ( name , **kwargs ) \u2192 str </> Returns a Run ID. Parameters name (str) \u2014 Experiment name. **kwargs \u2014 Examples To get a Run ID of run#4. client.get_run_id('example', run=4) To get a Run ID of task#10. client.get_run_id('example', task=10) generator get_run_ids ( name , **kwargs ) \u2192 str </> Returns an iterator that yields Run IDs. Parameters name (str) \u2014 Experiment name. **kwargs \u2014 Examples To get an iterator that yields Run IDs for Runs. client.get_run_id('example', run=[1, 2, 3]) To get an iterator that yields Run IDs for Tasks. client.get_run_id('example', task=range(3, 8)) method get_parent_run_id ( name , **kwargs ) \u2192 str </> Returns a parent Run ID of a nested run. Parameters name (str) \u2014 Experiment name. **kwargs \u2014 Examples To get a prarent Run ID of run#5. client.get_parent_run_id('example', run=5) generator get_nested_run_ids ( name , **kwargs ) \u2192 str </> Returns an iterator that yields nested Run IDs of parent runs. Parameters name (str) \u2014 Experiment name. **kwargs \u2014 Examples To get an iterator that yields Run IDs of runs whose parent is task#2. client.get_nested_run_ids('example', task=2) Multiple parents can be specified. client.get_nested_run_ids('example', task=range(3, 8)) method set_parent_run_id ( name , **kwargs ) </> Sets parent Run ID to runs. Parameters name (str) \u2014 Experiment name. **kwargs \u2014 Examples To set task#2 as a parant for run#4. client.set_parent_run_id('example', task=2, run=4) Multiple nested runs can be specified. client.set_parent_run_id('example', task=2, run=range(3)) method get_run_name ( run_id ) \u2192 str </> Returns a run name ( run#XXX , task#XXX , etc .) for Run ID. Parameters run_id (str) \u2014 Run ID method get_run_name_tuple ( run_id ) \u2192 (str, int) </> Returns a run name as a tuple of (run class name, run number). Parameters run_id (str) \u2014 Run ID generator search_run_ids ( name='' , run_name='' , parent_run_id='' , parent_only=False , nested_only=False , exclude_parent=False , best_score_limit=None , **query ) \u2192 str </> Returns an iterator that yields matching Run IDs. Parameters name (str, optional) \u2014 Experiment name pattern for filtering. run_name (str, optional) \u2014 Run name pattern for filtering. parent_run_id (str or iterable of str) \u2014 If specified, search from runs that have the parent id(s). parent_only (bool, optional) \u2014 If True, search from parent runs. nested_only (bool, optional) \u2014 If True, search from nested runs. exclude_parent (bool, optional) \u2014 If True, skip parent runs. best_score_limit (float, optional) \u2014 Yields runs with the best score better than this value. **query \u2014 Key-value pairs for filtering. generator search_parent_run_ids ( name='' , **query ) \u2192 str </> Returns an iterator that yields matching parent Run IDs. Parameters name (str, optional) \u2014 Experiment name pattern for filtering. **query \u2014 Key-value pairs for filtering. generator search_nested_run_ids ( name='' , **query ) \u2192 str </> Returns an iterator that yields matching nested Run IDs. Parameters name (str, optional) \u2014 Experiment name pattern for filtering. **query \u2014 Key-value pairs for filtering. method set_terminated ( name , status=None , **kwargs ) </> Sets runs' status to terminated. Parameters name (str) \u2014 status (str, optional) \u2014 A string value of mlflow.entities.RunStatus . Defaults to \u201cFINISHED\u201d. **kwargs \u2014 Examples To terminate a run: client.set_terminated('example', run=5) To kill multiple runs: client.set_terminated('example', 'KILLED', run=[3, 5, 7]) method set_terminated_all ( name='' ) </> Sets all runs' status to terminated. Parameters name (str, optional) \u2014 status \u2014 A string value of mlflow.entities.RunStatus . Defaults to \u201cFINISHED\u201d. Examples To terminate all of the runs of the example experiment: client.set_terminated_all('example') To terminate all of the runs globally: client.set_terminated_all() method load_params ( run_id ) \u2192 dict(str: any) </> Returns a parameter dictionary loaded from MLFlow Tracking. Parameters run_id (str) \u2014 Run ID for a run to be loaded. method load_run ( run_id , mode='test' ) \u2192 Run </> Returns a Run instance created using parameters loaded from MLFlow Tracking. Parameters run_id (str) \u2014 Run ID for a run to be loaded. mode (str, optional) \u2014 Mode name: 'current' , 'best' , or 'test' . Default is ''test'' . method load_run_by_name ( name , mode='test' , **kwargs ) \u2192 Run </> Returns a Run instance created using parameters loaded from MLFlow Tracking. Parameters name (str) \u2014 Experiment name pattern for filtering. mode (str, optional) \u2014 Mode name: 'current' , 'best' , or 'test' . **kwargs \u2014 Examples To load run#4 of the example experiment. client.load_run_by_name('example', run=4) method load_instance ( run_id , instance_name , mode='test' ) \u2192 any </> Returns a member of a Run created using parameters loaded from MLFlow Tracking. Parameters run_id (str) \u2014 Run ID for a run to be loaded. instance_name (str) \u2014 Instance name. mode (str, optional) \u2014 Mode name: 'current' , 'best' , or 'test' . method load_results ( run_ids , callback=None , reduction='none' , verbose=True ) </> Loads results from multiple runs and concatenates them. Parameters run_ids (Union(str, iterable of str)) \u2014 Multiple run ids to load. callback (callable) \u2014 Callback function for each run. This function must take a (index, output, target) and return a tuple with the same signature. reduction (str, optional) \u2014 verbose (bool, optional) \u2014 If True , tqdm progress bar is displayed. Returns ( Results ) A concatenated results instance. method remove_deleted_runs ( name='' ) </> Removes deleted runs from a local file system. Parameters name (str, optional) \u2014 Experiment name pattern for filtering. Returns (int) Number of removed runs. function ivory.core.client . create_client ( directory='' , name='client' , tracker=True , tuner=True ) </> Creates an Ivory Client instance. Parameters directory (str, optional) \u2014 A working directory. If a YAML file specified by the name parameter exists, the file is loaded to configure the client. In addition, this directory is automatically inserted to sys.path . name (str, optional) \u2014 A YAML config file name. tracker (bool, optional) \u2014 If True, the client instance has a tracker. tuner (bool, optional) \u2014 If True, the client instance has a tuner. Returns ( Client ) An created client. Note If tracker is True (default value), a mlruns directory is made under the working directory by MLFlow Tracking.","title":"ivory.core.client"},{"location":"api/ivory.core.client/#ivorycoreclient","text":"</> This module provides the Ivory Client class that is one of the main classes of Ivory library. To create an Client instance: import ivory client = ivory.create_client() Here, the current directory becomes the working directory in which experiment YAML files exist. If you want to refer other directory, use: client = ivory.create_client('path/to/working_directory') Classes Client \u2014 The Ivory Client class. </> Functions create_client ( directory , name , tracker , tuner ) ( Client ) \u2014 Creates an Ivory Client instance. </> class","title":"ivory.core.client"},{"location":"api/ivory.core.client/#ivorycoreclientclient","text":"</> Bases ivory.core.base.Base ivory.core.collections.Dict The Ivory Client class. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **objects \u2014 Attributes dict \u2014 experiments (dict(str: Experiment )) \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. tracker (Tracker) \u2014 A Tracker instance for tracking run process. tuner (Tuner) \u2014 A Tuner instance for hyperparameter tuning. Methods create_experiment ( name , *args , **kwargs ) ( Experiment ) \u2014 Creates an Experiment according to the YAML file specified by name . </> create_run ( name , args , **kwargs ) ( Run ) \u2014 Creates a Run . </> create_study ( name , args , run_number , **suggests ) ( Study ) \u2014 Creates a Study instance for hyperparameter tuning. </> create_task ( name , run_number ) ( Task ) \u2014 Creates a Task instance for multiple runs. </> get_nested_run_ids ( name , **kwargs ) (str) \u2014 Returns an iterator that yields nested Run IDs of parent runs. </> get_parent_run_id ( name , **kwargs ) (str) \u2014 Returns a parent Run ID of a nested run. </> get_run_id ( name , **kwargs ) (str) \u2014 Returns a Run ID. </> get_run_ids ( name , **kwargs ) (str) \u2014 Returns an iterator that yields Run IDs. </> get_run_name ( run_id ) (str) \u2014 Returns a run name ( run#XXX , task#XXX , etc .) for Run ID. </> get_run_name_tuple ( run_id ) (str, int) \u2014 Returns a run name as a tuple of (run class name, run number). </> load_instance ( run_id , instance_name , mode ) (any) \u2014 Returns a member of a Run created using parameters loaded from MLFlow Tracking. </> load_params ( run_id ) (dict(str: any)) \u2014 Returns a parameter dictionary loaded from MLFlow Tracking. </> load_results ( run_ids , callback , reduction , verbose ) ( Results ) \u2014 Loads results from multiple runs and concatenates them. </> load_run ( run_id , mode ) ( Run ) \u2014 Returns a Run instance created using parameters loaded from MLFlow Tracking. </> load_run_by_name ( name , mode , **kwargs ) ( Run ) \u2014 Returns a Run instance created using parameters loaded from MLFlow Tracking. </> remove_deleted_runs ( name ) (int) \u2014 Removes deleted runs from a local file system. </> search_nested_run_ids ( name , **query ) (str) \u2014 Returns an iterator that yields matching nested Run IDs. </> search_parent_run_ids ( name , **query ) (str) \u2014 Returns an iterator that yields matching parent Run IDs. </> search_run_ids ( name , run_name , parent_run_id , parent_only , nested_only , exclude_parent , best_score_limit , **query ) (str) \u2014 Returns an iterator that yields matching Run IDs. </> set_parent_run_id ( name , **kwargs ) \u2014 Sets parent Run ID to runs. </> set_terminated ( name , status , **kwargs ) \u2014 Sets runs' status to terminated. </> set_terminated_all ( name ) \u2014 Sets all runs' status to terminated. </> method","title":"ivory.core.client.Client"},{"location":"api/ivory.core.client/#ivorycoreclientclientcreate_experiment","text":"</> Creates an Experiment according to the YAML file specified by name . Parameters name (str) \u2014 Experiment name. *args \u2014 Additional parameter files. **kwargs \u2014 Additional parameter files. A YAML file named <name>.yml or <name>.yaml should exist under the working directory. Any additionanl parameter files are added through *args and/or **kwargs . Examples Positional argument style : experiment = client.create_experiment('example', 'study') In this case, study.yml is like this, including the instance name study : study: tuner: pruner: class: optuna.pruners.MedianPruner objective: lr: example.suggest_lr Keyword argument style : experiment = client.create_experiment('example', study='study') In this case, study.yml is like this, omitting the instance name study : tuner: pruner: class: optuna.pruners.MedianPruner objective: lr: example.suggest_lr method","title":"ivory.core.client.Client.create_experiment"},{"location":"api/ivory.core.client/#ivorycoreclientclientcreate_run","text":"</> Creates a Run . Parameters name (str) \u2014 Experiment name. args (dict, optional) \u2014 Parameter dictionary to update the default values of Experiment . **kwargs \u2014 Additional parameters. Examples To update a fold number: run = client.create_run('example', fold=3) If a parameter name includes dots: run = client.create_run('example', {'model.class': 'your.new.Model'}) method","title":"ivory.core.client.Client.create_run"},{"location":"api/ivory.core.client/#ivorycoreclientclientcreate_task","text":"</> Creates a Task instance for multiple runs. Parameters name (str) \u2014 Experiment name. run_number (int, optional) \u2014 If specified, load an existing task instead of creating a new one. See Also Multiple Runs in Tutorial method","title":"ivory.core.client.Client.create_task"},{"location":"api/ivory.core.client/#ivorycoreclientclientcreate_study","text":"</> Creates a Study instance for hyperparameter tuning. Parameters name (str) \u2014 Experiment name. args (str or dict) \u2014 Suggest name (str) or parametric optimization (dict). run_number (int, optional) \u2014 If specified, load an existing study instead of creating a new one. **suggests \u2014 Parametric optimization. Examples To use a suggest function: study = client.create_study('example', 'lr') For parametric optimization: study = client.create_study('example', lr=(1e-5, 1e-3)) If a parameter name includes dots: study = client.create_study('example', {'hidden_sizes.0': range(5, 20)}) See Also Hyperparameter Tuning in Tutorial method","title":"ivory.core.client.Client.create_study"},{"location":"api/ivory.core.client/#ivorycoreclientclientget_run_id","text":"</> Returns a Run ID. Parameters name (str) \u2014 Experiment name. **kwargs \u2014 Examples To get a Run ID of run#4. client.get_run_id('example', run=4) To get a Run ID of task#10. client.get_run_id('example', task=10) generator","title":"ivory.core.client.Client.get_run_id"},{"location":"api/ivory.core.client/#ivorycoreclientclientget_run_ids","text":"</> Returns an iterator that yields Run IDs. Parameters name (str) \u2014 Experiment name. **kwargs \u2014 Examples To get an iterator that yields Run IDs for Runs. client.get_run_id('example', run=[1, 2, 3]) To get an iterator that yields Run IDs for Tasks. client.get_run_id('example', task=range(3, 8)) method","title":"ivory.core.client.Client.get_run_ids"},{"location":"api/ivory.core.client/#ivorycoreclientclientget_parent_run_id","text":"</> Returns a parent Run ID of a nested run. Parameters name (str) \u2014 Experiment name. **kwargs \u2014 Examples To get a prarent Run ID of run#5. client.get_parent_run_id('example', run=5) generator","title":"ivory.core.client.Client.get_parent_run_id"},{"location":"api/ivory.core.client/#ivorycoreclientclientget_nested_run_ids","text":"</> Returns an iterator that yields nested Run IDs of parent runs. Parameters name (str) \u2014 Experiment name. **kwargs \u2014 Examples To get an iterator that yields Run IDs of runs whose parent is task#2. client.get_nested_run_ids('example', task=2) Multiple parents can be specified. client.get_nested_run_ids('example', task=range(3, 8)) method","title":"ivory.core.client.Client.get_nested_run_ids"},{"location":"api/ivory.core.client/#ivorycoreclientclientset_parent_run_id","text":"</> Sets parent Run ID to runs. Parameters name (str) \u2014 Experiment name. **kwargs \u2014 Examples To set task#2 as a parant for run#4. client.set_parent_run_id('example', task=2, run=4) Multiple nested runs can be specified. client.set_parent_run_id('example', task=2, run=range(3)) method","title":"ivory.core.client.Client.set_parent_run_id"},{"location":"api/ivory.core.client/#ivorycoreclientclientget_run_name","text":"</> Returns a run name ( run#XXX , task#XXX , etc .) for Run ID. Parameters run_id (str) \u2014 Run ID method","title":"ivory.core.client.Client.get_run_name"},{"location":"api/ivory.core.client/#ivorycoreclientclientget_run_name_tuple","text":"</> Returns a run name as a tuple of (run class name, run number). Parameters run_id (str) \u2014 Run ID generator","title":"ivory.core.client.Client.get_run_name_tuple"},{"location":"api/ivory.core.client/#ivorycoreclientclientsearch_run_ids","text":"</> Returns an iterator that yields matching Run IDs. Parameters name (str, optional) \u2014 Experiment name pattern for filtering. run_name (str, optional) \u2014 Run name pattern for filtering. parent_run_id (str or iterable of str) \u2014 If specified, search from runs that have the parent id(s). parent_only (bool, optional) \u2014 If True, search from parent runs. nested_only (bool, optional) \u2014 If True, search from nested runs. exclude_parent (bool, optional) \u2014 If True, skip parent runs. best_score_limit (float, optional) \u2014 Yields runs with the best score better than this value. **query \u2014 Key-value pairs for filtering. generator","title":"ivory.core.client.Client.search_run_ids"},{"location":"api/ivory.core.client/#ivorycoreclientclientsearch_parent_run_ids","text":"</> Returns an iterator that yields matching parent Run IDs. Parameters name (str, optional) \u2014 Experiment name pattern for filtering. **query \u2014 Key-value pairs for filtering. generator","title":"ivory.core.client.Client.search_parent_run_ids"},{"location":"api/ivory.core.client/#ivorycoreclientclientsearch_nested_run_ids","text":"</> Returns an iterator that yields matching nested Run IDs. Parameters name (str, optional) \u2014 Experiment name pattern for filtering. **query \u2014 Key-value pairs for filtering. method","title":"ivory.core.client.Client.search_nested_run_ids"},{"location":"api/ivory.core.client/#ivorycoreclientclientset_terminated","text":"</> Sets runs' status to terminated. Parameters name (str) \u2014 status (str, optional) \u2014 A string value of mlflow.entities.RunStatus . Defaults to \u201cFINISHED\u201d. **kwargs \u2014 Examples To terminate a run: client.set_terminated('example', run=5) To kill multiple runs: client.set_terminated('example', 'KILLED', run=[3, 5, 7]) method","title":"ivory.core.client.Client.set_terminated"},{"location":"api/ivory.core.client/#ivorycoreclientclientset_terminated_all","text":"</> Sets all runs' status to terminated. Parameters name (str, optional) \u2014 status \u2014 A string value of mlflow.entities.RunStatus . Defaults to \u201cFINISHED\u201d. Examples To terminate all of the runs of the example experiment: client.set_terminated_all('example') To terminate all of the runs globally: client.set_terminated_all() method","title":"ivory.core.client.Client.set_terminated_all"},{"location":"api/ivory.core.client/#ivorycoreclientclientload_params","text":"</> Returns a parameter dictionary loaded from MLFlow Tracking. Parameters run_id (str) \u2014 Run ID for a run to be loaded. method","title":"ivory.core.client.Client.load_params"},{"location":"api/ivory.core.client/#ivorycoreclientclientload_run","text":"</> Returns a Run instance created using parameters loaded from MLFlow Tracking. Parameters run_id (str) \u2014 Run ID for a run to be loaded. mode (str, optional) \u2014 Mode name: 'current' , 'best' , or 'test' . Default is ''test'' . method","title":"ivory.core.client.Client.load_run"},{"location":"api/ivory.core.client/#ivorycoreclientclientload_run_by_name","text":"</> Returns a Run instance created using parameters loaded from MLFlow Tracking. Parameters name (str) \u2014 Experiment name pattern for filtering. mode (str, optional) \u2014 Mode name: 'current' , 'best' , or 'test' . **kwargs \u2014 Examples To load run#4 of the example experiment. client.load_run_by_name('example', run=4) method","title":"ivory.core.client.Client.load_run_by_name"},{"location":"api/ivory.core.client/#ivorycoreclientclientload_instance","text":"</> Returns a member of a Run created using parameters loaded from MLFlow Tracking. Parameters run_id (str) \u2014 Run ID for a run to be loaded. instance_name (str) \u2014 Instance name. mode (str, optional) \u2014 Mode name: 'current' , 'best' , or 'test' . method","title":"ivory.core.client.Client.load_instance"},{"location":"api/ivory.core.client/#ivorycoreclientclientload_results","text":"</> Loads results from multiple runs and concatenates them. Parameters run_ids (Union(str, iterable of str)) \u2014 Multiple run ids to load. callback (callable) \u2014 Callback function for each run. This function must take a (index, output, target) and return a tuple with the same signature. reduction (str, optional) \u2014 verbose (bool, optional) \u2014 If True , tqdm progress bar is displayed. Returns ( Results ) A concatenated results instance. method","title":"ivory.core.client.Client.load_results"},{"location":"api/ivory.core.client/#ivorycoreclientclientremove_deleted_runs","text":"</> Removes deleted runs from a local file system. Parameters name (str, optional) \u2014 Experiment name pattern for filtering. Returns (int) Number of removed runs. function","title":"ivory.core.client.Client.remove_deleted_runs"},{"location":"api/ivory.core.client/#ivorycoreclientcreate_client","text":"</> Creates an Ivory Client instance. Parameters directory (str, optional) \u2014 A working directory. If a YAML file specified by the name parameter exists, the file is loaded to configure the client. In addition, this directory is automatically inserted to sys.path . name (str, optional) \u2014 A YAML config file name. tracker (bool, optional) \u2014 If True, the client instance has a tracker. tuner (bool, optional) \u2014 If True, the client instance has a tuner. Returns ( Client ) An created client. Note If tracker is True (default value), a mlruns directory is made under the working directory by MLFlow Tracking.","title":"ivory.core.client.create_client"},{"location":"api/ivory.core.data/","text":"MODULE IVORY.CORE . DATA </> Ivory uses four classes for data presentation: Data , Dataset , Datasets , and DataLoaders . Basically, you only need to define a class that is a subclass of Data and use original Dataset and Datasets . An example parameter YAML file is: datasets: data: class: your.Data # a subclass of ivory.core.data.Data dataset: fold: 0 But if you need, you can define your Dataset and/or Datasets . datasets: class: your.Datasets # a subclass of ivory.core.data.Datasets data: class: your.Data # a subclass of ivory.core.data.Data dataset: def: your.Dataset # a subclass of ivory.core.data.Dataset fold: 0 The DataLoaders is used internally by ivory.torch.trainer.Trainer or ivory.nnabla.trainer.Trainer classes to yield a minibatch in training loop. Note Use a 'def' key for dataset instead of 'class' . See Tutorial Classes Data \u2014 Base class to provide data to a Dataset instance. </> Dataset \u2014 Dataset class represents a set of data for a mode and fold. </> Datasets \u2014 Dataset class represents a collection of Dataset for a fold. </> DataLoaders \u2014 DataLoaders class represents a collection of DataLoader . </> dataclass ivory.core.data . Data ( ) </> Base class to provide data to a Dataset instance. To make a subclass, you need to assign the following attributes in the Data.init() : index : Index of samples. input : Input data. target : Target data. fold : Fold number. Methods get ( index ) (tuple) \u2014 Returns a tuple of ( index , input , target ) according to the index. </> get_index ( mode , fold ) (ndarray) \u2014 Returns index according to the mode and fold. </> get_input ( index ) \u2014 Returns input data. </> get_target ( index ) \u2014 Returns target data. </> init ( ) \u2014 Initializes index , input , target , and fold attributes. </> method init ( ) </> Initializes index , input , target , and fold attributes. The fold number of test data must be -1 . Examples For regression def init(self): self.index = np.range(100) self.input = np.random.randn(100, 5) self.target = np.random.randn(100) self.fold = np.random.randint(5) self.fold[80:] = -1 For classification def init(self): self.index = np.range(100) self.input = np.random.randn(100, 5) self.target = np.random.randint(100, 10) self.fold = np.random.randint(5) self.fold[80:] = -1 method get_index ( mode , fold ) \u2192 ndarray </> Returns index according to the mode and fold. Parameters mode (str) \u2014 Mode name: 'train' , 'val' , or 'test' . fold (int) \u2014 Fold number. method get_input ( index ) </> Returns input data. By default, this function returns self.input[index] . You can override this behavior in a subclass. Parameters index (int or 1D-array) \u2014 Index. method get_target ( index ) </> Returns target data. By default, this function returns self.target[index] . You can override this behavior in a subclass. Parameters index (int or 1D-array) \u2014 Index. method get ( index ) \u2192 tuple </> Returns a tuple of ( index , input , target ) according to the index. Parameters index (int or 1D-array) \u2014 Index. dataclass ivory.core.data . Dataset ( data , mode , fold , transform=None ) </> Dataset class represents a set of data for a mode and fold. Parameters data ( Data ) \u2014 Data instance that provides data to Dataset instance. mode (str) \u2014 Mode name: 'train' , 'val' , or 'test' . fold (int) \u2014 Fold number. transform (callable, optional) \u2014 Callable to transform the data. The transform must take 2 or 3 arguments: ( mode , input , optional target ) and return a tuple of ( input , optional target ). Attributes data ( Data ) \u2014 Data instance that provides data to Dataset instance. fold (int) \u2014 Fold number. mode (str) \u2014 Mode name: 'train' , 'val' , or 'test' . transform (callable, optional) \u2014 Callable to transform the data. Methods get ( index ) (tuple) \u2014 Returns a tuple of ( index , input , target ) according to the index. </> init ( ) \u2014 Called at initialization. You can add any process in a subclass. </> sample ( n , frac ) (tuple) \u2014 Returns a tuple of ( index , input , target ) randomly sampled. </> method init ( ) </> Called at initialization. You can add any process in a subclass. method get ( index=None ) \u2192 tuple </> Returns a tuple of ( index , input , target ) according to the index. If index is None , reutrns all of the data. Parameters index (int or 1D-array, optional) \u2014 Index. method sample ( n=0 , frac=0.0 ) \u2192 tuple </> Returns a tuple of ( index , input , target ) randomly sampled. Parameters n (int, optional) \u2014 Size of sampling. frac (float, optional) \u2014 Ratio of sampling. dataclass ivory.core.data . Datasets ( data , dataset , fold ) </> Bases ivory.core.collections.Dict Dataset class represents a collection of Dataset for a fold. Parameters data ( Data ) \u2014 Data instance that provides data to Dataset instance. dataset (callable) \u2014 Dataset factory. fold (int) \u2014 Fold number. Attributes data ( Data ) \u2014 Data instance that provides data to Dataset instance. dataset (callable) \u2014 Dataset factory. fold (int) \u2014 Fold number. test (Dataset) \u2014 Test dataset. train (Dataset) \u2014 Train dataset. val (Dataset) \u2014 Validation dataset. class ivory.core.data . DataLoaders ( datasets , batch_size , shuffle ) </> Bases ivory.core.collections.Dict DataLoaders class represents a collection of DataLoader . Parameters datasets ( Datasets ) \u2014 Datasets instance. batch_size (int) \u2014 Batch_size shuffle (bool) \u2014 If True, train dataset is shuffled. Validation and test dataset are not shuffled regardless of this value. Attributes test (Dataset) \u2014 Test dataset. train (Dataset) \u2014 Train dataset. val (Dataset) \u2014 Validation dataset.","title":"ivory.core.data"},{"location":"api/ivory.core.data/#ivorycoredata","text":"</> Ivory uses four classes for data presentation: Data , Dataset , Datasets , and DataLoaders . Basically, you only need to define a class that is a subclass of Data and use original Dataset and Datasets . An example parameter YAML file is: datasets: data: class: your.Data # a subclass of ivory.core.data.Data dataset: fold: 0 But if you need, you can define your Dataset and/or Datasets . datasets: class: your.Datasets # a subclass of ivory.core.data.Datasets data: class: your.Data # a subclass of ivory.core.data.Data dataset: def: your.Dataset # a subclass of ivory.core.data.Dataset fold: 0 The DataLoaders is used internally by ivory.torch.trainer.Trainer or ivory.nnabla.trainer.Trainer classes to yield a minibatch in training loop. Note Use a 'def' key for dataset instead of 'class' . See Tutorial Classes Data \u2014 Base class to provide data to a Dataset instance. </> Dataset \u2014 Dataset class represents a set of data for a mode and fold. </> Datasets \u2014 Dataset class represents a collection of Dataset for a fold. </> DataLoaders \u2014 DataLoaders class represents a collection of DataLoader . </> dataclass","title":"ivory.core.data"},{"location":"api/ivory.core.data/#ivorycoredatadata","text":"</> Base class to provide data to a Dataset instance. To make a subclass, you need to assign the following attributes in the Data.init() : index : Index of samples. input : Input data. target : Target data. fold : Fold number. Methods get ( index ) (tuple) \u2014 Returns a tuple of ( index , input , target ) according to the index. </> get_index ( mode , fold ) (ndarray) \u2014 Returns index according to the mode and fold. </> get_input ( index ) \u2014 Returns input data. </> get_target ( index ) \u2014 Returns target data. </> init ( ) \u2014 Initializes index , input , target , and fold attributes. </> method","title":"ivory.core.data.Data"},{"location":"api/ivory.core.data/#ivorycoredatadatainit","text":"</> Initializes index , input , target , and fold attributes. The fold number of test data must be -1 . Examples For regression def init(self): self.index = np.range(100) self.input = np.random.randn(100, 5) self.target = np.random.randn(100) self.fold = np.random.randint(5) self.fold[80:] = -1 For classification def init(self): self.index = np.range(100) self.input = np.random.randn(100, 5) self.target = np.random.randint(100, 10) self.fold = np.random.randint(5) self.fold[80:] = -1 method","title":"ivory.core.data.Data.init"},{"location":"api/ivory.core.data/#ivorycoredatadataget_index","text":"</> Returns index according to the mode and fold. Parameters mode (str) \u2014 Mode name: 'train' , 'val' , or 'test' . fold (int) \u2014 Fold number. method","title":"ivory.core.data.Data.get_index"},{"location":"api/ivory.core.data/#ivorycoredatadataget_input","text":"</> Returns input data. By default, this function returns self.input[index] . You can override this behavior in a subclass. Parameters index (int or 1D-array) \u2014 Index. method","title":"ivory.core.data.Data.get_input"},{"location":"api/ivory.core.data/#ivorycoredatadataget_target","text":"</> Returns target data. By default, this function returns self.target[index] . You can override this behavior in a subclass. Parameters index (int or 1D-array) \u2014 Index. method","title":"ivory.core.data.Data.get_target"},{"location":"api/ivory.core.data/#ivorycoredatadataget","text":"</> Returns a tuple of ( index , input , target ) according to the index. Parameters index (int or 1D-array) \u2014 Index. dataclass","title":"ivory.core.data.Data.get"},{"location":"api/ivory.core.data/#ivorycoredatadataset","text":"</> Dataset class represents a set of data for a mode and fold. Parameters data ( Data ) \u2014 Data instance that provides data to Dataset instance. mode (str) \u2014 Mode name: 'train' , 'val' , or 'test' . fold (int) \u2014 Fold number. transform (callable, optional) \u2014 Callable to transform the data. The transform must take 2 or 3 arguments: ( mode , input , optional target ) and return a tuple of ( input , optional target ). Attributes data ( Data ) \u2014 Data instance that provides data to Dataset instance. fold (int) \u2014 Fold number. mode (str) \u2014 Mode name: 'train' , 'val' , or 'test' . transform (callable, optional) \u2014 Callable to transform the data. Methods get ( index ) (tuple) \u2014 Returns a tuple of ( index , input , target ) according to the index. </> init ( ) \u2014 Called at initialization. You can add any process in a subclass. </> sample ( n , frac ) (tuple) \u2014 Returns a tuple of ( index , input , target ) randomly sampled. </> method","title":"ivory.core.data.Dataset"},{"location":"api/ivory.core.data/#ivorycoredatadatasetinit","text":"</> Called at initialization. You can add any process in a subclass. method","title":"ivory.core.data.Dataset.init"},{"location":"api/ivory.core.data/#ivorycoredatadatasetget","text":"</> Returns a tuple of ( index , input , target ) according to the index. If index is None , reutrns all of the data. Parameters index (int or 1D-array, optional) \u2014 Index. method","title":"ivory.core.data.Dataset.get"},{"location":"api/ivory.core.data/#ivorycoredatadatasetsample","text":"</> Returns a tuple of ( index , input , target ) randomly sampled. Parameters n (int, optional) \u2014 Size of sampling. frac (float, optional) \u2014 Ratio of sampling. dataclass","title":"ivory.core.data.Dataset.sample"},{"location":"api/ivory.core.data/#ivorycoredatadatasets","text":"</> Bases ivory.core.collections.Dict Dataset class represents a collection of Dataset for a fold. Parameters data ( Data ) \u2014 Data instance that provides data to Dataset instance. dataset (callable) \u2014 Dataset factory. fold (int) \u2014 Fold number. Attributes data ( Data ) \u2014 Data instance that provides data to Dataset instance. dataset (callable) \u2014 Dataset factory. fold (int) \u2014 Fold number. test (Dataset) \u2014 Test dataset. train (Dataset) \u2014 Train dataset. val (Dataset) \u2014 Validation dataset. class","title":"ivory.core.data.Datasets"},{"location":"api/ivory.core.data/#ivorycoredatadataloaders","text":"</> Bases ivory.core.collections.Dict DataLoaders class represents a collection of DataLoader . Parameters datasets ( Datasets ) \u2014 Datasets instance. batch_size (int) \u2014 Batch_size shuffle (bool) \u2014 If True, train dataset is shuffled. Validation and test dataset are not shuffled regardless of this value. Attributes test (Dataset) \u2014 Test dataset. train (Dataset) \u2014 Train dataset. val (Dataset) \u2014 Validation dataset.","title":"ivory.core.data.DataLoaders"},{"location":"api/ivory.core/","text":"PACKAGE IVORY. CORE </> MODULE IVORY.CORE . BASE </> This module provides base classes for Ivory. Classes Base \u2014 Base class for an entity class such as Client , Experiment , and Run . </> Creator \u2014 Creator class to create Run instances. </> Callback \u2014 Callback class for the Ivory callback system. </> CallbackCaller \u2014 Callback caller class. </> Experiment \u2014 Experimet class is one of the main classes of Ivory library. </> MODULE IVORY.CORE . CLIENT </> This module provides the Ivory Client class that is one of the main classes of Ivory library. To create an Client instance: import ivory client = ivory.create_client() Here, the current directory becomes the working directory in which experiment YAML files exist. If you want to refer other directory, use: client = ivory.create_client('path/to/working_directory') Classes Client \u2014 The Ivory Client class. </> Functions create_client ( directory , name , tracker , tuner ) ( Client ) \u2014 Creates an Ivory Client instance. </> MODULE IVORY.CORE . DATA </> Ivory uses four classes for data presentation: Data , Dataset , Datasets , and DataLoaders . Basically, you only need to define a class that is a subclass of Data and use original Dataset and Datasets . An example parameter YAML file is: datasets: data: class: your.Data # a subclass of ivory.core.data.Data dataset: fold: 0 But if you need, you can define your Dataset and/or Datasets . datasets: class: your.Datasets # a subclass of ivory.core.data.Datasets data: class: your.Data # a subclass of ivory.core.data.Data dataset: def: your.Dataset # a subclass of ivory.core.data.Dataset fold: 0 The DataLoaders is used internally by ivory.torch.trainer.Trainer or ivory.nnabla.trainer.Trainer classes to yield a minibatch in training loop. Note Use a 'def' key for dataset instead of 'class' . See Tutorial Classes Data \u2014 Base class to provide data to a Dataset instance. </> Dataset \u2014 Dataset class represents a set of data for a mode and fold. </> Datasets \u2014 Dataset class represents a collection of Dataset for a fold. </> DataLoaders \u2014 DataLoaders class represents a collection of DataLoader . </> MODULE IVORY.CORE . RUN </> This module provides the Run class that is one of the main classes of Ivory library. In addition, Task and Study classes are defined, which manages multiple runs for cross validation, hyperparameter tuning, and so on. To create an Run instance: import ivory client = ivory.create_run('example') The argument example is an experiment name in which the created run is included. Ivory assumes that example.yml or example.yaml file exists under the client's working directory. You can comfirm the client's working directory by: os.path.dirname(client.source_name) One you got a Run instance. call Run.start() to start training. For test, call Run.start('test') instead. Also, you can perform traing and test by one step with Run.start('both') . Classes Run \u2014 Run class is one of the main classes of Ivory library. </> Task \u2014 Task class creates a parent run that generates multiple runs. </> Study \u2014 Study class create a parent run to manage hyperparameter tuning. </>","title":"ivory.core"},{"location":"api/ivory.core/#ivorycore","text":"</> MODULE","title":"ivory.core"},{"location":"api/ivory.core/#ivorycorebase","text":"</> This module provides base classes for Ivory. Classes Base \u2014 Base class for an entity class such as Client , Experiment , and Run . </> Creator \u2014 Creator class to create Run instances. </> Callback \u2014 Callback class for the Ivory callback system. </> CallbackCaller \u2014 Callback caller class. </> Experiment \u2014 Experimet class is one of the main classes of Ivory library. </> MODULE","title":"ivory.core.base"},{"location":"api/ivory.core/#ivorycoreclient","text":"</> This module provides the Ivory Client class that is one of the main classes of Ivory library. To create an Client instance: import ivory client = ivory.create_client() Here, the current directory becomes the working directory in which experiment YAML files exist. If you want to refer other directory, use: client = ivory.create_client('path/to/working_directory') Classes Client \u2014 The Ivory Client class. </> Functions create_client ( directory , name , tracker , tuner ) ( Client ) \u2014 Creates an Ivory Client instance. </> MODULE","title":"ivory.core.client"},{"location":"api/ivory.core/#ivorycoredata","text":"</> Ivory uses four classes for data presentation: Data , Dataset , Datasets , and DataLoaders . Basically, you only need to define a class that is a subclass of Data and use original Dataset and Datasets . An example parameter YAML file is: datasets: data: class: your.Data # a subclass of ivory.core.data.Data dataset: fold: 0 But if you need, you can define your Dataset and/or Datasets . datasets: class: your.Datasets # a subclass of ivory.core.data.Datasets data: class: your.Data # a subclass of ivory.core.data.Data dataset: def: your.Dataset # a subclass of ivory.core.data.Dataset fold: 0 The DataLoaders is used internally by ivory.torch.trainer.Trainer or ivory.nnabla.trainer.Trainer classes to yield a minibatch in training loop. Note Use a 'def' key for dataset instead of 'class' . See Tutorial Classes Data \u2014 Base class to provide data to a Dataset instance. </> Dataset \u2014 Dataset class represents a set of data for a mode and fold. </> Datasets \u2014 Dataset class represents a collection of Dataset for a fold. </> DataLoaders \u2014 DataLoaders class represents a collection of DataLoader . </> MODULE","title":"ivory.core.data"},{"location":"api/ivory.core/#ivorycorerun","text":"</> This module provides the Run class that is one of the main classes of Ivory library. In addition, Task and Study classes are defined, which manages multiple runs for cross validation, hyperparameter tuning, and so on. To create an Run instance: import ivory client = ivory.create_run('example') The argument example is an experiment name in which the created run is included. Ivory assumes that example.yml or example.yaml file exists under the client's working directory. You can comfirm the client's working directory by: os.path.dirname(client.source_name) One you got a Run instance. call Run.start() to start training. For test, call Run.start('test') instead. Also, you can perform traing and test by one step with Run.start('both') . Classes Run \u2014 Run class is one of the main classes of Ivory library. </> Task \u2014 Task class creates a parent run that generates multiple runs. </> Study \u2014 Study class create a parent run to manage hyperparameter tuning. </>","title":"ivory.core.run"},{"location":"api/ivory.core.run/","text":"MODULE IVORY.CORE . RUN </> This module provides the Run class that is one of the main classes of Ivory library. In addition, Task and Study classes are defined, which manages multiple runs for cross validation, hyperparameter tuning, and so on. To create an Run instance: import ivory client = ivory.create_run('example') The argument example is an experiment name in which the created run is included. Ivory assumes that example.yml or example.yaml file exists under the client's working directory. You can comfirm the client's working directory by: os.path.dirname(client.source_name) One you got a Run instance. call Run.start() to start training. For test, call Run.start('test') instead. Also, you can perform traing and test by one step with Run.start('both') . Classes Run \u2014 Run class is one of the main classes of Ivory library. </> Task \u2014 Task class creates a parent run that generates multiple runs. </> Study \u2014 Study class create a parent run to manage hyperparameter tuning. </> class ivory.core.run . Run ( params=None , **instances ) </> Bases ivory.core.base.CallbackCaller ivory.core.base.Creator ivory.core.base.Base ivory.core.collections.Dict Run class is one of the main classes of Ivory library. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . mode \u2014 name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. Methods create_callbacks ( ) \u2014 Creates callback functions and store them in a dictionary. </> create_instance ( instance_name , args , name , **kwargs ) \u2014 Creates an member instance of a Run according to arguments. </> create_params ( args , name , **kwargs ) (dict, dict) \u2014 Returns a tuple of (parameter dictionary, update dictionary). </> create_run ( args , name , **kwargs ) (Run) \u2014 Creates a Run instance according to arguments. </> load ( directory ) (dict(str: any)) \u2014 Loads member instances. </> load_state_dict ( state_dict ) \u2014 Loads a state dictionary to all of member instances. </> save ( directory ) \u2014 Saves member instances. </> set_tracker ( tracker , name ) \u2014 Sets tracker for tracking. </> start ( mode ) \u2014 Starts traing and/or test. </> state_dict ( ) (dict) \u2014 Returns a state dictionary for all of member instances. </> method create_params ( args=None , name='run' , **kwargs ) \u2192 (dict, dict) </> Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Examples Use args for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The params is the full parameter dictionary, while the update is a part of params , i.e., update = {'hidden_sizes.0': 100, 'fold': 3} . method create_run ( args=None , name='run' , **kwargs ) </> Creates a Run instance according to arguments. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns (Run) Created Run instance. The parameter for this instance is the returned dictionary from the create_params() function. method create_instance ( instance_name , args=None , name='run' , **kwargs ) </> Creates an member instance of a Run according to arguments. Parameters instance_name (str) \u2014 Name of a member instance to create. args (dict, optional) \u2014 Update dictionary. name (optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns Created instance. The parameter for this instance is the returned directory from the create_params() function. method create_callbacks ( ) </> Creates callback functions and store them in a dictionary. method set_tracker ( tracker , name ) </> Sets tracker for tracking. By setting a tracker, a Run instance can be a run of MLFlow Tracking at the same time. MLFlow Tracking gives the Run ID and name for this instance. Parameters tracker (Tracker) \u2014 Tracker instance. name (str) \u2014 method start ( mode='train' ) </> Starts traing and/or test. Parameters mode (str, optional) \u2014 Mode name: 'train' , 'test' , or 'both' . method state_dict ( ) \u2192 dict </> Returns a state dictionary for all of member instances. method load_state_dict ( state_dict ) </> Loads a state dictionary to all of member instances. Parameters state_dict (dict) \u2014 state dictionary for all of member instances. method save ( directory ) </> Saves member instances. Parameters directory (str) \u2014 Directory where member instances are saved. method load ( directory ) \u2192 dict(str: any) </> Loads member instances. Parameters directory (str) \u2014 Directory where member instances have been saved. class ivory.core.run . Task ( params=None , **instances ) </> Bases ivory.core.run.Run ivory.core.base.CallbackCaller ivory.core.base.Creator ivory.core.base.Base ivory.core.collections.Dict Task class creates a parent run that generates multiple runs. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . mode \u2014 name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. Methods chain ( params , use_best_param , **kwargs ) ( Run ) \u2014 Makes a chain iterator. </> create_callbacks ( ) \u2014 Creates callback functions and store them in a dictionary. </> create_instance ( instance_name , args , name , **kwargs ) \u2014 Creates an member instance of a Run according to arguments. </> create_params ( args , name , **kwargs ) (dict, dict) \u2014 Returns a tuple of (parameter dictionary, update dictionary). </> create_run ( args , **kwargs ) (Run) \u2014 Create a nested run according to arguments </> load ( directory ) (dict(str: any)) \u2014 Loads member instances. </> load_state_dict ( state_dict ) \u2014 Loads a state dictionary to all of member instances. </> product ( params , repeat , **kwargs ) ( Run ) \u2014 Makes a product iterator. </> save ( directory ) \u2014 Saves member instances. </> set_tracker ( tracker , name ) \u2014 Sets tracker for tracking. </> start ( mode ) \u2014 Starts traing and/or test. </> state_dict ( ) (dict) \u2014 Returns a state dictionary for all of member instances. </> method create_params ( args=None , name='run' , **kwargs ) \u2192 (dict, dict) </> Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Examples Use args for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The params is the full parameter dictionary, while the update is a part of params , i.e., update = {'hidden_sizes.0': 100, 'fold': 3} . method create_instance ( instance_name , args=None , name='run' , **kwargs ) </> Creates an member instance of a Run according to arguments. Parameters instance_name (str) \u2014 Name of a member instance to create. args (dict, optional) \u2014 Update dictionary. name (optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns Created instance. The parameter for this instance is the returned directory from the create_params() function. method create_callbacks ( ) </> Creates callback functions and store them in a dictionary. method set_tracker ( tracker , name ) </> Sets tracker for tracking. By setting a tracker, a Run instance can be a run of MLFlow Tracking at the same time. MLFlow Tracking gives the Run ID and name for this instance. Parameters tracker (Tracker) \u2014 Tracker instance. name (str) \u2014 method start ( mode='train' ) </> Starts traing and/or test. Parameters mode (str, optional) \u2014 Mode name: 'train' , 'test' , or 'both' . method state_dict ( ) \u2192 dict </> Returns a state dictionary for all of member instances. method load_state_dict ( state_dict ) </> Loads a state dictionary to all of member instances. Parameters state_dict (dict) \u2014 state dictionary for all of member instances. method save ( directory ) </> Saves member instances. Parameters directory (str) \u2014 Directory where member instances are saved. method load ( directory ) \u2192 dict(str: any) </> Loads member instances. Parameters directory (str) \u2014 Directory where member instances have been saved. method create_run ( args , **kwargs ) </> Create a nested run according to arguments Parameters args (dict, optional) \u2014 Update dictionary. **kwargs \u2014 Additional update dictionary. Returns (Run) Created nested Run instance. generator product ( params=None , repeat=1 , **kwargs ) \u2192 Run </> Makes a product iterator. This iterator returns runs from cartesian product of input parameters. Parameters params (dict, optional) \u2014 Parameter range. Key is a parameter name and value is an iterable of parameter's value. repeat (int, optional) \u2014 Number of repeatation. **kwargs \u2014 Additional parameter range. See Also Product section in Multiple Runs Tutorial generator chain ( params=None , use_best_param=True , **kwargs ) \u2192 Run </> Makes a chain iterator. This iterator returns runs from the first input paramter until it is exhausted, then proceeds to the next parameter, until all of the parameters are exhausted. Other parameters have default values if they don't be specified by additional key-value pairs. Parameters params (dict, optional) \u2014 Parameter range. Key is a parameter name and value is an iterable of parameter's value. use_best_param: If True (default), the parameter that got the best score is used during the following iterations. use_best_param (bool, optional) \u2014 **kwargs \u2014 Additional parameter range. See Also Chain section in Multiple Runs Tutorial class ivory.core.run . Study ( params=None , **instances ) </> Bases ivory.core.run.Task ivory.core.run.Run ivory.core.base.CallbackCaller ivory.core.base.Creator ivory.core.base.Base ivory.core.collections.Dict Study class create a parent run to manage hyperparameter tuning. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . mode \u2014 name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. Methods chain ( params , use_best_param , **kwargs ) ( Run ) \u2014 Makes a chain iterator. </> create_callbacks ( ) \u2014 Creates callback functions and store them in a dictionary. </> create_instance ( instance_name , args , name , **kwargs ) \u2014 Creates an member instance of a Run according to arguments. </> create_params ( args , name , **kwargs ) (dict, dict) \u2014 Returns a tuple of (parameter dictionary, update dictionary). </> create_run ( args , **kwargs ) (Run) \u2014 Create a nested run according to arguments </> load ( directory ) (dict(str: any)) \u2014 Loads member instances. </> load_state_dict ( state_dict ) \u2014 Loads a state dictionary to all of member instances. </> optimize ( suggest_name , **kwargs ) \u2014 Performs parameter optimizations using Optuna. </> product ( params , repeat , **kwargs ) ( Run ) \u2014 Makes a product iterator. </> save ( directory ) \u2014 Saves member instances. </> set_tracker ( tracker , name ) \u2014 Sets tracker for tracking. </> start ( mode ) \u2014 Starts traing and/or test. </> state_dict ( ) (dict) \u2014 Returns a state dictionary for all of member instances. </> method create_params ( args=None , name='run' , **kwargs ) \u2192 (dict, dict) </> Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Examples Use args for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The params is the full parameter dictionary, while the update is a part of params , i.e., update = {'hidden_sizes.0': 100, 'fold': 3} . method create_instance ( instance_name , args=None , name='run' , **kwargs ) </> Creates an member instance of a Run according to arguments. Parameters instance_name (str) \u2014 Name of a member instance to create. args (dict, optional) \u2014 Update dictionary. name (optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns Created instance. The parameter for this instance is the returned directory from the create_params() function. method create_callbacks ( ) </> Creates callback functions and store them in a dictionary. method set_tracker ( tracker , name ) </> Sets tracker for tracking. By setting a tracker, a Run instance can be a run of MLFlow Tracking at the same time. MLFlow Tracking gives the Run ID and name for this instance. Parameters tracker (Tracker) \u2014 Tracker instance. name (str) \u2014 method start ( mode='train' ) </> Starts traing and/or test. Parameters mode (str, optional) \u2014 Mode name: 'train' , 'test' , or 'both' . method state_dict ( ) \u2192 dict </> Returns a state dictionary for all of member instances. method load_state_dict ( state_dict ) </> Loads a state dictionary to all of member instances. Parameters state_dict (dict) \u2014 state dictionary for all of member instances. method save ( directory ) </> Saves member instances. Parameters directory (str) \u2014 Directory where member instances are saved. method load ( directory ) \u2192 dict(str: any) </> Loads member instances. Parameters directory (str) \u2014 Directory where member instances have been saved. method create_run ( args , **kwargs ) </> Create a nested run according to arguments Parameters args (dict, optional) \u2014 Update dictionary. **kwargs \u2014 Additional update dictionary. Returns (Run) Created nested Run instance. generator product ( params=None , repeat=1 , **kwargs ) \u2192 Run </> Makes a product iterator. This iterator returns runs from cartesian product of input parameters. Parameters params (dict, optional) \u2014 Parameter range. Key is a parameter name and value is an iterable of parameter's value. repeat (int, optional) \u2014 Number of repeatation. **kwargs \u2014 Additional parameter range. See Also Product section in Multiple Runs Tutorial generator chain ( params=None , use_best_param=True , **kwargs ) \u2192 Run </> Makes a chain iterator. This iterator returns runs from the first input paramter until it is exhausted, then proceeds to the next parameter, until all of the parameters are exhausted. Other parameters have default values if they don't be specified by additional key-value pairs. Parameters params (dict, optional) \u2014 Parameter range. Key is a parameter name and value is an iterable of parameter's value. use_best_param: If True (default), the parameter that got the best score is used during the following iterations. use_best_param (bool, optional) \u2014 **kwargs \u2014 Additional parameter range. See Also Chain section in Multiple Runs Tutorial method optimize ( suggest_name='' , **kwargs ) </> Performs parameter optimizations using Optuna. Parameters suggest_name (str, optional) \u2014 Name of suggest function. **kwargs \u2014 Key-iterable pairs for parametric optimization. See Also Hyperparameter Tuning in Tutorial","title":"ivory.core.run"},{"location":"api/ivory.core.run/#ivorycorerun","text":"</> This module provides the Run class that is one of the main classes of Ivory library. In addition, Task and Study classes are defined, which manages multiple runs for cross validation, hyperparameter tuning, and so on. To create an Run instance: import ivory client = ivory.create_run('example') The argument example is an experiment name in which the created run is included. Ivory assumes that example.yml or example.yaml file exists under the client's working directory. You can comfirm the client's working directory by: os.path.dirname(client.source_name) One you got a Run instance. call Run.start() to start training. For test, call Run.start('test') instead. Also, you can perform traing and test by one step with Run.start('both') . Classes Run \u2014 Run class is one of the main classes of Ivory library. </> Task \u2014 Task class creates a parent run that generates multiple runs. </> Study \u2014 Study class create a parent run to manage hyperparameter tuning. </> class","title":"ivory.core.run"},{"location":"api/ivory.core.run/#ivorycorerunrun","text":"</> Bases ivory.core.base.CallbackCaller ivory.core.base.Creator ivory.core.base.Base ivory.core.collections.Dict Run class is one of the main classes of Ivory library. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . mode \u2014 name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. Methods create_callbacks ( ) \u2014 Creates callback functions and store them in a dictionary. </> create_instance ( instance_name , args , name , **kwargs ) \u2014 Creates an member instance of a Run according to arguments. </> create_params ( args , name , **kwargs ) (dict, dict) \u2014 Returns a tuple of (parameter dictionary, update dictionary). </> create_run ( args , name , **kwargs ) (Run) \u2014 Creates a Run instance according to arguments. </> load ( directory ) (dict(str: any)) \u2014 Loads member instances. </> load_state_dict ( state_dict ) \u2014 Loads a state dictionary to all of member instances. </> save ( directory ) \u2014 Saves member instances. </> set_tracker ( tracker , name ) \u2014 Sets tracker for tracking. </> start ( mode ) \u2014 Starts traing and/or test. </> state_dict ( ) (dict) \u2014 Returns a state dictionary for all of member instances. </> method","title":"ivory.core.run.Run"},{"location":"api/ivory.core.run/#ivorycorebasecreatorcreate_params","text":"</> Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Examples Use args for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The params is the full parameter dictionary, while the update is a part of params , i.e., update = {'hidden_sizes.0': 100, 'fold': 3} . method","title":"ivory.core.base.Creator.create_params"},{"location":"api/ivory.core.run/#ivorycorebasecreatorcreate_run","text":"</> Creates a Run instance according to arguments. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns (Run) Created Run instance. The parameter for this instance is the returned dictionary from the create_params() function. method","title":"ivory.core.base.Creator.create_run"},{"location":"api/ivory.core.run/#ivorycorebasecreatorcreate_instance","text":"</> Creates an member instance of a Run according to arguments. Parameters instance_name (str) \u2014 Name of a member instance to create. args (dict, optional) \u2014 Update dictionary. name (optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns Created instance. The parameter for this instance is the returned directory from the create_params() function. method","title":"ivory.core.base.Creator.create_instance"},{"location":"api/ivory.core.run/#ivorycorebasecallbackcallercreate_callbacks","text":"</> Creates callback functions and store them in a dictionary. method","title":"ivory.core.base.CallbackCaller.create_callbacks"},{"location":"api/ivory.core.run/#ivorycorerunrunset_tracker","text":"</> Sets tracker for tracking. By setting a tracker, a Run instance can be a run of MLFlow Tracking at the same time. MLFlow Tracking gives the Run ID and name for this instance. Parameters tracker (Tracker) \u2014 Tracker instance. name (str) \u2014 method","title":"ivory.core.run.Run.set_tracker"},{"location":"api/ivory.core.run/#ivorycorerunrunstart","text":"</> Starts traing and/or test. Parameters mode (str, optional) \u2014 Mode name: 'train' , 'test' , or 'both' . method","title":"ivory.core.run.Run.start"},{"location":"api/ivory.core.run/#ivorycorerunrunstate_dict","text":"</> Returns a state dictionary for all of member instances. method","title":"ivory.core.run.Run.state_dict"},{"location":"api/ivory.core.run/#ivorycorerunrunload_state_dict","text":"</> Loads a state dictionary to all of member instances. Parameters state_dict (dict) \u2014 state dictionary for all of member instances. method","title":"ivory.core.run.Run.load_state_dict"},{"location":"api/ivory.core.run/#ivorycorerunrunsave","text":"</> Saves member instances. Parameters directory (str) \u2014 Directory where member instances are saved. method","title":"ivory.core.run.Run.save"},{"location":"api/ivory.core.run/#ivorycorerunrunload","text":"</> Loads member instances. Parameters directory (str) \u2014 Directory where member instances have been saved. class","title":"ivory.core.run.Run.load"},{"location":"api/ivory.core.run/#ivorycoreruntask","text":"</> Bases ivory.core.run.Run ivory.core.base.CallbackCaller ivory.core.base.Creator ivory.core.base.Base ivory.core.collections.Dict Task class creates a parent run that generates multiple runs. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . mode \u2014 name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. Methods chain ( params , use_best_param , **kwargs ) ( Run ) \u2014 Makes a chain iterator. </> create_callbacks ( ) \u2014 Creates callback functions and store them in a dictionary. </> create_instance ( instance_name , args , name , **kwargs ) \u2014 Creates an member instance of a Run according to arguments. </> create_params ( args , name , **kwargs ) (dict, dict) \u2014 Returns a tuple of (parameter dictionary, update dictionary). </> create_run ( args , **kwargs ) (Run) \u2014 Create a nested run according to arguments </> load ( directory ) (dict(str: any)) \u2014 Loads member instances. </> load_state_dict ( state_dict ) \u2014 Loads a state dictionary to all of member instances. </> product ( params , repeat , **kwargs ) ( Run ) \u2014 Makes a product iterator. </> save ( directory ) \u2014 Saves member instances. </> set_tracker ( tracker , name ) \u2014 Sets tracker for tracking. </> start ( mode ) \u2014 Starts traing and/or test. </> state_dict ( ) (dict) \u2014 Returns a state dictionary for all of member instances. </> method","title":"ivory.core.run.Task"},{"location":"api/ivory.core.run/#ivorycorebasecreatorcreate_params_1","text":"</> Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Examples Use args for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The params is the full parameter dictionary, while the update is a part of params , i.e., update = {'hidden_sizes.0': 100, 'fold': 3} . method","title":"ivory.core.base.Creator.create_params"},{"location":"api/ivory.core.run/#ivorycorebasecreatorcreate_instance_1","text":"</> Creates an member instance of a Run according to arguments. Parameters instance_name (str) \u2014 Name of a member instance to create. args (dict, optional) \u2014 Update dictionary. name (optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns Created instance. The parameter for this instance is the returned directory from the create_params() function. method","title":"ivory.core.base.Creator.create_instance"},{"location":"api/ivory.core.run/#ivorycorebasecallbackcallercreate_callbacks_1","text":"</> Creates callback functions and store them in a dictionary. method","title":"ivory.core.base.CallbackCaller.create_callbacks"},{"location":"api/ivory.core.run/#ivorycorerunrunset_tracker_1","text":"</> Sets tracker for tracking. By setting a tracker, a Run instance can be a run of MLFlow Tracking at the same time. MLFlow Tracking gives the Run ID and name for this instance. Parameters tracker (Tracker) \u2014 Tracker instance. name (str) \u2014 method","title":"ivory.core.run.Run.set_tracker"},{"location":"api/ivory.core.run/#ivorycorerunrunstart_1","text":"</> Starts traing and/or test. Parameters mode (str, optional) \u2014 Mode name: 'train' , 'test' , or 'both' . method","title":"ivory.core.run.Run.start"},{"location":"api/ivory.core.run/#ivorycorerunrunstate_dict_1","text":"</> Returns a state dictionary for all of member instances. method","title":"ivory.core.run.Run.state_dict"},{"location":"api/ivory.core.run/#ivorycorerunrunload_state_dict_1","text":"</> Loads a state dictionary to all of member instances. Parameters state_dict (dict) \u2014 state dictionary for all of member instances. method","title":"ivory.core.run.Run.load_state_dict"},{"location":"api/ivory.core.run/#ivorycorerunrunsave_1","text":"</> Saves member instances. Parameters directory (str) \u2014 Directory where member instances are saved. method","title":"ivory.core.run.Run.save"},{"location":"api/ivory.core.run/#ivorycorerunrunload_1","text":"</> Loads member instances. Parameters directory (str) \u2014 Directory where member instances have been saved. method","title":"ivory.core.run.Run.load"},{"location":"api/ivory.core.run/#ivorycoreruntaskcreate_run","text":"</> Create a nested run according to arguments Parameters args (dict, optional) \u2014 Update dictionary. **kwargs \u2014 Additional update dictionary. Returns (Run) Created nested Run instance. generator","title":"ivory.core.run.Task.create_run"},{"location":"api/ivory.core.run/#ivorycoreruntaskproduct","text":"</> Makes a product iterator. This iterator returns runs from cartesian product of input parameters. Parameters params (dict, optional) \u2014 Parameter range. Key is a parameter name and value is an iterable of parameter's value. repeat (int, optional) \u2014 Number of repeatation. **kwargs \u2014 Additional parameter range. See Also Product section in Multiple Runs Tutorial generator","title":"ivory.core.run.Task.product"},{"location":"api/ivory.core.run/#ivorycoreruntaskchain","text":"</> Makes a chain iterator. This iterator returns runs from the first input paramter until it is exhausted, then proceeds to the next parameter, until all of the parameters are exhausted. Other parameters have default values if they don't be specified by additional key-value pairs. Parameters params (dict, optional) \u2014 Parameter range. Key is a parameter name and value is an iterable of parameter's value. use_best_param: If True (default), the parameter that got the best score is used during the following iterations. use_best_param (bool, optional) \u2014 **kwargs \u2014 Additional parameter range. See Also Chain section in Multiple Runs Tutorial class","title":"ivory.core.run.Task.chain"},{"location":"api/ivory.core.run/#ivorycorerunstudy","text":"</> Bases ivory.core.run.Task ivory.core.run.Run ivory.core.base.CallbackCaller ivory.core.base.Creator ivory.core.base.Base ivory.core.collections.Dict Study class create a parent run to manage hyperparameter tuning. Parameters params (optional) \u2014 Parameter dictionary to create this instance. **instances \u2014 Member instances. Key is its name and value is the member instance. Attributes dict \u2014 id (str) \u2014 Instance ID given by MLFlow Tracking . mode \u2014 name (str) \u2014 Instance name. params (dict, optional) \u2014 Parameter dictionary that is used to to create this instance. source_name (str) \u2014 Name of the YAML parameter file that is used to create this instance. Methods chain ( params , use_best_param , **kwargs ) ( Run ) \u2014 Makes a chain iterator. </> create_callbacks ( ) \u2014 Creates callback functions and store them in a dictionary. </> create_instance ( instance_name , args , name , **kwargs ) \u2014 Creates an member instance of a Run according to arguments. </> create_params ( args , name , **kwargs ) (dict, dict) \u2014 Returns a tuple of (parameter dictionary, update dictionary). </> create_run ( args , **kwargs ) (Run) \u2014 Create a nested run according to arguments </> load ( directory ) (dict(str: any)) \u2014 Loads member instances. </> load_state_dict ( state_dict ) \u2014 Loads a state dictionary to all of member instances. </> optimize ( suggest_name , **kwargs ) \u2014 Performs parameter optimizations using Optuna. </> product ( params , repeat , **kwargs ) ( Run ) \u2014 Makes a product iterator. </> save ( directory ) \u2014 Saves member instances. </> set_tracker ( tracker , name ) \u2014 Sets tracker for tracking. </> start ( mode ) \u2014 Starts traing and/or test. </> state_dict ( ) (dict) \u2014 Returns a state dictionary for all of member instances. </> method","title":"ivory.core.run.Study"},{"location":"api/ivory.core.run/#ivorycorebasecreatorcreate_params_2","text":"</> Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Parameters args (dict, optional) \u2014 Update dictionary. name (str, optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Examples Use args for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The params is the full parameter dictionary, while the update is a part of params , i.e., update = {'hidden_sizes.0': 100, 'fold': 3} . method","title":"ivory.core.base.Creator.create_params"},{"location":"api/ivory.core.run/#ivorycorebasecreatorcreate_instance_2","text":"</> Creates an member instance of a Run according to arguments. Parameters instance_name (str) \u2014 Name of a member instance to create. args (dict, optional) \u2014 Update dictionary. name (optional) \u2014 Run class name in lower case. **kwargs \u2014 Additional update dictionary. Returns Created instance. The parameter for this instance is the returned directory from the create_params() function. method","title":"ivory.core.base.Creator.create_instance"},{"location":"api/ivory.core.run/#ivorycorebasecallbackcallercreate_callbacks_2","text":"</> Creates callback functions and store them in a dictionary. method","title":"ivory.core.base.CallbackCaller.create_callbacks"},{"location":"api/ivory.core.run/#ivorycorerunrunset_tracker_2","text":"</> Sets tracker for tracking. By setting a tracker, a Run instance can be a run of MLFlow Tracking at the same time. MLFlow Tracking gives the Run ID and name for this instance. Parameters tracker (Tracker) \u2014 Tracker instance. name (str) \u2014 method","title":"ivory.core.run.Run.set_tracker"},{"location":"api/ivory.core.run/#ivorycorerunrunstart_2","text":"</> Starts traing and/or test. Parameters mode (str, optional) \u2014 Mode name: 'train' , 'test' , or 'both' . method","title":"ivory.core.run.Run.start"},{"location":"api/ivory.core.run/#ivorycorerunrunstate_dict_2","text":"</> Returns a state dictionary for all of member instances. method","title":"ivory.core.run.Run.state_dict"},{"location":"api/ivory.core.run/#ivorycorerunrunload_state_dict_2","text":"</> Loads a state dictionary to all of member instances. Parameters state_dict (dict) \u2014 state dictionary for all of member instances. method","title":"ivory.core.run.Run.load_state_dict"},{"location":"api/ivory.core.run/#ivorycorerunrunsave_2","text":"</> Saves member instances. Parameters directory (str) \u2014 Directory where member instances are saved. method","title":"ivory.core.run.Run.save"},{"location":"api/ivory.core.run/#ivorycorerunrunload_2","text":"</> Loads member instances. Parameters directory (str) \u2014 Directory where member instances have been saved. method","title":"ivory.core.run.Run.load"},{"location":"api/ivory.core.run/#ivorycoreruntaskcreate_run_1","text":"</> Create a nested run according to arguments Parameters args (dict, optional) \u2014 Update dictionary. **kwargs \u2014 Additional update dictionary. Returns (Run) Created nested Run instance. generator","title":"ivory.core.run.Task.create_run"},{"location":"api/ivory.core.run/#ivorycoreruntaskproduct_1","text":"</> Makes a product iterator. This iterator returns runs from cartesian product of input parameters. Parameters params (dict, optional) \u2014 Parameter range. Key is a parameter name and value is an iterable of parameter's value. repeat (int, optional) \u2014 Number of repeatation. **kwargs \u2014 Additional parameter range. See Also Product section in Multiple Runs Tutorial generator","title":"ivory.core.run.Task.product"},{"location":"api/ivory.core.run/#ivorycoreruntaskchain_1","text":"</> Makes a chain iterator. This iterator returns runs from the first input paramter until it is exhausted, then proceeds to the next parameter, until all of the parameters are exhausted. Other parameters have default values if they don't be specified by additional key-value pairs. Parameters params (dict, optional) \u2014 Parameter range. Key is a parameter name and value is an iterable of parameter's value. use_best_param: If True (default), the parameter that got the best score is used during the following iterations. use_best_param (bool, optional) \u2014 **kwargs \u2014 Additional parameter range. See Also Chain section in Multiple Runs Tutorial method","title":"ivory.core.run.Task.chain"},{"location":"api/ivory.core.run/#ivorycorerunstudyoptimize","text":"</> Performs parameter optimizations using Optuna. Parameters suggest_name (str, optional) \u2014 Name of suggest function. **kwargs \u2014 Key-iterable pairs for parametric optimization. See Also Hyperparameter Tuning in Tutorial","title":"ivory.core.run.Study.optimize"},{"location":"api/ivory.nnabla/","text":"PACKAGE IVORY. NNABLA </> MODULE IVORY.NNABLA . TRAINER </> The ivory.nnabla.trainer module provides the Trainer class for nnable. Classes Trainer \u2014 Trainer(epoch: int = -1, epochs: int = 1, global_step: int = -1, batch_size: int = 32, shuffle: bool = True, dataloaders: str = 'ivory.nnabla.data.DataLoaders', verbose: int = 1, loss: Union[Callable, NoneType] = None, gpu: bool = False, precision: int = 32, amp_level: str = 'O1') </>","title":"ivory.nnabla"},{"location":"api/ivory.nnabla/#ivorynnabla","text":"</> MODULE","title":"ivory.nnabla"},{"location":"api/ivory.nnabla/#ivorynnablatrainer","text":"</> The ivory.nnabla.trainer module provides the Trainer class for nnable. Classes Trainer \u2014 Trainer(epoch: int = -1, epochs: int = 1, global_step: int = -1, batch_size: int = 32, shuffle: bool = True, dataloaders: str = 'ivory.nnabla.data.DataLoaders', verbose: int = 1, loss: Union[Callable, NoneType] = None, gpu: bool = False, precision: int = 32, amp_level: str = 'O1') </>","title":"ivory.nnabla.trainer"},{"location":"api/ivory.nnabla.trainer/","text":"MODULE IVORY.NNABLA . TRAINER </> The ivory.nnabla.trainer module provides the Trainer class for nnable. Classes Trainer \u2014 Trainer(epoch: int = -1, epochs: int = 1, global_step: int = -1, batch_size: int = 32, shuffle: bool = True, dataloaders: str = 'ivory.nnabla.data.DataLoaders', verbose: int = 1, loss: Union[Callable, NoneType] = None, gpu: bool = False, precision: int = 32, amp_level: str = 'O1') </> dataclass ivory.nnabla.trainer . Trainer ( epoch=-1 , epochs=1 , global_step=-1 , batch_size=32 , shuffle=True , dataloaders='ivory.nnabla.data.DataLoaders' , verbose=1 , loss=None , gpu=False , precision=32 , amp_level='O1' ) </> Bases ivory.core.trainer.Trainer ivory.core.state.State Trainer(epoch: int = -1, epochs: int = 1, global_step: int = -1, batch_size: int = 32, shuffle: bool = True, dataloaders: str = 'ivory.nnabla.data.DataLoaders', verbose: int = 1, loss: Union[Callable, NoneType] = None, gpu: bool = False, precision: int = 32, amp_level: str = 'O1') Parameters epoch (int, optional) \u2014 epochs (int, optional) \u2014 global_step (int, optional) \u2014 batch_size (int, optional) \u2014 shuffle (bool, optional) \u2014 dataloaders (str, optional) \u2014 verbose (int, optional) \u2014 loss (callable, optional) \u2014 gpu (bool, optional) \u2014 precision (int, optional) \u2014 amp_level (str, optional) \u2014 Attributes amp_level (str) \u2014 batch_size (int) \u2014 dataloaders (str) \u2014 epoch (int) \u2014 epochs (int) \u2014 global_step (int) \u2014 gpu (bool) \u2014 loss (callable, optional) \u2014 precision (int) \u2014 shuffle (bool) \u2014 verbose (int) \u2014 Methods start ( run ) \u2014 Starts a train or test loop. </> test_step ( run , index , input , target ) \u2014 Performs a single test step. </> train_step ( run , index , input , target ) \u2014 Performs a single train step. </> val_step ( run , index , input , target ) \u2014 Performs a single validation step. </> method start ( run ) </> Starts a train or test loop. Parameters run ( Run ) \u2014 a run instance. method train_step ( run , index , input , target ) </> Performs a single train step. Parameters run ( Run ) \u2014 index \u2014 input \u2014 target \u2014 method val_step ( run , index , input , target ) </> Performs a single validation step. Parameters run ( Run ) \u2014 index \u2014 input \u2014 target \u2014 method test_step ( run , index , input , target ) </> Performs a single test step. Parameters run ( Run ) \u2014 index \u2014 input \u2014 target \u2014","title":"ivory.nnabla.trainer"},{"location":"api/ivory.nnabla.trainer/#ivorynnablatrainer","text":"</> The ivory.nnabla.trainer module provides the Trainer class for nnable. Classes Trainer \u2014 Trainer(epoch: int = -1, epochs: int = 1, global_step: int = -1, batch_size: int = 32, shuffle: bool = True, dataloaders: str = 'ivory.nnabla.data.DataLoaders', verbose: int = 1, loss: Union[Callable, NoneType] = None, gpu: bool = False, precision: int = 32, amp_level: str = 'O1') </> dataclass","title":"ivory.nnabla.trainer"},{"location":"api/ivory.nnabla.trainer/#ivorynnablatrainertrainer","text":"</> Bases ivory.core.trainer.Trainer ivory.core.state.State Trainer(epoch: int = -1, epochs: int = 1, global_step: int = -1, batch_size: int = 32, shuffle: bool = True, dataloaders: str = 'ivory.nnabla.data.DataLoaders', verbose: int = 1, loss: Union[Callable, NoneType] = None, gpu: bool = False, precision: int = 32, amp_level: str = 'O1') Parameters epoch (int, optional) \u2014 epochs (int, optional) \u2014 global_step (int, optional) \u2014 batch_size (int, optional) \u2014 shuffle (bool, optional) \u2014 dataloaders (str, optional) \u2014 verbose (int, optional) \u2014 loss (callable, optional) \u2014 gpu (bool, optional) \u2014 precision (int, optional) \u2014 amp_level (str, optional) \u2014 Attributes amp_level (str) \u2014 batch_size (int) \u2014 dataloaders (str) \u2014 epoch (int) \u2014 epochs (int) \u2014 global_step (int) \u2014 gpu (bool) \u2014 loss (callable, optional) \u2014 precision (int) \u2014 shuffle (bool) \u2014 verbose (int) \u2014 Methods start ( run ) \u2014 Starts a train or test loop. </> test_step ( run , index , input , target ) \u2014 Performs a single test step. </> train_step ( run , index , input , target ) \u2014 Performs a single train step. </> val_step ( run , index , input , target ) \u2014 Performs a single validation step. </> method","title":"ivory.nnabla.trainer.Trainer"},{"location":"api/ivory.nnabla.trainer/#ivorycoretrainertrainerstart","text":"</> Starts a train or test loop. Parameters run ( Run ) \u2014 a run instance. method","title":"ivory.core.trainer.Trainer.start"},{"location":"api/ivory.nnabla.trainer/#ivorynnablatrainertrainertrain_step","text":"</> Performs a single train step. Parameters run ( Run ) \u2014 index \u2014 input \u2014 target \u2014 method","title":"ivory.nnabla.trainer.Trainer.train_step"},{"location":"api/ivory.nnabla.trainer/#ivorynnablatrainertrainerval_step","text":"</> Performs a single validation step. Parameters run ( Run ) \u2014 index \u2014 input \u2014 target \u2014 method","title":"ivory.nnabla.trainer.Trainer.val_step"},{"location":"api/ivory.nnabla.trainer/#ivorynnablatrainertrainertest_step","text":"</> Performs a single test step. Parameters run ( Run ) \u2014 index \u2014 input \u2014 target \u2014","title":"ivory.nnabla.trainer.Trainer.test_step"},{"location":"api/source/ivory.callbacks.early_stopping/","text":"SOURCE CODE IVORY.CALLBACKS. EARLY_STOPPING DOCS \"\"\"Early stopping when a monitored metric has stopped improving.\"\"\" from dataclasses import dataclass , field from ivory.core.exceptions import EarlyStopped from ivory.core.run import Run from ivory.core.state import State @dataclass class EarlyStopping ( State ): DOCS \"\"\"Early stops a training loop when a monitored metric has stopped improving. Args: patience: Number of epochs with no improvement after which training will be stopped. Attributes: patience: Number of epochs with no improvement after which training will be stopped. wait: Number of continuous epochs with no imporovement. Raises: EarlyStopped: When ealry stopping occurs. \"\"\" patience : int wait : int = field ( default = 0 , init = False ) def on_epoch_end ( self , run : Run ): if run . monitor . is_best : self . wait = 0 else : self . wait += 1 if self . wait >= self . patience : raise EarlyStopped","title":"ivory.callbacks.early_stopping"},{"location":"api/source/ivory.callbacks/","text":"SOURCE CODE IVORY. CALLBACKS DOCS","title":"ivory.callbacks"},{"location":"api/source/ivory.callbacks.metrics/","text":"SOURCE CODE IVORY.CALLBACKS. METRICS DOCS \"\"\"Metrics to record scores while training.\"\"\" from typing import Any , Dict , List import numpy as np import ivory.core.collections from ivory.core import instance from ivory.core.run import Run from ivory.core.state import State class Metrics ( ivory . core . collections . Dict , State ): DOCS \"\"\"Metrics object.\"\"\" def __init__ ( self , ** kwargs ): super () . __init__ () self . metrics_fn = {} for key , value in kwargs . items (): self . metrics_fn [ key ] = get_metric_function ( key , value ) self . history = ivory . core . collections . Dict () def __str__ ( self ): metrics = [] for metric in self : metrics . append ( f \" { metric } = { self [ metric ] : .4g } \" ) return \" \" . join ( metrics ) def __repr__ ( self ): class_name = self . __class__ . __name__ args = str ( self ) . replace ( \" \" , \", \" ) return f \" { class_name } ( { args } )\" def on_epoch_begin ( self , run : Run ): if run . trainer : self . epoch = run . trainer . epoch else : self . epoch = 0 def on_epoch_end ( self , run : Run ): self . update ( self . metrics_dict ( run )) self . update_history () def update_history ( self ): for metric , value in self . items (): if metric not in self . history : self . history [ metric ] = { self . epoch : value } else : self . history [ metric ][ self . epoch ] = value def metrics_dict ( self , run : Run ) -> Dict [ str , Any ]: DOCS \"\"\"Returns an extra custom metrics dictionary.\"\"\" pred = run . results . val . output . reshape ( - 1 ) true = run . results . val . target . reshape ( - 1 ) metrics_dict = {} for key , func in self . metrics_fn . items (): metrics_dict [ key ] = func ( true , pred ) return metrics_dict class BatchMetrics ( Metrics ): DOCS def on_epoch_begin ( self , run : Run ): self . epoch = run . trainer . epoch def on_train_begin ( self , run : Run ): self . losses : List [ float ] = [] def step ( self , loss : float ): self . losses . append ( loss ) def on_train_end ( self , run : Run ): self [ \"loss\" ] = np . mean ( self . losses ) def on_val_begin ( self , run : Run ): self . losses = [] def on_val_end ( self , run : Run ): self [ \"val_loss\" ] = np . mean ( self . losses ) METRICS = { \"mse\" : \"sklearn.metrics.mean_squared_error\" } def get_metric_function ( key , value ): if value is None : if key not in METRICS : raise ValueError ( f \"Unkown metric: { key } \" ) value = METRICS [ key ] if isinstance ( value , str ) and \".\" not in value : value = f \"sklearn.metrics. { value } \" return instance . get_attr ( value )","title":"ivory.callbacks.metrics"},{"location":"api/source/ivory.callbacks.pruning/","text":"SOURCE CODE IVORY.CALLBACKS. PRUNING DOCS \"\"\"Pruning class to prune unpromising trials.\"\"\" from dataclasses import dataclass from typing import Optional import numpy as np import optuna from optuna.trial import Trial from ivory.core.exceptions import Pruned from ivory.core.run import Run @dataclass class Pruning : DOCS \"\"\"Callback to prune unpromising trials. Args: trial: A `Trial` corresponding to the current evaluation of the objective function. metric: An evaluation metric for pruning, e.g., `val_loss` \"\"\" trial : Optional [ Trial ] = None metric : str = \"\" def on_epoch_end ( self , run : Run ): if self . trial is not None : score = run . metrics [ self . metric ] if np . isnan ( score ): return epoch = run . metrics . epoch self . trial . report ( score , step = epoch ) if self . trial . should_prune (): message = f \"Trial was pruned at epoch { epoch } .\" raise optuna . exceptions . TrialPruned ( message ) if run . tracking : status = run . tracking . client . get_run ( run . id ) . info . status if status == \"KILLED\" : raise Pruned","title":"ivory.callbacks.pruning"},{"location":"api/source/ivory.callbacks.results/","text":"SOURCE CODE IVORY.CALLBACKS. RESULTS DOCS \"\"\"A container to store training, validation and test results. \"\"\" from typing import Callable , Dict , Iterable , Optional import numpy as np import pandas as pd import ivory.core.collections from ivory.core.run import Run from ivory.core.state import State class Results ( ivory . core . collections . Dict , State ): DOCS \"\"\"Results callback stores training, validation and test results. Each result is `ivory.core.collections.Dict` type that has `index`, `output`, and `target` array. To get `target` array of validation, use target = results.val.target Attributes: train (Dict): Train results. val (Dict): Validation results. test (Dict): Test results. \"\"\" def reset ( self ): self . index = None self . output = None self . target = None def on_train_begin ( self , run : Run ): self . reset () def on_test_begin ( self , run : Run ): self . reset () def step ( self , index , output , target = None ): self . index = index self . output = output self . target = target def on_train_end ( self , run : Run ): self [ \"train\" ] = self . result_dict () self . reset () def on_val_end ( self , run : Run ): self [ \"val\" ] = self . result_dict () self . reset () def on_test_end ( self , run : Run ): self [ \"test\" ] = self . result_dict () self . reset () def result_dict ( self ): dict = ivory . core . collections . Dict () return dict ( index = self . index , output = self . output , target = self . target ) def mean ( self ) -> \"Results\" : DOCS \"\"\"Returns a reduced `Results` instance aveaged by `index`.\"\"\" results = Results () for mode , result in self . items (): index = result . index kwargs = {} for key , value in list ( result . items ())[ 1 :]: if value . ndim == 1 : series = pd . Series ( value , index = index ) value = series . groupby ( level = 0 ) . mean () else : df = pd . DataFrame ( value ) df [ \"index\" ] = index value = df . groupby ( \"index\" ) . mean () value . sort_index ( inplace = True ) kwargs [ key ] = value . to_numpy () kwargs [ \"index\" ] = value . index . to_numpy () dict = ivory . core . collections . Dict () results [ mode ] = dict ( ** kwargs ) return results class BatchResults ( Results ): DOCS def reset ( self ): super () . reset () self . indexes = [] self . outputs = [] self . targets = [] def step ( self , index , output , target = None ): self . indexes . append ( index ) self . outputs . append ( output ) if target is not None : self . targets . append ( target ) def result_dict ( self ): index = np . concatenate ( self . indexes ) output = np . concatenate ( self . outputs ) if self . targets : target = np . concatenate ( self . targets ) else : target = None super () . step ( index , output , target ) return super () . result_dict () def concatenate ( DOCS iterable : Iterable [ Results ], callback : Optional [ Callable ] = None , modes : Iterable [ str ] = ( \"val\" , \"test\" ), reduction : str = \"none\" , ) -> Results : \"\"\"Returns a concatenated Results. Args: iterable (iterable of Results): Iterable of `Results` instance. callback (callable, optional): Called for each `Results`. Must take (`mode`, `index`, `output`, `target`) arguments and return a tuple of ('index', `output`, `target`). modes (iterable of str): Specify modes to concatenate. reduction (str, optional): Reduction. `none` or `mean`. \"\"\" modes = list ( modes ) indexes : Dict [ str , list ] = { mode : [] for mode in modes } outputs : Dict [ str , list ] = { mode : [] for mode in modes } targets : Dict [ str , list ] = { mode : [] for mode in modes } for results in iterable : for mode in modes : if mode not in results : continue result = results [ mode ] index , output , target = result [ \"index\" ], result [ \"output\" ], result [ \"target\" ] if callback : index , output , target = callback ( index , output , target ) indexes [ mode ] . append ( index ) outputs [ mode ] . append ( output ) targets [ mode ] . append ( target ) results = Results () for mode in modes : index = np . concatenate ( indexes [ mode ]) output = np . concatenate ( outputs [ mode ]) target = np . concatenate ( targets [ mode ]) dict = ivory . core . collections . Dict () results [ mode ] = dict ( index = index , output = output , target = target ) if reduction != \"none\" : results = getattr ( results , reduction )() return results","title":"ivory.callbacks.results"},{"location":"api/source/ivory.core.base/","text":"SOURCE CODE IVORY.CORE. BASE DOCS \"\"\"This module provides base classes for Ivory.\"\"\" import copy import inspect from typing import Callable , Dict , Tuple import ivory.core.collections from ivory import utils from ivory.core import default , instance class Base ( ivory . core . collections . Dict ): DOCS \"\"\"Base class for an entity class such as `Client`, `Experiment`, and `Run`. Args: params (dict, optional): Parameter dictionary to create this instance. **instances: Member instances. Key is its name and value is the member instance. Attributes: params (dict, optional): Parameter dictionary that is used to to create this instance. id (str): Instance ID given by [MLFlow Tracking](https://www.mlflow.org/docs/latest/tracking.html). name (str): Instance name. source_name (str): Name of the YAML parameter file that is used to create this instance. \"\"\" def __init__ ( self , params = None , ** instances ): super () . __init__ () self . params = params self . id = self . name = self . source_name = \"\" if \"id\" in instances : self . id = instances . pop ( \"id\" ) if \"name\" in instances : self . name = instances . pop ( \"name\" ) if \"source_name\" in instances : self . source_name = instances . pop ( \"source_name\" ) self . dict = instances def __repr__ ( self ): args = [] if self . id : args . append ( f \"id= { self . id !r} \" ) if self . name : args . append ( f \"name= { self . name !r} \" ) args . append ( f \"num_instances= { len ( self ) } \" ) args = \", \" . join ( args ) return f \" { self . __class__ . __name__ } ( { args } )\" class Creator ( Base ): DOCS \"\"\"Creator class to create `Run` instances.\"\"\" @property def experiment_id ( self ) -> str : return self . params [ \"experiment\" ][ \"id\" ] @property def experiment_name ( self ) -> str : return self . params [ \"experiment\" ][ \"name\" ] def create_params ( DOCS self , args = None , name : str = \"run\" , ** kwargs ) -> Tuple [ dict , dict ]: \"\"\"Returns a tuple of (parameter dictionary, update dictionary). The parameter dictionary is deeply copied from original one, then updated according to the arguments. The update dictionary includes updated parameter only. Args: args (dict, optional): Update dictionary. name: Run class name in lower case. **kwargs: Additional update dictionary. Examples: Use `args` for parameters including dots: params, update = experiment.create_params( {'hidden_sizes.0': 100}, fold=3 ) The `params` is the full parameter dictionary, while the `update` is a part of `params`, i.e., `update = {'hidden_sizes.0': 100, 'fold': 3}`. \"\"\" params = copy . deepcopy ( self . params ) if name not in params : params . update ( default . get ( name )) update , args = utils . params . create_update ( params [ name ], args , ** kwargs ) utils . params . update_dict ( params [ name ], update ) return params , args def create_run ( self , args = None , name : str = \"run\" , ** kwargs ): DOCS \"\"\"Creates a `Run` instance according to arguments. Args: args (dict, optional): Update dictionary. name: Run class name in lower case. **kwargs: Additional update dictionary. Returns: Run: Created `Run` instance. The parameter for this instance is the returned dictionary from the [`create_params()`](#ivory.core.base.Creator.create_params) function. \"\"\" params , args = self . create_params ( args , name , ** kwargs ) run = instance . create_base_instance ( params , name , self . source_name ) if self . tracker : from ivory.callbacks.pruning import Pruning run . set_tracker ( self . tracker , name ) run . tracking . log_params_artifact ( run ) args = { arg : utils . params . get_value ( run . params [ name ], arg ) for arg in args } run . tracking . log_params ( run . id , args ) run . set ( pruning = Pruning ()) return run def create_instance ( self , instance_name : str , args = None , name = \"run\" , ** kwargs ): DOCS \"\"\"Creates an member instance of a `Run` according to arguments. Args: instance_name: Name of a member instance to create. args (dict, optional): Update dictionary. name: Run class name in lower case. **kwargs: Additional update dictionary. Returns: Created instance. The parameter for this instance is the returned directory from the [`create_params()`](#ivory.core.base.Creator.create_params) function. \"\"\" params , _ = self . create_params ( args , name , ** kwargs ) return instance . create_instance ( params [ name ], instance_name ) class Callback : DOCS \"\"\"Callback class for the Ivory callback system.\"\"\" METHODS = [ \"on_init_begin\" , \"on_init_end\" , \"on_fit_begin\" , \"on_epoch_begin\" , \"on_train_begin\" , \"on_train_end\" , \"on_val_begin\" , \"on_val_end\" , \"on_epoch_end\" , \"on_fit_end\" , \"on_test_begin\" , \"on_test_end\" , ] ARGUMENTS = [ \"run\" ] def __init__ ( self , caller : \"CallbackCaller\" , methods : Dict [ str , Callable ]): self . caller = caller self . methods = methods def __repr__ ( self ): class_name = self . __class__ . __name__ callbacks = list ( self . methods . keys ()) return f \" { class_name } ( { callbacks } )\" def __call__ ( self ): caller = self . caller for method in self . methods . values (): method ( caller ) class CallbackCaller ( Creator ): DOCS \"\"\"Callback caller class.\"\"\" def create_callbacks ( self ): DOCS \"\"\"Creates callback functions and store them in a dictionary.\"\"\" for method in Callback . METHODS : methods = {} for key in self : if hasattr ( self [ key ], method ): callback = getattr ( self [ key ], method ) if callable ( callback ): parameters = inspect . signature ( callback ) . parameters if list ( parameters . keys ()) == Callback . ARGUMENTS : methods [ key ] = callback self [ method ] = Callback ( self , methods ) class Experiment ( Creator ): DOCS \"\"\"Experimet class is one of the main classes of Ivory library. Basically, one experiment is corresponding to one YAML parameter file that is held in an `Experiment` instance as a parameter dictionary. This parameter dictionary defines the default parameter values to create `Run` instances. See Also: The base class [`ivory.core.base.Creator`](#ivory.core.base.Creator) defines some functions to create a `Run` instance or its member instance. \"\"\" def set_tracker ( self , tracker ): DOCS \"\"\"Sets a `Tracker` instance for tracking. Args: tracker (Tracker): Tracker instance. \"\"\" if not self . id : self . id = tracker . create_experiment ( self . name ) self . params [ \"experiment\" ][ \"id\" ] = self . id self . set ( tracker = tracker ) def create_task ( self ): DOCS \"\"\"Creates a `Task` instance for multiple runs. See Also: For more details, see [client.create_task()](/api/ivory.core.client#ivory.core.client.Client.create_task) [Multiple Runs](/tutorial/task) in Tutorial. \"\"\" return self . create_run ( name = \"task\" ) def create_study ( self , args = None , ** suggests ): DOCS \"\"\"Creates a `Study` instance for hyperparameter tuning. See Also: For more details, see [client.create_study()](/api/ivory.core.client#ivory.core.client.Client.create_study) [Hyperparameter Tuning](/tutorial/tuning) in Tutorial \"\"\" study = self . create_run ( name = \"study\" ) if isinstance ( args , str ) and args in study . objective : study . objective . suggests = { args : study . objective . suggests [ args ]} return study if args or suggests : study . objective . update ( args , ** suggests ) return study","title":"ivory.core.base"},{"location":"api/source/ivory.core.client/","text":"SOURCE CODE IVORY.CORE. CLIENT DOCS \"\"\" This module provides the Ivory Client class that is one of the main classes of Ivory library. To create an `Client` instance: import ivory client = ivory.create_client() Here, the current directory becomes the working directory in which experiment YAML files exist. If you want to refer other directory, use: client = ivory.create_client('path/to/working_directory') \"\"\" import os import re import subprocess import sys from typing import Any , Dict , Iterable , Iterator , Optional , Tuple , Union import ivory.callbacks.results from ivory import utils from ivory.callbacks.results import Results from ivory.core import default , instance from ivory.core.base import Base , Experiment from ivory.core.run import Run , Study , Task from ivory.utils.tqdm import tqdm class Client ( Base ): DOCS \"\"\"The Ivory Client class. Attributes: tracker (Tracker): A Tracker instance for tracking run process. tuner (Tuner): A Tuner instance for hyperparameter tuning. \"\"\" def __init__ ( self , params = None , ** objects ): super () . __init__ ( params , ** objects ) self . experiments : Dict [ str , Experiment ] = {} def create_experiment ( self , name : str , * args , ** kwargs ) -> Experiment : DOCS \"\"\"Creates an `Experiment` according to the YAML file specified by `name`. Args: name: Experiment name. *args: Additional parameter files. **kwargs: Additional parameter files. A YAML file named `<name>.yml` or `<name>.yaml` should exist under the working directory. Any additionanl parameter files are added through `*args` and/or `**kwargs`. Examples: **Positional argument style**: experiment = client.create_experiment('example', 'study') In this case, `study.yml` is like this, including the instance name `study`: study: tuner: pruner: class: optuna.pruners.MedianPruner objective: lr: example.suggest_lr **Keyword argument style**: experiment = client.create_experiment('example', study='study') In this case, `study.yml` is like this, omitting the instance name `study`: tuner: pruner: class: optuna.pruners.MedianPruner objective: lr: example.suggest_lr \"\"\" if name in self . experiments and not args and not kwargs : return self . experiments [ name ] params , source_name = utils . path . load_params ( name , self . source_name ) if \"run\" not in params : params = { \"run\" : params } if \"experiment\" not in params : params . update ( default . get ( \"experiment\" )) if \"name\" not in params [ \"experiment\" ]: params [ \"experiment\" ][ \"name\" ] = name for value in args : option , _ = utils . path . load_params ( value , self . source_name ) params . update ( option ) for key , value in kwargs . items (): option , _ = utils . path . load_params ( value , self . source_name ) if key not in option : option = { key : option } params . update ( option ) experiment = instance . create_base_instance ( params , \"experiment\" , source_name ) if self . tracker : experiment . set_tracker ( self . tracker ) self . experiments [ name ] = experiment return experiment def create_run ( self , name : str , args = None , ** kwargs ) -> Run : DOCS \"\"\"Creates a `Run`. Args: name: Experiment name. args (dict, optional): Parameter dictionary to update the default values of `Experiment`. **kwargs: Additional parameters. Examples: To update a fold number: run = client.create_run('example', fold=3) If a parameter name includes dots: run = client.create_run('example', {'model.class': 'your.new.Model'}) \"\"\" return self . create_experiment ( name ) . create_run ( args , ** kwargs ) def create_task ( self , name : str , run_number : Optional [ int ] = None ) -> Task : DOCS \"\"\"Creates a `Task` instance for multiple runs. Args: name: Experiment name. run_number (int, optional): If specified, load an existing task instead of creating a new one. See Also: [Multiple Runs](/tutorial/task) in Tutorial \"\"\" if run_number is None : return self . create_experiment ( name ) . create_task () else : return self . load_run_by_name ( name , task = run_number ) # type:ignore def create_study ( DOCS self , name : str , args = None , run_number : Optional [ int ] = None , ** suggests ) -> Study : \"\"\"Creates a `Study` instance for hyperparameter tuning. Args: name: Experiment name. args (str or dict): Suggest name (str) or parametric optimization (dict). run_number (int, optional): If specified, load an existing study instead of creating a new one. **suggests: Parametric optimization. Examples: To use a suggest function: study = client.create_study('example', 'lr') For parametric optimization: study = client.create_study('example', lr=(1e-5, 1e-3)) If a parameter name includes dots: study = client.create_study('example', {'hidden_sizes.0': range(5, 20)}) See Also: [Hyperparameter Tuning](/tutorial/tuning) in Tutorial \"\"\" if run_number is None : study = self . create_experiment ( name ) . create_study ( args , ** suggests ) else : study = self . load_run_by_name ( name , study = run_number ) if self . tuner and \"storage\" not in study . params [ \"study\" ][ \"tuner\" ]: study . set ( tuner = self . tuner ) return study def get_run_id ( self , name : str , ** kwargs ) -> str : DOCS \"\"\"Returns a Run ID. Args: name: Experiment name. Examples: To get a Run ID of run#4. client.get_run_id('example', run=4) To get a Run ID of task#10. client.get_run_id('example', task=10) \"\"\" run_name = list ( kwargs )[ 0 ] run_number = kwargs [ run_name ] if run_number == - 1 : return next ( self . search_run_ids ( name , run_name )) else : experiment_id = self . tracker . get_experiment_id ( name ) return self . tracker . get_run_id ( experiment_id , run_name , run_number ) def get_run_ids ( self , name : str , ** kwargs ) -> Iterator [ str ]: DOCS \"\"\"Returns an iterator that yields Run IDs. Args: name: Experiment name. Examples: To get an iterator that yields Run IDs for Runs. client.get_run_id('example', run=[1, 2, 3]) To get an iterator that yields Run IDs for Tasks. client.get_run_id('example', task=range(3, 8)) \"\"\" for run_name , run_numbers in kwargs . items (): if isinstance ( run_numbers , int ): run_numbers = [ run_numbers ] for run_number in run_numbers : yield self . get_run_id ( name , ** { run_name : run_number }) def get_parent_run_id ( self , name : str , ** kwargs ) -> str : DOCS \"\"\"Returns a parent Run ID of a nested run. Args: name: Experiment name. Examples: To get a prarent Run ID of run#5. client.get_parent_run_id('example', run=5) \"\"\" run_id = self . get_run_id ( name , ** kwargs ) return self . tracker . get_parent_run_id ( run_id ) def get_nested_run_ids ( self , name : str , ** kwargs ) -> Iterator [ str ]: DOCS \"\"\"Returns an iterator that yields nested Run IDs of parent runs. Args: name: Experiment name. Examples: To get an iterator that yields Run IDs of runs whose parent is task#2. client.get_nested_run_ids('example', task=2) Multiple parents can be specified. client.get_nested_run_ids('example', task=range(3, 8)) \"\"\" run_name = list ( kwargs )[ 0 ] run_numbers = kwargs . pop ( run_name ) parent_run_ids = self . get_run_ids ( name , ** { run_name : run_numbers }) yield from self . search_run_ids ( name , parent_run_id = parent_run_ids , ** kwargs ) def set_parent_run_id ( self , name : str , ** kwargs ): DOCS \"\"\"Sets parent Run ID to runs. Args: name: Experiment name. Examples: To set task#2 as a parant for run#4. client.set_parent_run_id('example', task=2, run=4) Multiple nested runs can be specified. client.set_parent_run_id('example', task=2, run=range(3)) \"\"\" parent = { name : number for name , number in kwargs . items () if name != \"run\" } parent_run_id = self . get_run_id ( name , ** parent ) for run_id in self . get_run_ids ( name , run = kwargs [ \"run\" ]): self . tracker . set_parent_run_id ( run_id , parent_run_id ) def get_run_name ( self , run_id : str ) -> str : DOCS \"\"\"Returns a run name (`run#XXX`, `task#XXX`, *etc*.) for Run ID. Args: run_id: Run ID \"\"\" return self . tracker . get_run_name ( run_id ) def get_run_name_tuple ( self , run_id : str ) -> Tuple [ str , int ]: DOCS \"\"\"Returns a run name as a tuple of (run class name, run number). Args: run_id: Run ID \"\"\" return self . tracker . get_run_name_tuple ( run_id ) def search_run_ids ( DOCS self , name : str = \"\" , run_name : str = \"\" , parent_run_id : Union [ str , Iterable [ str ]] = \"\" , parent_only : bool = False , nested_only : bool = False , exclude_parent : bool = False , best_score_limit : Optional [ float ] = None , ** query , ) -> Iterator [ str ]: \"\"\"Returns an iterator that yields matching Run IDs. Args: name: Experiment name pattern for filtering. run_name: Run name pattern for filtering. parent_run_id (str or iterable of str): If specified, search from runs that have the parent id(s). parent_only: If True, search from parent runs. nested_only: If True, search from nested runs. exclude_parent: If True, skip parent runs. best_score_limit: Yields runs with the best score better than this value. **query: Key-value pairs for filtering. \"\"\" for experiment in self . tracker . list_experiments (): if name and not re . match ( name , experiment . name ): continue yield from self . tracker . search_run_ids ( experiment . experiment_id , run_name , parent_run_id , parent_only , nested_only , exclude_parent , best_score_limit , ** query , ) def search_parent_run_ids ( self , name : str = \"\" , ** query ) -> Iterator [ str ]: DOCS \"\"\"Returns an iterator that yields matching parent Run IDs. Args: name: Experiment name pattern for filtering. **query: Key-value pairs for filtering. \"\"\" yield from self . search_run_ids ( name , parent_only = True , ** query ) def search_nested_run_ids ( self , name : str = \"\" , ** query ) -> Iterator [ str ]: DOCS \"\"\"Returns an iterator that yields matching nested Run IDs. Args: name: Experiment name pattern for filtering. **query: Key-value pairs for filtering. \"\"\" yield from self . search_run_ids ( name , nested_only = True , ** query ) def set_terminated ( self , name : str , status : Optional [ str ] = None , ** kwargs ): DOCS \"\"\"Sets runs' status to terminated. Args: status: A string value of [`mlflow.entities.RunStatus`](https://mlflow.org/docs/latest/python_api/mlflow.entities.html#mlflow.entities.RunStatus). Defaults to \u201cFINISHED\u201d. Examples: To terminate a run: client.set_terminated('example', run=5) To kill multiple runs: client.set_terminated('example', 'KILLED', run=[3, 5, 7]) \"\"\" for run_id in self . get_run_ids ( name , ** kwargs ): self . tracker . client . set_terminated ( run_id , status = status ) def set_terminated_all ( self , name : str = \"\" ): DOCS \"\"\"Sets all runs' status to terminated. Args: status: A string value of [`mlflow.entities.RunStatus`](https://mlflow.org/docs/latest/python_api/mlflow.entities.html#mlflow.entities.RunStatus). Defaults to \u201cFINISHED\u201d. Examples: To terminate all of the runs of the `example` experiment: client.set_terminated_all('example') To terminate all of the runs globally: client.set_terminated_all() \"\"\" for run_id in self . search_run_ids ( name ): self . tracker . client . set_terminated ( run_id ) def load_params ( self , run_id : str ) -> Dict [ str , Any ]: DOCS \"\"\"Returns a parameter dictionary loaded from MLFlow Tracking. Args: run_id: Run ID for a run to be loaded. \"\"\" return self . tracker . load_params ( run_id ) def load_run ( self , run_id : str , mode : str = \"test\" ) -> Run : DOCS \"\"\"Returns a `Run` instance created using parameters loaded from MLFlow Tracking. Args: run_id: Run ID for a run to be loaded. mode: Mode name: `'current'`, `'best'`, or `'test'`. Default is `'{default}'`. \"\"\" return self . tracker . load_run ( run_id , mode ) def load_run_by_name ( self , name : str , mode : str = \"test\" , ** kwargs ) -> Run : DOCS \"\"\"Returns a `Run` instance created using parameters loaded from MLFlow Tracking. Args: name: Experiment name pattern for filtering. mode: Mode name: `'current'`, `'best'`, or `'test'`. Examples: To load run#4 of the `example` experiment. client.load_run_by_name('example', run=4) \"\"\" run_id = self . get_run_id ( name , ** kwargs ) return self . load_run ( run_id , mode ) def load_instance ( self , run_id : str , instance_name : str , mode : str = \"test\" ) -> Any : DOCS \"\"\"Returns a member of a `Run` created using parameters loaded from MLFlow Tracking. Args: run_id: Run ID for a run to be loaded. instance_name: Instance name. mode: Mode name: `'current'`, `'best'`, or `'test'`. \"\"\" return self . tracker . load_instance ( run_id , instance_name , mode ) def load_results ( DOCS self , run_ids : Union [ str , Iterable [ str ]], callback = None , reduction : str = \"none\" , verbose : bool = True , ) -> Results : \"\"\"Loads results from multiple runs and concatenates them. Args: run_ids: Multiple run ids to load. callback (callable): Callback function for each run. This function must take a `(index, output, target)` and return a tuple with the same signature. verbose: If `True`, tqdm progress bar is displayed. Returns: A concatenated results instance. \"\"\" if isinstance ( run_ids , str ): return self . load_instance ( run_ids , \"results\" ) run_ids = list ( run_ids ) it = ( self . load_instance ( run_id , \"results\" ) for run_id in run_ids ) if verbose : it = tqdm ( it , total = len ( run_ids ), leave = False ) return ivory . callbacks . results . concatenate ( it , callback = callback , reduction = reduction ) def ui ( self ): tracking_uri = self . tracker . tracking_uri try : subprocess . run ([ \"mlflow\" , \"ui\" , \"--backend-store-uri\" , tracking_uri ]) except KeyboardInterrupt : pass def update_params ( self , name : str = \"\" , ** default ): for experiment in self . tracker . list_experiments (): if name and not re . match ( name , experiment . name ): continue self . tracker . update_params ( experiment . experiment_id , ** default ) def remove_deleted_runs ( self , name : str = \"\" ) -> int : DOCS \"\"\"Removes deleted runs from a local file system. Args: name: Experiment name pattern for filtering. Returns: Number of removed runs. \"\"\" num_runs = 0 for experiment in self . tracker . list_experiments (): if name and not re . match ( name , experiment . name ): continue num_runs += self . tracker . remove_deleted_runs ( experiment . experiment_id ) return num_runs def create_client ( DOCS directory : str = \"\" , name : str = \"client\" , tracker : bool = True , tuner : bool = True ) -> Client : \"\"\"Creates an Ivory Client instance. Args: directory: A working directory. If a YAML file specified by the `name` parameter exists, the file is loaded to configure the client. In addition, this directory is automatically inserted to `sys.path`. name: A YAML config file name. tracker: If True, the client instance has a tracker. tuner: If True, the client instance has a tuner. Returns: An created client. Note: If `tracker` is True (default value), a `mlruns` directory is made under the working directory by MLFlow Tracking. \"\"\" if directory : path = os . path . abspath ( directory ) if path not in sys . path : sys . path . insert ( 0 , path ) source_name = utils . path . normpath ( name , directory ) if os . path . exists ( source_name ): params , _ = utils . path . load_params ( source_name ) else : params = default . get ( \"client\" ) if not tracker and \"tracker\" in params [ \"client\" ]: params [ \"client\" ] . pop ( \"tracker\" ) if not tuner and \"tuner\" in params [ \"client\" ]: params [ \"client\" ] . pop ( \"tuner\" ) with utils . path . chdir ( source_name ): client = instance . create_base_instance ( params , \"client\" , source_name ) return client","title":"ivory.core.client"},{"location":"api/source/ivory.core.data/","text":"SOURCE CODE IVORY.CORE. DATA DOCS \"\"\" Ivory uses four classes for data presentation: `Data`, `Dataset`, `Datasets`, and `DataLoaders`. Basically, you only need to define a class that is a subclass of `Data` and use original `Dataset` and `Datasets`. An example parameter YAML file is: datasets: data: class: your.Data # a subclass of ivory.core.data.Data dataset: fold: 0 But if you need, you can define your `Dataset` and/or `Datasets`. datasets: class: your.Datasets # a subclass of ivory.core.data.Datasets data: class: your.Data # a subclass of ivory.core.data.Data dataset: def: your.Dataset # a subclass of ivory.core.data.Dataset fold: 0 The `DataLoaders` is used internally by `ivory.torch.trainer.Trainer` or `ivory.nnabla.trainer.Trainer` classes to yield a minibatch in training loop. Note: Use a `'def'` key for `dataset` instead of `'class'`. See [Tutorial](/tutorial/data) \"\"\" from dataclasses import dataclass from typing import Callable , Optional , Tuple import numpy as np import ivory.core.collections from ivory.core import instance @dataclass class Data : DOCS \"\"\"Base class to provide data to a `Dataset` instance. To make a subclass, you need to assign the following attributes in the `Data.init()`: * `index`: Index of samples. * `input`: Input data. * `target`: Target data. * `fold`: Fold number. \"\"\" def __post_init__ ( self ): self . fold = None self . index = None self . input = None self . target = None self . init () def __repr__ ( self ): cls_name = self . __class__ . __name__ if self . fold is None : return f \" { cls_name } ()\" else : num_train = self . fold [ self . fold != - 1 ] . shape [ 0 ] num_test = len ( self . fold ) - num_train return f \" { cls_name } (train_size= { num_train } , test_size= { num_test } )\" def init ( self ): DOCS \"\"\"Initializes `index`, `input`, `target`, and `fold` attributes. The fold number of test data must be `-1`. Examples: For regression def init(self): self.index = np.range(100) self.input = np.random.randn(100, 5) self.target = np.random.randn(100) self.fold = np.random.randint(5) self.fold[80:] = -1 For classification def init(self): self.index = np.range(100) self.input = np.random.randn(100, 5) self.target = np.random.randint(100, 10) self.fold = np.random.randint(5) self.fold[80:] = -1 \"\"\" def get_index ( self , mode : str , fold : int ) -> np . ndarray : DOCS \"\"\"Returns index according to the mode and fold. Args: mode: Mode name: `'train'`, `'val'`, or `'test'`. fold: Fold number. \"\"\" index = np . arange ( len ( self . fold )) if mode == \"train\" : return index [( self . fold != fold ) & ( self . fold != - 1 )] elif mode == \"val\" : return index [ self . fold == fold ] else : return index [ self . fold == - 1 ] def get_input ( self , index ): DOCS \"\"\"Returns input data. By default, this function returns `self.input[index]`. You can override this behavior in a subclass. Args: index (int or 1D-array): Index. \"\"\" return self . input [ index ] def get_target ( self , index ): DOCS \"\"\"Returns target data. By default, this function returns `self.target[index]`. You can override this behavior in a subclass. Args: index (int or 1D-array): Index. \"\"\" return self . target [ index ] def get ( self , index ) -> Tuple : DOCS \"\"\"Returns a tuple of (`index`, `input`, `target`) according to the index. Args: index (int or 1D-array): Index. \"\"\" return self . index [ index ], self . get_input ( index ), self . get_target ( index ) @dataclass class Dataset : DOCS \"\"\"Dataset class represents a set of data for a mode and fold. Args: data: `Data` instance that provides data to `Dataset` instance. mode: Mode name: `'train'`, `'val'`, or `'test'`. fold: Fold number. transform (callable, optional): Callable to transform the data. The `transform` must take 2 or 3 arguments: (`mode`, `input`, optional `target`) and return a tuple of (`input`, optional `target`). \"\"\" data : Data mode : str fold : int transform : Optional [ Callable ] = None def __post_init__ ( self ): self . index = self . data . get_index ( self . mode , self . fold ) if self . mode == \"test\" : self . fold = - 1 if self . transform : self . transform = instance . get_attr ( self . transform ) self . init () def init ( self ): DOCS \"\"\"Called at initialization. You can add any process in a subclass.\"\"\" pass def __repr__ ( self ): cls_name = self . __class__ . __name__ return f \" { cls_name } (mode= { self . mode !r} , num_samples= { len ( self ) } )\" def __len__ ( self ): return len ( self . index ) def __getitem__ ( self , index ): if index == slice ( None , None , None ): index , input , * target = self . get () else : index , input , * target = self . get ( index ) if self . transform : input , * target = self . transform ( self . mode , input , * target ) return ( index , input , * target ) def __iter__ ( self ): for index in range ( len ( self )): yield self [ index ] def get ( self , index = None ) -> Tuple : DOCS \"\"\"Returns a tuple of (`index`, `input`, `target`) according to the index. If index is `None`, reutrns all of the data. Args: index (int or 1D-array, optional): Index. \"\"\" if index is None : return self . data . get ( self . index ) else : return self . data . get ( self . index [ index ]) def sample ( self , n : int = 0 , frac : float = 0.0 ) -> Tuple : DOCS \"\"\"Returns a tuple of (`index`, `input`, `target`) randomly sampled. Args: n: Size of sampling. frac: Ratio of sampling. \"\"\" index , input , * target = self [:] if frac : n = int ( len ( index ) * frac ) idx = np . random . permutation ( len ( index ))[: n ] return tuple ([ x [ idx ] for x in [ index , input , * target ]]) @dataclass class Datasets ( ivory . core . collections . Dict ): DOCS \"\"\"Dataset class represents a collection of `Dataset` for a fold. Args: data: `Data` instance that provides data to `Dataset` instance. dataset: Dataset factory. fold: Fold number. Attributes: train (Dataset): Train dataset. val (Dataset): Validation dataset. test (Dataset): Test dataset. fold: Fold number. \"\"\" data : Data dataset : Callable fold : int def __post_init__ ( self ): super () . __init__ () for mode in [ \"train\" , \"val\" , \"test\" ]: self [ mode ] = self . dataset ( self . data , mode , self . fold ) class DataLoaders ( ivory . core . collections . Dict ): DOCS \"\"\"DataLoaders class represents a collection of `DataLoader`. Args: datasets: `Datasets` instance. batch_size: Batch_size shuffle: If True, train dataset is shuffled. Validation and test dataset are not shuffled regardless of this value. Attributes: train (Dataset): Train dataset. val (Dataset): Validation dataset. test (Dataset): Test dataset. \"\"\" def __init__ ( self , datasets : Datasets , batch_size : int , shuffle : bool ): super () . __init__ () for mode in [ \"train\" , \"val\" , \"test\" ]: self [ mode ] = self . get_dataloader ( datasets [ mode ], batch_size , shuffle ) shuffle = False def get_dataloader ( self , dataset , batch_size , shuffle ): raise NotImplementedError","title":"ivory.core.data"},{"location":"api/source/ivory.core/","text":"SOURCE CODE IVORY. CORE DOCS","title":"ivory.core"},{"location":"api/source/ivory.core.run/","text":"SOURCE CODE IVORY.CORE. RUN DOCS \"\"\" This module provides the `Run` class that is one of the main classes of Ivory library. In addition, `Task` and `Study` classes are defined, which manages multiple runs for cross validation, hyperparameter tuning, and so on. To create an `Run` instance: import ivory client = ivory.create_run('example') The argument `example` is an experiment name in which the created run is included. Ivory assumes that `example.yml` or `example.yaml` file exists under the client's working directory. You can comfirm the client's working directory by: os.path.dirname(client.source_name) One you got a `Run` instance. call `Run.start()` to start training. For test, call `Run.start('test')` instead. Also, you can perform traing and test by one step with `Run.start('both')`. \"\"\" import functools import gc import inspect import os import warnings from typing import Any , Dict , Iterable , Iterator , Optional from termcolor import colored import ivory.core.collections import ivory.core.state from ivory import utils from ivory.core.base import CallbackCaller from ivory.utils.tqdm import tqdm class Run ( CallbackCaller ): DOCS \"\"\"Run class is one of the main classes of Ivory library.\"\"\" def set_tracker ( self , tracker , name : str ): DOCS \"\"\"Sets tracker for tracking. By setting a tracker, a `Run` instance can be a run of MLFlow Tracking at the same time. MLFlow Tracking gives the Run ID and name for this instance. Args: tracker (Tracker): Tracker instance. \"\"\" if not self . id : self . name , self . id = tracker . create_run ( self . experiment_id , name , self . source_name ) self . params [ name ][ \"name\" ] = self . name self . params [ name ][ \"id\" ] = self . id self . set ( tracker = tracker ) self . set ( tracking = tracker . create_tracking ()) def init ( self , mode : str = \"train\" ): self . create_callbacks () self . mode = mode self . on_init_begin () self . on_init_end () def start ( self , mode : str = \"train\" ): DOCS \"\"\"Starts traing and/or test. Args: mode: Mode name: `'train'`, `'test'`, or `'both'`. \"\"\" if mode == \"both\" : self . start ( \"train\" ) if self . tracker : self . tracker . load_state_dict ( self , \"best\" ) self . start ( \"test\" ) else : self . init ( mode ) for obj in self . values (): if hasattr ( obj , \"start\" ) and callable ( obj . start ): obj . start ( self ) def state_dict ( self ) -> dict : DOCS \"\"\"Returns a state dictionary for all of member instances.\"\"\" state_dict = {} for name , obj in self . items (): if hasattr ( obj , \"state_dict\" ) and callable ( obj . state_dict ): with warnings . catch_warnings (): # for torch LambdaLR scheduler warnings . simplefilter ( \"ignore\" ) state_dict [ name ] = obj . state_dict () return state_dict def load_state_dict ( self , state_dict : Dict [ str , Any ]): DOCS \"\"\"Loads a state dictionary to all of member instances. Args: state_dict (dict): state dictionary for all of member instances. \"\"\" for name in state_dict : with warnings . catch_warnings (): # for torch LambdaLR scheduler warnings . simplefilter ( \"ignore\" ) self [ name ] . load_state_dict ( state_dict [ name ]) def save ( self , directory : str ): DOCS \"\"\"Saves member instances. Args: directory: Directory where member instances are saved. \"\"\" for name , state_dict in self . state_dict () . items (): path = os . path . join ( directory , name ) if hasattr ( self [ name ], \"save\" ) and callable ( self [ name ] . save ): self [ name ] . save ( state_dict , path ) elif isinstance ( self [ name ], ivory . core . state . State ): ivory . core . state . save ( state_dict , path ) else : self . save_instance ( state_dict , path ) def save_instance ( self , state_dict : Dict [ str , Any ], path : str ): raise NotImplementedError def load ( self , directory : str ) -> Dict [ str , Any ]: DOCS \"\"\"Loads member instances. Args: directory: Directory where member instances have been saved. \"\"\" state_dict = {} for name in os . listdir ( directory ): path = os . path . join ( directory , name ) if hasattr ( self [ name ], \"load\" ) and callable ( self [ name ] . load ): state_dict [ name ] = self [ name ] . load ( path ) elif isinstance ( self [ name ], ivory . core . state . State ): state_dict [ name ] = ivory . core . state . load ( path ) else : instance_state_dict = self . load_instance ( path ) if instance_state_dict : state_dict [ name ] = instance_state_dict return state_dict def load_instance ( self , path ): raise NotImplementedError class Task ( Run ): DOCS \"\"\"Task class creates a parent run that generates multiple runs.\"\"\" def create_run ( self , args , ** kwargs ) -> Run : # type:ignore DOCS \"\"\"Create a nested run according to arguments Args: args (dict, optional): Update dictionary. **kwargs: Additional update dictionary. Returns: Run: Created nested `Run` instance. \"\"\" run = super () . create_run ( args , ** kwargs ) run_name = colored ( f \"[ { run . name } ]\" , \"green\" ) msg = utils . params . to_str ( args ) tqdm . write ( run_name + f \" { msg } \" ) if self . tracking : self . tracking . set_parent_run_id ( run . id , self . id ) return run def terminate ( self ): if self . tracking : self . tracking . client . set_terminated ( self . id ) def product ( DOCS self , params : Optional [ Dict [ str , Iterable [ Any ]]] = None , repeat : int = 1 , ** kwargs , ) -> Iterator [ Run ]: \"\"\"Makes a product iterator. This iterator returns runs from cartesian product of input parameters. Args: params (dict, optional): Parameter range. Key is a parameter name and value is an iterable of parameter's value. repeat: Number of repeatation. **kwargs: Additional parameter range. See Also: [Product section](/tutorial/task#product) in Multiple Runs Tutorial \"\"\" params , base_params = utils . params . split_params ( params , ** kwargs ) if self . tracking : self . tracking . set_tags ( self . id , params ) self . tracking . set_tags ( self . id , base_params ) params_list = list ( utils . params . product ( params )) * repeat if \"verbose\" not in base_params or base_params [ \"verbose\" ]: params_list = tqdm ( params_list , desc = \"Prod \" ) for args_ in params_list : args = base_params . copy () args . update ( args_ ) run = self . create_run ( args ) yield run del run gc . collect () self . terminate () def chain ( DOCS self , params : Optional [ Dict [ str , Iterable [ Any ]]] = None , use_best_param : bool = True , ** kwargs , ) -> Iterator [ Run ]: \"\"\"Makes a chain iterator. This iterator returns runs from the first input paramter until it is exhausted, then proceeds to the next parameter, until all of the parameters are exhausted. Other parameters have default values if they don't be specified by additional key-value pairs. Args: params (dict, optional): Parameter range. Key is a parameter name and value is an iterable of parameter's value. use_best_param: If True (default), the parameter that got the best score is used during the following iterations. **kwargs: Additional parameter range. See Also: [Chain section](/tutorial/task#chain) in Multiple Runs Tutorial \"\"\" params , base_params = utils . params . split_params ( params , ** kwargs ) if self . tracking : self . tracking . set_tags ( self . id , params ) self . tracking . set_tags ( self . id , base_params ) params_list = { arg : list ( value ) for arg , value in params . items ()} total = sum ( len ( value ) for value in params_list . values ()) if \"verbose\" in base_params and base_params [ \"verbose\" ] == 0 : bar = None else : bar = tqdm ( total = total , desc = \"Chain\" ) best_params : Dict [ str , Any ] = {} for arg , values in params_list . items (): best_param = None for value in values : args = base_params . copy () args . update ({ arg : value }) if use_best_param : args . update ( best_params ) run = self . create_run ( args ) yield run if run . monitor : current_score = run . monitor . best_score if best_param is None : best_score = current_score best_param = value elif run . monitor . mode == \"min\" and current_score < best_score : best_score = current_score best_param = value if current_score > best_score : best_score = current_score best_param = value del run gc . collect () if best_param is not None : best_params [ arg ] = best_param if bar is not None : bar . update ( 1 ) self . terminate () class Study ( Task ): DOCS \"\"\"Study class create a parent run to manage hyperparameter tuning.\"\"\" def optimize ( self , suggest_name : str = \"\" , ** kwargs ): DOCS \"\"\"Performs parameter optimizations using Optuna. Args: suggest_name: Name of suggest function. **kwargs: Key-iterable pairs for parametric optimization. See Also: [Hyperparameter Tuning](/tutorial/tuning) in Tutorial \"\"\" if not suggest_name : suggest_name = list ( self . objective . suggests . keys ())[ 0 ] study_name = \".\" . join ([ self . experiment_name , suggest_name , self . name ]) mode = self . create_instance ( \"monitor\" ) . mode study = self . tuner . create_study ( study_name , mode ) if self . tracking : self . tracking . set_tags ( self . id , { \"study_name\" : study_name }) study . set_user_attr ( \"run_id\" , self . id ) has_pruning = self . tuner . pruner is not None optimize_args = inspect . signature ( study . optimize ) . parameters . keys () params = {} for key in list ( kwargs . keys ()): if key not in optimize_args : params [ key ] = kwargs . pop ( key ) create_run = functools . partial ( self . create_run , ** params ) objective = self . objective ( suggest_name , create_run , has_pruning ) study . optimize ( objective , ** kwargs ) self . terminate () return study","title":"ivory.core.run"},{"location":"api/source/ivory.nnabla/","text":"SOURCE CODE IVORY. NNABLA DOCS","title":"ivory.nnabla"},{"location":"api/source/ivory.nnabla.trainer/","text":"SOURCE CODE IVORY.NNABLA. TRAINER DOCS \"\"\"The `ivory.nnabla.trainer` module provides the `Trainer` class for nnable.\"\"\" from dataclasses import dataclass from typing import Callable , Optional import nnabla as nn from nnabla.ext_utils import get_extension_context import ivory.core.trainer import ivory.nnabla.data import ivory.nnabla.functions from ivory.core import instance @dataclass class Trainer ( ivory . core . trainer . Trainer ): DOCS loss : Optional [ Callable ] = None dataloaders : str = \"ivory.nnabla.data.DataLoaders\" gpu : bool = False precision : int = 32 # Full precision (32), half precision (16). amp_level : str = \"O1\" def __post_init__ ( self ): if isinstance ( self . loss , str ) and \".\" not in self . loss : self . loss = getattr ( ivory . nnabla . functions , self . loss ) else : self . loss = instance . get_attr ( self . loss ) def on_init_begin ( self , run ): super () . on_init_begin ( run ) if self . gpu : context = \"cudnn\" else : context = \"cpu\" if self . precision == 32 : type_config = \"float\" elif self . precision == 16 : type_config = \"half\" else : raise ValueError ( f \"Unknown precision: { self . precision } \" ) context = get_extension_context ( context , type_config = type_config ) nn . set_default_context ( context ) if not run . model . parameters (): run . model . build ( self . loss , run . datasets . train , self . batch_size ) run . optimizer . set_parameters ( run . model . parameters ()) def on_train_begin ( self , run ): run . model . train () def train_step ( self , run , index , input , target ): DOCS optimizer = run . optimizer optimizer . zero_grad () output , loss = run . model ( input , target ) run . results . step ( index , output , target ) run . metrics . step ( loss ) run . model . backward () optimizer . update () def on_val_begin ( self , run ): run . model . eval () def val_step ( self , run , index , input , target ): DOCS output , loss = run . model ( input , target ) run . results . step ( index , output , target ) run . metrics . step ( loss ) def on_epoch_end ( self , run ): pass def on_test_begin ( self , run ): run . model . eval () def test_step ( self , run , index , input , target ): DOCS output = run . model ( input ) run . results . step ( index , output , target )","title":"ivory.nnabla.trainer"},{"location":"tutorial/callback/","text":"Callback System Basics Ivory implements a simple but powerful callback system. Here is the list of callback functions in the order of invocation: import ivory.core.base ivory.core.base.Callback.METHODS [2] 2020-06-20 15:23:39 ( 3.00ms ) python3 ( 6.37s ) ['on_init_begin', 'on_init_end', 'on_fit_begin', 'on_epoch_begin', 'on_train_begin', 'on_train_end', 'on_val_begin', 'on_val_end', 'on_epoch_end', 'on_fit_end', 'on_test_begin', 'on_test_end'] Any class that defines these functions can be a callback. class SimpleCallback: # No base class is needed. # You don't have to define all of the callback functions def on_fit_begin(self, run): # Must have an only `run` argument. print(f'on_fit_begin is called from id={id(run)}') # Do something with `run`. [3] 2020-06-20 15:23:39 ( 3.00ms ) python3 ( 6.38s ) To invoke callback functions, create a CallbackCaller instance. caller = ivory.core.base.CallbackCaller(simple=SimpleCallback()) caller [4] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.38s ) CallbackCaller(num_instances=1) The number of registered instances is 1. list(caller) [5] 2020-06-20 15:23:39 ( 3.00ms ) python3 ( 6.38s ) ['simple'] Then call CallbackCaller.create_callbacks() to build a callback network. caller.create_callbacks() caller [6] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.39s ) CallbackCaller(num_instances=13) The number of instances increased up to 13. list(caller) [7] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.39s ) ['simple', 'on_init_begin', 'on_init_end', 'on_fit_begin', 'on_epoch_begin', 'on_train_begin', 'on_train_end', 'on_val_begin', 'on_val_end', 'on_epoch_end', 'on_fit_end', 'on_test_begin', 'on_test_end'] Callback functions are added to the caller instance. Let's inspect some callback functions. caller.on_init_begin [8] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.40s ) Callback([]) This is an empty callback because the caller has no instances that define the on_init_begin() . On the other hand, caller.on_fit_begin [9] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.40s ) Callback(['simple']) The simple instance is registered as a receiver of the on_fit_begin() . We can call this. caller.on_fit_begin() [10] 2020-06-20 15:23:39 ( 5.00ms ) python3 ( 6.40s ) on_fit_begin is called from id=1375756371208 id(caller) [11] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.41s ) 1375756371208 This caller-receiver network among arbitrary instance collection builds a complex machine learning workflow. Run class is a subclass of the CallbackCaller and performs more library-specific process. We uses Run below. Example Callback: Results To work with the Results callback, we create a set of data and a model. For more details about the following code, see Creating Instance section. import yaml from ivory.core.instance import create_instance # A helper function. def create(doc, name, **kwargs): params = yaml.safe_load(doc) return create_instance(params, name, **kwargs) doc = \"\"\" library: torch datasets: data: class: rectangle.data.Data n_splits: 5 dataset: fold: 0 model: class: rectangle.torch.Model hidden_sizes: [3, 4, 5] \"\"\" datasets = create(doc, 'datasets') model = create(doc, 'model') [12] 2020-06-20 15:23:39 ( 8.00ms ) python3 ( 6.42s ) The Results callback stores index, output, and target data. To save memory, a Results instance ignores input data. # import ivory.callbacks.results # For Scikit-learn or TensorFlow. import ivory.torch.results results = ivory.torch.results.Results() results [13] 2020-06-20 15:23:39 ( 5.00ms ) python3 ( 6.42s ) Results([]) import ivory.core.run run = ivory.core.run.Run( datasets=datasets, model=model, results=results ) run.create_callbacks() run [14] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.43s ) Run(num_instances=15) # A helper function def print_callbacks(obj): for func in ivory.core.base.Callback.METHODS: if hasattr(obj, func) and callable(getattr(obj, func)): print(func) print_callbacks(results) [15] 2020-06-20 15:23:39 ( 8.00ms ) python3 ( 6.43s ) on_train_begin on_train_end on_val_end on_test_begin on_test_end Let's play with the Results callback. Results.step() records the current index, output, and target. import torch # For simplicity, just one epoch with some batches. run.on_train_begin() dataset = run.datasets.train for k in range(3): index, input, target = dataset[4 * k : 4 * (k + 1)] input, target = torch.tensor(input), torch.tensor(target) output = run.model(input) run.results.step(index, output, target) # Do something for example parameter update or early stopping. run.on_train_end() run.on_val_begin() # Can call even if there is no callback. dataset = run.datasets.val for k in range(2): index, input, target = dataset[4 * k : 4 * (k + 1)] input, target = torch.tensor(input), torch.tensor(target) output = run.model(input) run.results.step(index, output, target) run.on_val_end() run.on_epoch_end() results [16] 2020-06-20 15:23:39 ( 10.0ms ) python3 ( 6.44s ) Results(['train', 'val']) We performed a train and validation loop so that the Results instance has these data, but doesn't have test data. results.train [17] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.45s ) Dict(['index', 'output', 'target']) results.train.index # The length is 4 x 3. [18] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.45s ) array([ 0, 2, 3, 4, 5, 6, 7, 10, 12, 13, 15, 16]) results.val.index # The length is 4 x 2. [19] 2020-06-20 15:23:39 ( 3.00ms ) python3 ( 6.45s ) array([ 1, 8, 14, 27, 30, 31, 34, 45]) results.val.output [20] 2020-06-20 15:23:39 ( 5.00ms ) python3 ( 6.46s ) array([[-0.19664028], [-0.18804704], [-0.18011682], [-0.1660739 ], [-0.19168824], [-0.1673036 ], [-0.20202819], [-0.18093993]], dtype=float32) results.val.target [21] 2020-06-20 15:23:39 ( 5.00ms ) python3 ( 6.46s ) array([[ 9.582911 ], [ 8.156037 ], [ 5.7045836], [ 3.401937 ], [ 7.614189 ], [ 3.2392535], [15.3450165], [ 4.47484 ]], dtype=float32) Other Callback There are several callback such as Metrics , Monitor , etc. We will learn about them in next Training a Model tutorial.","title":"Callback System"},{"location":"tutorial/callback/#callback-system","text":"","title":"Callback System"},{"location":"tutorial/callback/#basics","text":"Ivory implements a simple but powerful callback system. Here is the list of callback functions in the order of invocation: import ivory.core.base ivory.core.base.Callback.METHODS [2] 2020-06-20 15:23:39 ( 3.00ms ) python3 ( 6.37s ) ['on_init_begin', 'on_init_end', 'on_fit_begin', 'on_epoch_begin', 'on_train_begin', 'on_train_end', 'on_val_begin', 'on_val_end', 'on_epoch_end', 'on_fit_end', 'on_test_begin', 'on_test_end'] Any class that defines these functions can be a callback. class SimpleCallback: # No base class is needed. # You don't have to define all of the callback functions def on_fit_begin(self, run): # Must have an only `run` argument. print(f'on_fit_begin is called from id={id(run)}') # Do something with `run`. [3] 2020-06-20 15:23:39 ( 3.00ms ) python3 ( 6.38s ) To invoke callback functions, create a CallbackCaller instance. caller = ivory.core.base.CallbackCaller(simple=SimpleCallback()) caller [4] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.38s ) CallbackCaller(num_instances=1) The number of registered instances is 1. list(caller) [5] 2020-06-20 15:23:39 ( 3.00ms ) python3 ( 6.38s ) ['simple'] Then call CallbackCaller.create_callbacks() to build a callback network. caller.create_callbacks() caller [6] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.39s ) CallbackCaller(num_instances=13) The number of instances increased up to 13. list(caller) [7] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.39s ) ['simple', 'on_init_begin', 'on_init_end', 'on_fit_begin', 'on_epoch_begin', 'on_train_begin', 'on_train_end', 'on_val_begin', 'on_val_end', 'on_epoch_end', 'on_fit_end', 'on_test_begin', 'on_test_end'] Callback functions are added to the caller instance. Let's inspect some callback functions. caller.on_init_begin [8] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.40s ) Callback([]) This is an empty callback because the caller has no instances that define the on_init_begin() . On the other hand, caller.on_fit_begin [9] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.40s ) Callback(['simple']) The simple instance is registered as a receiver of the on_fit_begin() . We can call this. caller.on_fit_begin() [10] 2020-06-20 15:23:39 ( 5.00ms ) python3 ( 6.40s ) on_fit_begin is called from id=1375756371208 id(caller) [11] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.41s ) 1375756371208 This caller-receiver network among arbitrary instance collection builds a complex machine learning workflow. Run class is a subclass of the CallbackCaller and performs more library-specific process. We uses Run below.","title":"Basics"},{"location":"tutorial/callback/#example-callback-results","text":"To work with the Results callback, we create a set of data and a model. For more details about the following code, see Creating Instance section. import yaml from ivory.core.instance import create_instance # A helper function. def create(doc, name, **kwargs): params = yaml.safe_load(doc) return create_instance(params, name, **kwargs) doc = \"\"\" library: torch datasets: data: class: rectangle.data.Data n_splits: 5 dataset: fold: 0 model: class: rectangle.torch.Model hidden_sizes: [3, 4, 5] \"\"\" datasets = create(doc, 'datasets') model = create(doc, 'model') [12] 2020-06-20 15:23:39 ( 8.00ms ) python3 ( 6.42s ) The Results callback stores index, output, and target data. To save memory, a Results instance ignores input data. # import ivory.callbacks.results # For Scikit-learn or TensorFlow. import ivory.torch.results results = ivory.torch.results.Results() results [13] 2020-06-20 15:23:39 ( 5.00ms ) python3 ( 6.42s ) Results([]) import ivory.core.run run = ivory.core.run.Run( datasets=datasets, model=model, results=results ) run.create_callbacks() run [14] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.43s ) Run(num_instances=15) # A helper function def print_callbacks(obj): for func in ivory.core.base.Callback.METHODS: if hasattr(obj, func) and callable(getattr(obj, func)): print(func) print_callbacks(results) [15] 2020-06-20 15:23:39 ( 8.00ms ) python3 ( 6.43s ) on_train_begin on_train_end on_val_end on_test_begin on_test_end Let's play with the Results callback. Results.step() records the current index, output, and target. import torch # For simplicity, just one epoch with some batches. run.on_train_begin() dataset = run.datasets.train for k in range(3): index, input, target = dataset[4 * k : 4 * (k + 1)] input, target = torch.tensor(input), torch.tensor(target) output = run.model(input) run.results.step(index, output, target) # Do something for example parameter update or early stopping. run.on_train_end() run.on_val_begin() # Can call even if there is no callback. dataset = run.datasets.val for k in range(2): index, input, target = dataset[4 * k : 4 * (k + 1)] input, target = torch.tensor(input), torch.tensor(target) output = run.model(input) run.results.step(index, output, target) run.on_val_end() run.on_epoch_end() results [16] 2020-06-20 15:23:39 ( 10.0ms ) python3 ( 6.44s ) Results(['train', 'val']) We performed a train and validation loop so that the Results instance has these data, but doesn't have test data. results.train [17] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.45s ) Dict(['index', 'output', 'target']) results.train.index # The length is 4 x 3. [18] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.45s ) array([ 0, 2, 3, 4, 5, 6, 7, 10, 12, 13, 15, 16]) results.val.index # The length is 4 x 2. [19] 2020-06-20 15:23:39 ( 3.00ms ) python3 ( 6.45s ) array([ 1, 8, 14, 27, 30, 31, 34, 45]) results.val.output [20] 2020-06-20 15:23:39 ( 5.00ms ) python3 ( 6.46s ) array([[-0.19664028], [-0.18804704], [-0.18011682], [-0.1660739 ], [-0.19168824], [-0.1673036 ], [-0.20202819], [-0.18093993]], dtype=float32) results.val.target [21] 2020-06-20 15:23:39 ( 5.00ms ) python3 ( 6.46s ) array([[ 9.582911 ], [ 8.156037 ], [ 5.7045836], [ 3.401937 ], [ 7.614189 ], [ 3.2392535], [15.3450165], [ 4.47484 ]], dtype=float32)","title":"Example Callback: Results"},{"location":"tutorial/callback/#other-callback","text":"There are several callback such as Metrics , Monitor , etc. We will learn about them in next Training a Model tutorial.","title":"Other Callback"},{"location":"tutorial/cli/","text":"Command Line Interface If you define data and model, and prepare a YAML parameter file, you don't need to write another Python script code to invoke runs. Ivory's command line interface can do it. For cross validation: $ ivory run torch fold=0-4 For grid search: $ ivory run torch dropout=0-0.5:5 hidden_sizes.0=10-20-2 For optimization using a suggest function: $ ivory optimize torch lr For parametric optimization: $ ivory optimize torch lr=1e-5_1e-3.log Right-hand side string for each parameter creates a Range instance to determine the range of parameters.","title":"Command Line Interface"},{"location":"tutorial/cli/#command-line-interface","text":"If you define data and model, and prepare a YAML parameter file, you don't need to write another Python script code to invoke runs. Ivory's command line interface can do it. For cross validation: $ ivory run torch fold=0-4 For grid search: $ ivory run torch dropout=0-0.5:5 hidden_sizes.0=10-20-2 For optimization using a suggest function: $ ivory optimize torch lr For parametric optimization: $ ivory optimize torch lr=1e-5_1e-3.log Right-hand side string for each parameter creates a Range instance to determine the range of parameters.","title":"Command Line Interface"},{"location":"tutorial/core/","text":"Ivory Core Entities Client Ivory has the Client class that manages the workflow of machine learning. In this tutorial, we are working with data and model to predict rectangle area. The source module exists under the examples directory. First, create a Client instance. import ivory client = ivory.create_client(\"examples\") # Set the working directory client [3] 2020-06-20 15:23:39 ( 6.00ms ) python3 ( 6.53s ) Client(num_instances=2) list(client) [4] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.53s ) ['tracker', 'tuner'] The first instance is a Tracker instance that connects Ivory to MLFlow Tracking . The second instance is named tuner . A Tuner instance connects Ivory to Optuna . Show files in the working directory examples . import os os.listdir('examples') [5] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.53s ) ['base.yml', 'client.yml', 'data.yml', 'data2.yml', 'lgb.yml', 'mlruns', 'nnabla.yml', 'rectangle', 'rfr.yml', 'ridge.yml', 'study.yml', 'tensorflow.yml', 'torch.yml', 'torch2.yml'] rectangle is a Python package that contains our examples. YAML files with extension of .yml or possibly .yaml are parameter files to define a machine learning workflow. Basically, one YAML file is corresponding to one Experiment as discussed later, except the client.yml file. A YAML file name without the extension becomes an experiment name. mlruns is a directory automatically created by MLFlow Tracking in which our trained model and callbacks instances are saved. The client.yml is a configuration file for a Client instance. In our case, the file just contains the minimal settings. File 7 client.yml client: tracker: tuner: Note If you don't need any customization, the YAML file for client is not required. If there is no file for client, Ivory creates a default client with a tracker and tuner. (So, the above file is unnecessary.) If you don't need a tracker and/or tuner, for example in debugging, use ivory.create_client(tracker=False, tuner=False) . Experiment Client.create_experiment() creates an Experiment instance. If the Client instance has a tracker , an experiment of MLFlow Tracking is also created at the same time if it hasn't existed yet. By clicking an icon ( ) in the below cell, you can see the log. experiment = client.create_experiment('torch') # Read torch.yml as params. experiment [6] 2020-06-20 15:23:39 ( 15.0ms ) python3 ( 6.55s ) [I 200620 15:23:39 tracker:48] A new experiment created with name: 'torch' Experiment(id='1', name='torch', num_instances=1) The ID for this experiment was given by MLFlow Tracking. The Client.create_experiment() loads a YAML file corresponding to the first argument from the working directory. File 8 torch.yml library: torch datasets: data: class: rectangle.data.Data n_splits: 4 dataset: fold: 0 model: class: rectangle.torch.Model hidden_sizes: [20, 30] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 results: metrics: monitor: metric: val_loss early_stopping: patience: 10 trainer: loss: mse batch_size: 10 epochs: 10 shuffle: true verbose: 2 After loading, the Experiment instance setups the parameters for creating runs later. The parameters are stored in the params attribute. experiment.params [7] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.55s ) {'run': {'datasets': {'data': {'class': 'rectangle.data.Data', 'n_splits': 4}, 'dataset': {'def': 'ivory.torch.data.Dataset'}, 'fold': 0, 'class': 'ivory.core.data.Datasets'}, 'model': {'class': 'rectangle.torch.Model', 'hidden_sizes': [20, 30]}, 'optimizer': {'class': 'torch.optim.SGD', 'params': '$.model.parameters()', 'lr': 0.001}, 'scheduler': {'class': 'torch.optim.lr_scheduler.ReduceLROnPlateau', 'optimizer': '$', 'factor': 0.5, 'patience': 4}, 'results': {'class': 'ivory.torch.results.Results'}, 'metrics': {'class': 'ivory.torch.metrics.Metrics'}, 'monitor': {'metric': 'val_loss', 'class': 'ivory.callbacks.monitor.Monitor'}, 'early_stopping': {'patience': 10, 'class': 'ivory.callbacks.early_stopping.EarlyStopping'}, 'trainer': {'loss': 'mse', 'batch_size': 10, 'epochs': 10, 'shuffle': True, 'verbose': 2, 'class': 'ivory.torch.trainer.Trainer'}, 'class': 'ivory.torch.run.Run'}, 'experiment': {'name': 'torch', 'class': 'ivory.core.base.Experiment', 'id': '1'}} This is similar to the YAML file we read before, but has been slightly changed. Run and experiment keys are inserted. Run name is assigned by Ivory Client. Experiment ID and Run ID are assigned by MLFlow Tracking. Default classes are specified, for example the ivory.torch.trainer.Trainer class for a trainer instance. Run After setting up an Experiment instance, you can create runs with various parameters. Ivory provides several way to configure them as below. Default parameters Calling without arguments creates a run with default parameters. run = experiment.create_run() run [8] 2020-06-20 15:23:39 ( 34.0ms ) python3 ( 6.59s ) Run(id='a69b41e9dbf344d692ce184f069fc514', name='run#0', num_instances=12) Here, the ID for this run is assigned by MLFlow Tracking. On the other hand, the name is assigned by Ivory as the form of \" (run class name in lower case)#(run number) \". Simple literal (int, float, str) Passing key-value pairs, you can change the parameters. run = experiment.create_run(fold=1) run.datasets.fold [9] 2020-06-20 15:23:39 ( 37.0ms ) python3 ( 6.62s ) 1 But the type of parameter must be equal, otherwise a ValueError is raised. run = experiment.create_run(fold=0.5) run.datasets.fold [10] 2020-06-20 15:23:39 ( 137ms ) python3 ( 6.76s ) ValueError: different type: <class 'int'> != <class 'float'> ValueError Traceback (most recent call last) <ipython-input-100-db3b6dd1af57> in <module> ----> 1 run = experiment.create_run(fold=0.5) 2 run.datasets.fold ~\\Documents\\github\\ivory\\ivory\\core\\base.py in create_run(self, args, name, **kwargs) 104 [`create_params()`](#ivory.core.base.Creator.create_params) function. 105 \"\"\" --> 106 params, args = self.create_params(args, name, **kwargs) 107 run = instance.create_base_instance(params, name, self.source_name) 108 if self.tracker: ~\\Documents\\github\\ivory\\ivory\\core\\base.py in create_params(self, args, name, **kwargs) 88 params.update(default.get(name)) 89 update, args = utils.params.create_update(params[name], args, **kwargs) ---> 90 utils.params.update_dict(params[name], update) 91 return params, args 92 ~\\Documents\\github\\ivory\\ivory\\utils\\params.py in update_dict(org, update) 28 x[k] = value 29 elif type(x[k]) is not type(value) and x[k] is not None: ---> 30 raise ValueError(f\"different type: {type(x[k])} != {type(value)}\") 31 else: 32 if isinstance(x[k], dict): List A list parameter can be overwritten by passing a new list. Off course you can change the length of the list. The original hidden_sizes was [10, 20] . Modify it. run = experiment.create_run(hidden_sizes=[2, 3, 4]) run.model [11] 2020-06-20 15:23:39 ( 139ms ) python3 ( 6.90s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=3, bias=True) (2): Linear(in_features=3, out_features=4, bias=True) (3): Linear(in_features=4, out_features=1, bias=True) ) ) As an alternative way, you can use 0-indexed colon-notation like below. In this case, pass a dictionary to the first argument, because a colon ( : ) can't be in keyword arguments. params = { \"hidden_sizes:0\": 10, # Order is important. \"hidden_sizes:1\": 20, # Start from 0. \"hidden_sizes:2\": 30, # No skip. No reverse. } run = experiment.create_run(params) run.model [12] 2020-06-20 15:23:39 ( 46.9ms ) python3 ( 6.95s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=10, bias=True) (1): Linear(in_features=10, out_features=20, bias=True) (2): Linear(in_features=20, out_features=30, bias=True) (3): Linear(in_features=30, out_features=1, bias=True) ) ) Do you feel this function is unnecessary? This function is prepared for hyperparameter tuning . In some case, you may want to change elements of list. Use 0-indexed dot-notation . params = {\"hidden_sizes.1\": 5} run = experiment.create_run(params) run.model [13] 2020-06-20 15:23:39 ( 48.5ms ) python3 ( 6.99s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=20, bias=True) (1): Linear(in_features=20, out_features=5, bias=True) (2): Linear(in_features=5, out_features=1, bias=True) ) ) Duplicated parameter name Duplicated parameters with the same name are updated together. run = experiment.create_run(patience=5) run.scheduler.patience, run.early_stopping.patience [14] 2020-06-20 15:23:40 ( 46.0ms ) python3 ( 7.04s ) (5, 5) This behavior is natural to update the parameters with the same meaning. But in the above example, the patience of early stopping becomes equal to that of scheduler, so the scheduler doesn't work at all. Scoping by dots To specify an individual parameter even if there are other parameters with the same name, use scoping by dots, or parameter fullname . params = {'scheduler.patience': 8, 'early_stopping.patience': 20} run = experiment.create_run(params) run.scheduler.patience, run.early_stopping.patience [15] 2020-06-20 15:23:40 ( 49.0ms ) python3 ( 7.09s ) (8, 20) Object type Parameters are not limited to a literal such as int , float , or str . For example, run = experiment.create_run() run.optimizer [16] 2020-06-20 15:23:40 ( 49.0ms ) python3 ( 7.14s ) SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) run = experiment.create_run({'optimizer.class': 'torch.optim.Adam'}) run.optimizer [17] 2020-06-20 15:23:40 ( 52.0ms ) python3 ( 7.19s ) Adam ( Parameter Group 0 amsgrad: False betas: (0.9, 0.999) eps: 1e-08 lr: 0.001 weight_decay: 0 ) This means that you can compare optimizer algorithms easily through multiple runs with minimal effort. Creating a run from a client In the above examples, we created runs using the experiment.create_run() . In addition, you can do the same thing by client.create_run() with an experiment name as the first argument. The following code blocks are equivalent. Code 1 experiment = client.create_experiment('torch') run = experiment.create_run(fold=3) Code 2 run = client.create_run('torch', fold=3)","title":"Ivory Core Entities"},{"location":"tutorial/core/#ivory-core-entities","text":"","title":"Ivory Core Entities"},{"location":"tutorial/core/#client","text":"Ivory has the Client class that manages the workflow of machine learning. In this tutorial, we are working with data and model to predict rectangle area. The source module exists under the examples directory. First, create a Client instance. import ivory client = ivory.create_client(\"examples\") # Set the working directory client [3] 2020-06-20 15:23:39 ( 6.00ms ) python3 ( 6.53s ) Client(num_instances=2) list(client) [4] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.53s ) ['tracker', 'tuner'] The first instance is a Tracker instance that connects Ivory to MLFlow Tracking . The second instance is named tuner . A Tuner instance connects Ivory to Optuna . Show files in the working directory examples . import os os.listdir('examples') [5] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.53s ) ['base.yml', 'client.yml', 'data.yml', 'data2.yml', 'lgb.yml', 'mlruns', 'nnabla.yml', 'rectangle', 'rfr.yml', 'ridge.yml', 'study.yml', 'tensorflow.yml', 'torch.yml', 'torch2.yml'] rectangle is a Python package that contains our examples. YAML files with extension of .yml or possibly .yaml are parameter files to define a machine learning workflow. Basically, one YAML file is corresponding to one Experiment as discussed later, except the client.yml file. A YAML file name without the extension becomes an experiment name. mlruns is a directory automatically created by MLFlow Tracking in which our trained model and callbacks instances are saved. The client.yml is a configuration file for a Client instance. In our case, the file just contains the minimal settings. File 7 client.yml client: tracker: tuner: Note If you don't need any customization, the YAML file for client is not required. If there is no file for client, Ivory creates a default client with a tracker and tuner. (So, the above file is unnecessary.) If you don't need a tracker and/or tuner, for example in debugging, use ivory.create_client(tracker=False, tuner=False) .","title":"Client"},{"location":"tutorial/core/#experiment","text":"Client.create_experiment() creates an Experiment instance. If the Client instance has a tracker , an experiment of MLFlow Tracking is also created at the same time if it hasn't existed yet. By clicking an icon ( ) in the below cell, you can see the log. experiment = client.create_experiment('torch') # Read torch.yml as params. experiment [6] 2020-06-20 15:23:39 ( 15.0ms ) python3 ( 6.55s ) [I 200620 15:23:39 tracker:48] A new experiment created with name: 'torch' Experiment(id='1', name='torch', num_instances=1) The ID for this experiment was given by MLFlow Tracking. The Client.create_experiment() loads a YAML file corresponding to the first argument from the working directory. File 8 torch.yml library: torch datasets: data: class: rectangle.data.Data n_splits: 4 dataset: fold: 0 model: class: rectangle.torch.Model hidden_sizes: [20, 30] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 results: metrics: monitor: metric: val_loss early_stopping: patience: 10 trainer: loss: mse batch_size: 10 epochs: 10 shuffle: true verbose: 2 After loading, the Experiment instance setups the parameters for creating runs later. The parameters are stored in the params attribute. experiment.params [7] 2020-06-20 15:23:39 ( 4.00ms ) python3 ( 6.55s ) {'run': {'datasets': {'data': {'class': 'rectangle.data.Data', 'n_splits': 4}, 'dataset': {'def': 'ivory.torch.data.Dataset'}, 'fold': 0, 'class': 'ivory.core.data.Datasets'}, 'model': {'class': 'rectangle.torch.Model', 'hidden_sizes': [20, 30]}, 'optimizer': {'class': 'torch.optim.SGD', 'params': '$.model.parameters()', 'lr': 0.001}, 'scheduler': {'class': 'torch.optim.lr_scheduler.ReduceLROnPlateau', 'optimizer': '$', 'factor': 0.5, 'patience': 4}, 'results': {'class': 'ivory.torch.results.Results'}, 'metrics': {'class': 'ivory.torch.metrics.Metrics'}, 'monitor': {'metric': 'val_loss', 'class': 'ivory.callbacks.monitor.Monitor'}, 'early_stopping': {'patience': 10, 'class': 'ivory.callbacks.early_stopping.EarlyStopping'}, 'trainer': {'loss': 'mse', 'batch_size': 10, 'epochs': 10, 'shuffle': True, 'verbose': 2, 'class': 'ivory.torch.trainer.Trainer'}, 'class': 'ivory.torch.run.Run'}, 'experiment': {'name': 'torch', 'class': 'ivory.core.base.Experiment', 'id': '1'}} This is similar to the YAML file we read before, but has been slightly changed. Run and experiment keys are inserted. Run name is assigned by Ivory Client. Experiment ID and Run ID are assigned by MLFlow Tracking. Default classes are specified, for example the ivory.torch.trainer.Trainer class for a trainer instance.","title":"Experiment"},{"location":"tutorial/core/#run","text":"After setting up an Experiment instance, you can create runs with various parameters. Ivory provides several way to configure them as below.","title":"Run"},{"location":"tutorial/core/#default-parameters","text":"Calling without arguments creates a run with default parameters. run = experiment.create_run() run [8] 2020-06-20 15:23:39 ( 34.0ms ) python3 ( 6.59s ) Run(id='a69b41e9dbf344d692ce184f069fc514', name='run#0', num_instances=12) Here, the ID for this run is assigned by MLFlow Tracking. On the other hand, the name is assigned by Ivory as the form of \" (run class name in lower case)#(run number) \".","title":"Default parameters"},{"location":"tutorial/core/#simple-literal-int-float-str","text":"Passing key-value pairs, you can change the parameters. run = experiment.create_run(fold=1) run.datasets.fold [9] 2020-06-20 15:23:39 ( 37.0ms ) python3 ( 6.62s ) 1 But the type of parameter must be equal, otherwise a ValueError is raised. run = experiment.create_run(fold=0.5) run.datasets.fold [10] 2020-06-20 15:23:39 ( 137ms ) python3 ( 6.76s ) ValueError: different type: <class 'int'> != <class 'float'> ValueError Traceback (most recent call last) <ipython-input-100-db3b6dd1af57> in <module> ----> 1 run = experiment.create_run(fold=0.5) 2 run.datasets.fold ~\\Documents\\github\\ivory\\ivory\\core\\base.py in create_run(self, args, name, **kwargs) 104 [`create_params()`](#ivory.core.base.Creator.create_params) function. 105 \"\"\" --> 106 params, args = self.create_params(args, name, **kwargs) 107 run = instance.create_base_instance(params, name, self.source_name) 108 if self.tracker: ~\\Documents\\github\\ivory\\ivory\\core\\base.py in create_params(self, args, name, **kwargs) 88 params.update(default.get(name)) 89 update, args = utils.params.create_update(params[name], args, **kwargs) ---> 90 utils.params.update_dict(params[name], update) 91 return params, args 92 ~\\Documents\\github\\ivory\\ivory\\utils\\params.py in update_dict(org, update) 28 x[k] = value 29 elif type(x[k]) is not type(value) and x[k] is not None: ---> 30 raise ValueError(f\"different type: {type(x[k])} != {type(value)}\") 31 else: 32 if isinstance(x[k], dict):","title":"Simple literal (int, float, str)"},{"location":"tutorial/core/#list","text":"A list parameter can be overwritten by passing a new list. Off course you can change the length of the list. The original hidden_sizes was [10, 20] . Modify it. run = experiment.create_run(hidden_sizes=[2, 3, 4]) run.model [11] 2020-06-20 15:23:39 ( 139ms ) python3 ( 6.90s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=3, bias=True) (2): Linear(in_features=3, out_features=4, bias=True) (3): Linear(in_features=4, out_features=1, bias=True) ) ) As an alternative way, you can use 0-indexed colon-notation like below. In this case, pass a dictionary to the first argument, because a colon ( : ) can't be in keyword arguments. params = { \"hidden_sizes:0\": 10, # Order is important. \"hidden_sizes:1\": 20, # Start from 0. \"hidden_sizes:2\": 30, # No skip. No reverse. } run = experiment.create_run(params) run.model [12] 2020-06-20 15:23:39 ( 46.9ms ) python3 ( 6.95s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=10, bias=True) (1): Linear(in_features=10, out_features=20, bias=True) (2): Linear(in_features=20, out_features=30, bias=True) (3): Linear(in_features=30, out_features=1, bias=True) ) ) Do you feel this function is unnecessary? This function is prepared for hyperparameter tuning . In some case, you may want to change elements of list. Use 0-indexed dot-notation . params = {\"hidden_sizes.1\": 5} run = experiment.create_run(params) run.model [13] 2020-06-20 15:23:39 ( 48.5ms ) python3 ( 6.99s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=20, bias=True) (1): Linear(in_features=20, out_features=5, bias=True) (2): Linear(in_features=5, out_features=1, bias=True) ) )","title":"List"},{"location":"tutorial/core/#duplicated-parameter-name","text":"Duplicated parameters with the same name are updated together. run = experiment.create_run(patience=5) run.scheduler.patience, run.early_stopping.patience [14] 2020-06-20 15:23:40 ( 46.0ms ) python3 ( 7.04s ) (5, 5) This behavior is natural to update the parameters with the same meaning. But in the above example, the patience of early stopping becomes equal to that of scheduler, so the scheduler doesn't work at all.","title":"Duplicated parameter name"},{"location":"tutorial/core/#scoping-by-dots","text":"To specify an individual parameter even if there are other parameters with the same name, use scoping by dots, or parameter fullname . params = {'scheduler.patience': 8, 'early_stopping.patience': 20} run = experiment.create_run(params) run.scheduler.patience, run.early_stopping.patience [15] 2020-06-20 15:23:40 ( 49.0ms ) python3 ( 7.09s ) (8, 20)","title":"Scoping by dots"},{"location":"tutorial/core/#object-type","text":"Parameters are not limited to a literal such as int , float , or str . For example, run = experiment.create_run() run.optimizer [16] 2020-06-20 15:23:40 ( 49.0ms ) python3 ( 7.14s ) SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) run = experiment.create_run({'optimizer.class': 'torch.optim.Adam'}) run.optimizer [17] 2020-06-20 15:23:40 ( 52.0ms ) python3 ( 7.19s ) Adam ( Parameter Group 0 amsgrad: False betas: (0.9, 0.999) eps: 1e-08 lr: 0.001 weight_decay: 0 ) This means that you can compare optimizer algorithms easily through multiple runs with minimal effort.","title":"Object type"},{"location":"tutorial/core/#creating-a-run-from-a-client","text":"In the above examples, we created runs using the experiment.create_run() . In addition, you can do the same thing by client.create_run() with an experiment name as the first argument. The following code blocks are equivalent. Code 1 experiment = client.create_experiment('torch') run = experiment.create_run(fold=3) Code 2 run = client.create_run('torch', fold=3)","title":"Creating a run from a client"},{"location":"tutorial/data/","text":"Set of Data classes Ivory uses four classes for data presentation: Data , Dataset , Datasets , and DataLoaders . In this tutorial, we use the following Python module to explain them. File 5 rectangle/data.py from dataclasses import dataclass import numpy as np import ivory.core.data from ivory.utils.fold import kfold_split def create_data(num_samples=1000): xy = 4 * np.random.rand(num_samples, 2) + 1 xy = xy.astype(np.float32) dx = 0.1 * (np.random.rand(num_samples) - 0.5) dy = 0.1 * (np.random.rand(num_samples) - 0.5) z = ((xy[:, 0] + dx) * (xy[:, 1] + dy)).astype(np.float32) return xy, z @dataclass(repr=False) class Data(ivory.core.data.Data): n_splits: int = 4 DATA = create_data(1000) # Shared by each run. def init(self): # Called from self.__post_init__() self.input, self.target = self.DATA self.index = np.arange(len(self.input)) # Extra fold for test data. self.fold = kfold_split(self.input, n_splits=self.n_splits + 1) # Creating dummy test data just for demonstration. is_test = self.fold == self.n_splits # Use an extra fold. self.fold[is_test] = -1 # -1 for test data. self.target = self.target.copy() # n_splits may be different among runs. self.target[is_test] = np.nan # Delete target for test data. self.target = self.target.reshape(-1, 1) # (sample, class) def transform(mode, input, target): return input, target.reshape(-1) Data Class First import the module and check the basic behavior. import rectangle.data data = rectangle.data.Data() data [2] 2020-06-20 15:23:40 ( 5.00ms ) python3 ( 7.21s ) Data(train_size=800, test_size=200) In Data.init() , we need to define 4 attributes: index : Index of samples. input : Input data. target : Target data. fold : Fold number. Data.get() returns a tuple of ( index , input , target ). This function is called from Dataset instances when the dataset is indexed. data.get(0) # Integer index. [3] 2020-06-20 15:23:40 ( 5.00ms ) python3 ( 7.21s ) (0, array([2.0772183, 4.9461417], dtype=float32), array([10.418284], dtype=float32)) data.get([0, 10, 20]) # Array-like index. list or np.ndarray [4] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.22s ) (array([ 0, 10, 20]), array([[2.0772183, 4.9461417], [3.9508767, 1.6149015], [1.3982536, 2.687021 ]], dtype=float32), array([[10.418284 ], [ 6.441745 ], [ 3.8166006]], dtype=float32)) Dataset Class An instance of the Dataset class holds one of train, validation, and test dataset. We use the Ivory's default Dataset here instead of defining a subclass. Dataset() initializer requires three arguments: A Data instance, mode , and fold . import ivory.core.data dataset = ivory.core.data.Dataset(data, 'train', 0) dataset [5] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.22s ) Dataset(mode='train', num_samples=600) ivory.core.data.Dataset(data, 'val', 1) # Another mode is `test`. [6] 2020-06-20 15:23:40 ( 3.00ms ) python3 ( 7.23s ) Dataset(mode='val', num_samples=200) As the Data , the Dataset has init() without any arguments and returned value. You can define any code to modify data. To get data from an Dataset instance, use normal indexing dataset[0] # Integer index. [7] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.23s ) (0, array([2.0772183, 4.9461417], dtype=float32), array([10.418284], dtype=float32)) dataset[[0, 10, 20]] # Array-like index. list or np.ndarray [8] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.23s ) (array([ 0, 16, 33]), array([[2.0772183, 4.9461417], [4.3623295, 3.6915543], [1.6263498, 4.103133 ]], dtype=float32), array([[10.418284], [15.843957], [ 6.581139]], dtype=float32)) index, *_ = dataset[:] # Get all data. print(len(index)) index[:10] [9] 2020-06-20 15:23:40 ( 5.00ms ) python3 ( 7.24s ) 600 array([ 0, 2, 3, 4, 6, 7, 10, 12, 13, 15]) These data come from a subset of the Data instance according to the mode and fold. The Dataset takes an optional and callable argument: transform . def transform(mode: str, input, target): if mode == 'train': input = input * 2 target = target * 2 return input, target dataset_transformed = ivory.core.data.Dataset(data, 'train', 0, transform) dataset_transformed[0] [10] 2020-06-20 15:23:40 ( 5.00ms ) python3 ( 7.24s ) (0, array([4.1544366, 9.892283 ], dtype=float32), array([20.836569], dtype=float32)) 2 * dataset[0][1], 2 * dataset[0][2] [11] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.25s ) (array([4.1544366, 9.892283 ], dtype=float32), array([20.836569], dtype=float32)) Usually, we don't instantiate the Dataset directly. Instead, the Datasets class create dataset instances. Datasets Class An instance of the Datasets class holds a set of train, validation, and test dataset. We use the Ivory's default Datasets here instead of defining a subclass. The Datasets() initializer requires three arguments: A Data instance, Dataset factory, and fold . from ivory.core.data import Dataset datasets = ivory.core.data.Datasets(data, Dataset, 0) datasets [12] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.25s ) Datasets(data=Data(train_size=800, test_size=200), dataset=<class 'ivory.core.data.Dataset'>, fold=0) Note The second argument ( dataset ) is not a Dataset instance but its factory that returns a Dataset instance. It may be a Dataset itself or any other function that returns a Dataset instance. for mode, dataset in datasets.items(): print(mode, dataset) [13] 2020-06-20 15:23:40 ( 10.0ms ) python3 ( 7.26s ) train Dataset(mode='train', num_samples=600) val Dataset(mode='val', num_samples=200) test Dataset(mode='test', num_samples=200) Each dataset can be accessed by indexing or attributes. datasets['train'], datasets.val [14] 2020-06-20 15:23:40 ( 5.00ms ) python3 ( 7.27s ) (Dataset(mode='train', num_samples=600), Dataset(mode='val', num_samples=200)) Using the Datasets , we can easily split a whole data stored in a Data instance into three train, validation, and test dataset. DataLoaders Class The DataLoaders class is used internally by ivory.torch.trainer.Trainer or ivory.nnabla.trainer.Trainer classes to yield a minibatch in training loop. from ivory.torch.data import DataLoaders dataloaders = DataLoaders(datasets, batch_size=4, shuffle=True) dataloaders [15] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.27s ) DataLoaders(['train', 'val', 'test']) for mode, dataloader in dataloaders.items(): print(mode, dataloader) [16] 2020-06-20 15:23:40 ( 8.00ms ) python3 ( 7.28s ) train <torch.utils.data.dataloader.DataLoader object at 0x00000140518D5408> val <torch.utils.data.dataloader.DataLoader object at 0x00000140518D59C8> test <torch.utils.data.dataloader.DataLoader object at 0x00000140518D5448> next(iter(dataloaders.train)) # Shuffled [17] 2020-06-20 15:23:40 ( 13.0ms ) python3 ( 7.29s ) [tensor([110, 857, 0, 21], dtype=torch.int32), tensor([[3.0064, 3.9603], [1.2233, 2.2332], [2.0772, 4.9461], [4.4023, 2.1658]]), tensor([[11.8943], [ 2.7360], [10.4183], [ 9.3503]])] next(iter(dataloaders.val)) # Not shuffled, regardless of `shuffle` argument [18] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.30s ) [tensor([ 1, 5, 8, 14], dtype=torch.int32), tensor([[1.9011, 4.9406], [2.8263, 2.8066], [2.2066, 3.6569], [4.0281, 1.4676]]), tensor([[9.5829], [8.0773], [8.1560], [5.7046]])] next(iter(dataloaders.test)) # Not shuffled, regardless of `shuffle` argument [19] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.30s ) [tensor([ 9, 11, 19, 23], dtype=torch.int32), tensor([[4.3053, 4.1206], [1.8368, 3.9429], [3.0406, 3.5035], [3.2625, 4.5660]]), tensor([[nan], [nan], [nan], [nan]])]","title":"Set of Data classes"},{"location":"tutorial/data/#set-of-data-classes","text":"Ivory uses four classes for data presentation: Data , Dataset , Datasets , and DataLoaders . In this tutorial, we use the following Python module to explain them. File 5 rectangle/data.py from dataclasses import dataclass import numpy as np import ivory.core.data from ivory.utils.fold import kfold_split def create_data(num_samples=1000): xy = 4 * np.random.rand(num_samples, 2) + 1 xy = xy.astype(np.float32) dx = 0.1 * (np.random.rand(num_samples) - 0.5) dy = 0.1 * (np.random.rand(num_samples) - 0.5) z = ((xy[:, 0] + dx) * (xy[:, 1] + dy)).astype(np.float32) return xy, z @dataclass(repr=False) class Data(ivory.core.data.Data): n_splits: int = 4 DATA = create_data(1000) # Shared by each run. def init(self): # Called from self.__post_init__() self.input, self.target = self.DATA self.index = np.arange(len(self.input)) # Extra fold for test data. self.fold = kfold_split(self.input, n_splits=self.n_splits + 1) # Creating dummy test data just for demonstration. is_test = self.fold == self.n_splits # Use an extra fold. self.fold[is_test] = -1 # -1 for test data. self.target = self.target.copy() # n_splits may be different among runs. self.target[is_test] = np.nan # Delete target for test data. self.target = self.target.reshape(-1, 1) # (sample, class) def transform(mode, input, target): return input, target.reshape(-1)","title":"Set of Data classes"},{"location":"tutorial/data/#data-class","text":"First import the module and check the basic behavior. import rectangle.data data = rectangle.data.Data() data [2] 2020-06-20 15:23:40 ( 5.00ms ) python3 ( 7.21s ) Data(train_size=800, test_size=200) In Data.init() , we need to define 4 attributes: index : Index of samples. input : Input data. target : Target data. fold : Fold number. Data.get() returns a tuple of ( index , input , target ). This function is called from Dataset instances when the dataset is indexed. data.get(0) # Integer index. [3] 2020-06-20 15:23:40 ( 5.00ms ) python3 ( 7.21s ) (0, array([2.0772183, 4.9461417], dtype=float32), array([10.418284], dtype=float32)) data.get([0, 10, 20]) # Array-like index. list or np.ndarray [4] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.22s ) (array([ 0, 10, 20]), array([[2.0772183, 4.9461417], [3.9508767, 1.6149015], [1.3982536, 2.687021 ]], dtype=float32), array([[10.418284 ], [ 6.441745 ], [ 3.8166006]], dtype=float32))","title":"Data Class"},{"location":"tutorial/data/#dataset-class","text":"An instance of the Dataset class holds one of train, validation, and test dataset. We use the Ivory's default Dataset here instead of defining a subclass. Dataset() initializer requires three arguments: A Data instance, mode , and fold . import ivory.core.data dataset = ivory.core.data.Dataset(data, 'train', 0) dataset [5] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.22s ) Dataset(mode='train', num_samples=600) ivory.core.data.Dataset(data, 'val', 1) # Another mode is `test`. [6] 2020-06-20 15:23:40 ( 3.00ms ) python3 ( 7.23s ) Dataset(mode='val', num_samples=200) As the Data , the Dataset has init() without any arguments and returned value. You can define any code to modify data. To get data from an Dataset instance, use normal indexing dataset[0] # Integer index. [7] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.23s ) (0, array([2.0772183, 4.9461417], dtype=float32), array([10.418284], dtype=float32)) dataset[[0, 10, 20]] # Array-like index. list or np.ndarray [8] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.23s ) (array([ 0, 16, 33]), array([[2.0772183, 4.9461417], [4.3623295, 3.6915543], [1.6263498, 4.103133 ]], dtype=float32), array([[10.418284], [15.843957], [ 6.581139]], dtype=float32)) index, *_ = dataset[:] # Get all data. print(len(index)) index[:10] [9] 2020-06-20 15:23:40 ( 5.00ms ) python3 ( 7.24s ) 600 array([ 0, 2, 3, 4, 6, 7, 10, 12, 13, 15]) These data come from a subset of the Data instance according to the mode and fold. The Dataset takes an optional and callable argument: transform . def transform(mode: str, input, target): if mode == 'train': input = input * 2 target = target * 2 return input, target dataset_transformed = ivory.core.data.Dataset(data, 'train', 0, transform) dataset_transformed[0] [10] 2020-06-20 15:23:40 ( 5.00ms ) python3 ( 7.24s ) (0, array([4.1544366, 9.892283 ], dtype=float32), array([20.836569], dtype=float32)) 2 * dataset[0][1], 2 * dataset[0][2] [11] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.25s ) (array([4.1544366, 9.892283 ], dtype=float32), array([20.836569], dtype=float32)) Usually, we don't instantiate the Dataset directly. Instead, the Datasets class create dataset instances.","title":"Dataset Class"},{"location":"tutorial/data/#datasets-class","text":"An instance of the Datasets class holds a set of train, validation, and test dataset. We use the Ivory's default Datasets here instead of defining a subclass. The Datasets() initializer requires three arguments: A Data instance, Dataset factory, and fold . from ivory.core.data import Dataset datasets = ivory.core.data.Datasets(data, Dataset, 0) datasets [12] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.25s ) Datasets(data=Data(train_size=800, test_size=200), dataset=<class 'ivory.core.data.Dataset'>, fold=0) Note The second argument ( dataset ) is not a Dataset instance but its factory that returns a Dataset instance. It may be a Dataset itself or any other function that returns a Dataset instance. for mode, dataset in datasets.items(): print(mode, dataset) [13] 2020-06-20 15:23:40 ( 10.0ms ) python3 ( 7.26s ) train Dataset(mode='train', num_samples=600) val Dataset(mode='val', num_samples=200) test Dataset(mode='test', num_samples=200) Each dataset can be accessed by indexing or attributes. datasets['train'], datasets.val [14] 2020-06-20 15:23:40 ( 5.00ms ) python3 ( 7.27s ) (Dataset(mode='train', num_samples=600), Dataset(mode='val', num_samples=200)) Using the Datasets , we can easily split a whole data stored in a Data instance into three train, validation, and test dataset.","title":"Datasets Class"},{"location":"tutorial/data/#dataloaders-class","text":"The DataLoaders class is used internally by ivory.torch.trainer.Trainer or ivory.nnabla.trainer.Trainer classes to yield a minibatch in training loop. from ivory.torch.data import DataLoaders dataloaders = DataLoaders(datasets, batch_size=4, shuffle=True) dataloaders [15] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.27s ) DataLoaders(['train', 'val', 'test']) for mode, dataloader in dataloaders.items(): print(mode, dataloader) [16] 2020-06-20 15:23:40 ( 8.00ms ) python3 ( 7.28s ) train <torch.utils.data.dataloader.DataLoader object at 0x00000140518D5408> val <torch.utils.data.dataloader.DataLoader object at 0x00000140518D59C8> test <torch.utils.data.dataloader.DataLoader object at 0x00000140518D5448> next(iter(dataloaders.train)) # Shuffled [17] 2020-06-20 15:23:40 ( 13.0ms ) python3 ( 7.29s ) [tensor([110, 857, 0, 21], dtype=torch.int32), tensor([[3.0064, 3.9603], [1.2233, 2.2332], [2.0772, 4.9461], [4.4023, 2.1658]]), tensor([[11.8943], [ 2.7360], [10.4183], [ 9.3503]])] next(iter(dataloaders.val)) # Not shuffled, regardless of `shuffle` argument [18] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.30s ) [tensor([ 1, 5, 8, 14], dtype=torch.int32), tensor([[1.9011, 4.9406], [2.8263, 2.8066], [2.2066, 3.6569], [4.0281, 1.4676]]), tensor([[9.5829], [8.0773], [8.1560], [5.7046]])] next(iter(dataloaders.test)) # Not shuffled, regardless of `shuffle` argument [19] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.30s ) [tensor([ 9, 11, 19, 23], dtype=torch.int32), tensor([[4.3053, 4.1206], [1.8368, 3.9429], [3.0406, 3.5035], [3.2625, 4.5660]]), tensor([[nan], [nan], [nan], [nan]])]","title":"DataLoaders Class"},{"location":"tutorial/instance/","text":"Creating Instances In this tutorial, we will learn about Ivory's internal instance creation system. This is worth to understand the way of writing a YAML file for machine learning. Basic idea A syntax to create an instance is similar to a dictionary. example = ExampleCalss(arg1=123, arg2='abc') can be equivalently written as {'example': {'class': 'ExampleCalss', 'args1': 123, 'arg2': 'abc'}} Ivory exactly uses this relationship. from ivory.core.instance import create_instance params = {'data': {'class': 'rectangle.data.Data', 'n_splits': 5}} data = create_instance(params, 'data') data [2] 2020-06-20 15:23:40 ( 5.00ms ) python3 ( 7.32s ) Data(train_size=834, test_size=166) Here, the create_instance() requires the second argument name to specify a key because the first argument params can have multiple keys. Note that we added a n_splits parameter that is different from the default value 5. Let's see unique values of fold. import numpy as np np.unique(data.fold) # 5-fold for train and 1-fold for test. [3] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.32s ) array([-1, 0, 1, 2, 3, 4], dtype=int8) For writing a dictionary easily, we use PyYAML library in this tutorial. import yaml # A helper function. def create(doc, name, **kwargs): params = yaml.safe_load(doc) return create_instance(params, name, **kwargs) doc = \"\"\" data: class: rectangle.data.Data n_splits: 5 \"\"\" create(doc, 'data') [4] 2020-06-20 15:23:40 ( 7.00ms ) python3 ( 7.33s ) Data(train_size=834, test_size=166) Hierarchal Structure Next create a Dataset instance. The Dataset class requires a Data instance as the first argument so that the corresponding dictionary have a hierarchal structure. doc = \"\"\" dataset: class: ivory.core.data.Dataset data: class: rectangle.data.Data n_splits: 5 mode: train fold: 0 \"\"\" create(doc, 'dataset') [5] 2020-06-20 15:23:40 ( 6.00ms ) python3 ( 7.34s ) Dataset(mode='train', num_samples=667) As you can see, Ivory can treat this hierarchal structure correctly. Next, create a Datasets instance. doc = \"\"\" datasets: class: ivory.core.data.Datasets data: class: rectangle.data.Data n_splits: 5 dataset: def: ivory.core.data.Dataset fold: 0 \"\"\" create(doc, 'datasets') [6] 2020-06-20 15:23:40 ( 6.00ms ) python3 ( 7.34s ) Datasets(data=Data(train_size=834, test_size=166), dataset=<class 'ivory.core.data.Dataset'>, fold=0) Remember that the argument dataset for the Datasets class is not an instance but a callable that returns a Dataset instance (See the previous section ). To describe this behavior, we use a new def key to create a callable instead of a class key. Default Class In the above example, the two lines using an Ivory's original class seems to be verbose a little bit. Ivory adds a default class if the class or def key is missing. Here is the list of default classes prepared by Ivory: from ivory.core.default import DEFAULT_CLASS for library, values in DEFAULT_CLASS.items(): print(f'library: {library}') for name, value in values.items(): print(\" \", name, \"---\", value) [7] 2020-06-20 15:23:40 ( 80.7ms ) python3 ( 7.42s ) library: core client --- ivory.core.client.Client tracker --- ivory.core.tracker.Tracker tuner --- ivory.core.tuner.Tuner experiment --- ivory.core.base.Experiment objective --- ivory.core.objective.Objective run --- ivory.core.run.Run task --- ivory.core.run.Task study --- ivory.core.run.Study data --- ivory.core.data.Data dataset --- ivory.core.data.Dataset datasets --- ivory.core.data.Datasets results --- ivory.callbacks.results.Results metrics --- ivory.callbacks.metrics.Metrics monitor --- ivory.callbacks.monitor.Monitor early_stopping --- ivory.callbacks.early_stopping.EarlyStopping library: torch run --- ivory.torch.run.Run dataset --- ivory.torch.data.Dataset results --- ivory.torch.results.Results metrics --- ivory.torch.metrics.Metrics trainer --- ivory.torch.trainer.Trainer library: tensorflow run --- ivory.tensorflow.run.Run trainer --- ivory.tensorflow.trainer.Trainer library: nnabla results --- ivory.callbacks.results.BatchResults metrics --- ivory.nnabla.metrics.Metrics trainer --- ivory.nnabla.trainer.Trainer library: sklearn estimator --- ivory.sklearn.estimator.Estimator metrics --- ivory.sklearn.metrics.Metrics Therefore, we can omit the lines using default classes like below. Here, the library key is used to overload the default classes of the ivory.core package by the specific library. import torch.utils.data doc = \"\"\" library: torch # Use default class for PyTorch. datasets: data: class: rectangle.data.Data n_splits: 5 dataset: fold: 0 \"\"\" datasets = create(doc, 'datasets') isinstance(datasets.train, torch.utils.data.Dataset) [8] 2020-06-20 15:23:40 ( 7.00ms ) python3 ( 7.43s ) True Default Value If a callable has arguments with default value, you can use __default__ to get the default value from the callable signature. doc = \"\"\" datasets: data: class: rectangle.data.Data n_splits: __default__ dataset: fold: 0 \"\"\" datasets = create(doc, 'datasets') datasets.data.n_splits [9] 2020-06-20 15:23:40 ( 6.00ms ) python3 ( 7.44s ) 4 Positional Arguments Do you know the name of the first argument of numpy.array() ? import numpy as np print(np.array.__doc__[:200]) [10] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.44s ) array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0) Create an array. Parameters ---------- object : array_like An array, any object exposing the array inter It's object . But do you want to write like this? doc = \"\"\" x: class: numpy.array # Or `call` instead of `class`. object: [1, 2, 3] \"\"\" create(doc, 'x') [11] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.44s ) array([1, 2, 3]) This is inconvenient and ugly. Use underscore-notation : doc = \"\"\" x: class: numpy.array _: [1, 2, 3] \"\"\" create(doc, 'x') [12] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.45s ) array([1, 2, 3]) The second argument of numpy.array() is dtype . You can also use double underscore , which is unpacked. doc = \"\"\" x: call: numpy.array __: [[1, 2, 3], 'float'] \"\"\" create(doc, 'x') [13] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.45s ) array([1., 2., 3.])","title":"Creating Instances"},{"location":"tutorial/instance/#creating-instances","text":"In this tutorial, we will learn about Ivory's internal instance creation system. This is worth to understand the way of writing a YAML file for machine learning.","title":"Creating Instances"},{"location":"tutorial/instance/#basic-idea","text":"A syntax to create an instance is similar to a dictionary. example = ExampleCalss(arg1=123, arg2='abc') can be equivalently written as {'example': {'class': 'ExampleCalss', 'args1': 123, 'arg2': 'abc'}} Ivory exactly uses this relationship. from ivory.core.instance import create_instance params = {'data': {'class': 'rectangle.data.Data', 'n_splits': 5}} data = create_instance(params, 'data') data [2] 2020-06-20 15:23:40 ( 5.00ms ) python3 ( 7.32s ) Data(train_size=834, test_size=166) Here, the create_instance() requires the second argument name to specify a key because the first argument params can have multiple keys. Note that we added a n_splits parameter that is different from the default value 5. Let's see unique values of fold. import numpy as np np.unique(data.fold) # 5-fold for train and 1-fold for test. [3] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.32s ) array([-1, 0, 1, 2, 3, 4], dtype=int8) For writing a dictionary easily, we use PyYAML library in this tutorial. import yaml # A helper function. def create(doc, name, **kwargs): params = yaml.safe_load(doc) return create_instance(params, name, **kwargs) doc = \"\"\" data: class: rectangle.data.Data n_splits: 5 \"\"\" create(doc, 'data') [4] 2020-06-20 15:23:40 ( 7.00ms ) python3 ( 7.33s ) Data(train_size=834, test_size=166)","title":"Basic idea"},{"location":"tutorial/instance/#hierarchal-structure","text":"Next create a Dataset instance. The Dataset class requires a Data instance as the first argument so that the corresponding dictionary have a hierarchal structure. doc = \"\"\" dataset: class: ivory.core.data.Dataset data: class: rectangle.data.Data n_splits: 5 mode: train fold: 0 \"\"\" create(doc, 'dataset') [5] 2020-06-20 15:23:40 ( 6.00ms ) python3 ( 7.34s ) Dataset(mode='train', num_samples=667) As you can see, Ivory can treat this hierarchal structure correctly. Next, create a Datasets instance. doc = \"\"\" datasets: class: ivory.core.data.Datasets data: class: rectangle.data.Data n_splits: 5 dataset: def: ivory.core.data.Dataset fold: 0 \"\"\" create(doc, 'datasets') [6] 2020-06-20 15:23:40 ( 6.00ms ) python3 ( 7.34s ) Datasets(data=Data(train_size=834, test_size=166), dataset=<class 'ivory.core.data.Dataset'>, fold=0) Remember that the argument dataset for the Datasets class is not an instance but a callable that returns a Dataset instance (See the previous section ). To describe this behavior, we use a new def key to create a callable instead of a class key.","title":"Hierarchal Structure"},{"location":"tutorial/instance/#default-class","text":"In the above example, the two lines using an Ivory's original class seems to be verbose a little bit. Ivory adds a default class if the class or def key is missing. Here is the list of default classes prepared by Ivory: from ivory.core.default import DEFAULT_CLASS for library, values in DEFAULT_CLASS.items(): print(f'library: {library}') for name, value in values.items(): print(\" \", name, \"---\", value) [7] 2020-06-20 15:23:40 ( 80.7ms ) python3 ( 7.42s ) library: core client --- ivory.core.client.Client tracker --- ivory.core.tracker.Tracker tuner --- ivory.core.tuner.Tuner experiment --- ivory.core.base.Experiment objective --- ivory.core.objective.Objective run --- ivory.core.run.Run task --- ivory.core.run.Task study --- ivory.core.run.Study data --- ivory.core.data.Data dataset --- ivory.core.data.Dataset datasets --- ivory.core.data.Datasets results --- ivory.callbacks.results.Results metrics --- ivory.callbacks.metrics.Metrics monitor --- ivory.callbacks.monitor.Monitor early_stopping --- ivory.callbacks.early_stopping.EarlyStopping library: torch run --- ivory.torch.run.Run dataset --- ivory.torch.data.Dataset results --- ivory.torch.results.Results metrics --- ivory.torch.metrics.Metrics trainer --- ivory.torch.trainer.Trainer library: tensorflow run --- ivory.tensorflow.run.Run trainer --- ivory.tensorflow.trainer.Trainer library: nnabla results --- ivory.callbacks.results.BatchResults metrics --- ivory.nnabla.metrics.Metrics trainer --- ivory.nnabla.trainer.Trainer library: sklearn estimator --- ivory.sklearn.estimator.Estimator metrics --- ivory.sklearn.metrics.Metrics Therefore, we can omit the lines using default classes like below. Here, the library key is used to overload the default classes of the ivory.core package by the specific library. import torch.utils.data doc = \"\"\" library: torch # Use default class for PyTorch. datasets: data: class: rectangle.data.Data n_splits: 5 dataset: fold: 0 \"\"\" datasets = create(doc, 'datasets') isinstance(datasets.train, torch.utils.data.Dataset) [8] 2020-06-20 15:23:40 ( 7.00ms ) python3 ( 7.43s ) True","title":"Default Class"},{"location":"tutorial/instance/#default-value","text":"If a callable has arguments with default value, you can use __default__ to get the default value from the callable signature. doc = \"\"\" datasets: data: class: rectangle.data.Data n_splits: __default__ dataset: fold: 0 \"\"\" datasets = create(doc, 'datasets') datasets.data.n_splits [9] 2020-06-20 15:23:40 ( 6.00ms ) python3 ( 7.44s ) 4","title":"Default Value"},{"location":"tutorial/instance/#positional-arguments","text":"Do you know the name of the first argument of numpy.array() ? import numpy as np print(np.array.__doc__[:200]) [10] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.44s ) array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0) Create an array. Parameters ---------- object : array_like An array, any object exposing the array inter It's object . But do you want to write like this? doc = \"\"\" x: class: numpy.array # Or `call` instead of `class`. object: [1, 2, 3] \"\"\" create(doc, 'x') [11] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.44s ) array([1, 2, 3]) This is inconvenient and ugly. Use underscore-notation : doc = \"\"\" x: class: numpy.array _: [1, 2, 3] \"\"\" create(doc, 'x') [12] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.45s ) array([1, 2, 3]) The second argument of numpy.array() is dtype . You can also use double underscore , which is unpacked. doc = \"\"\" x: call: numpy.array __: [[1, 2, 3], 'float'] \"\"\" create(doc, 'x') [13] 2020-06-20 15:23:40 ( 4.00ms ) python3 ( 7.45s ) array([1., 2., 3.])","title":"Positional Arguments"},{"location":"tutorial/library/","text":"Library Comparison So far, we have used PyTorch in this tutorial, but Ivory can perform machine learning with other libraries. Base Parameter File Before examples, we write two base or template parameter files, which are extended by other parameter files later. File 14 A base parameter YAML file for various libraries (data.yml) datasets: data: class: rectangle.data.Data n_splits: 4 dataset: fold: 0 File 15 A base parameter YAML file for various libraries (base.yml) extends: data model: hidden_sizes: [20, 30] optimizer: lr: 1e-3 results: metrics: monitor: metric: val_loss trainer: loss: mse batch_size: 5 epochs: 5 shuffle: false verbose: 2 In base.yml , the first line \" extends: data \" means that the file extends (or includes, in this case) data.yml . Neural Network Libraries In this section we compare three neural network libraries ( TensorFlow , NNabla , and PyTorch ), and show that using different libraries on the same problem is straightforward. import tensorflow import nnabla import torch print(tensorflow.__version__) print(nnabla.__version__) print(torch.__version__) [3] 2020-06-20 15:23:40 ( 1.22s ) python3 ( 8.70s ) 2020-06-20 15:23:41,736 [nnabla][INFO]: Initializing CPU extension... C:\\Users\\daizu\\miniconda3\\envs\\daizu\\lib\\site-packages\\nnabla\\function_bases.py:58: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly spec.args[1:], spec.varargs, spec.keywords, defaults) 2.1.0 1.7.0 1.5.0+cu101 First define models: File 16 A Model definition in TensorFlow (rectangle/tensorflow.py) from tensorflow import keras from tensorflow.keras.layers import Dense def create_model(hidden_sizes): layers = [Dense(hidden_sizes[0], activation=\"relu\", input_shape=[2])] for hidden_size in hidden_sizes[1:]: layers.append(Dense(hidden_size, activation=\"relu\")) layers.append(Dense(1)) return keras.Sequential(layers) File 17 A Model definition in NNabla (rectangle/nnabla.py) import nnabla as nn import nnabla.functions as F import nnabla.parametric_functions as PF import ivory.nnabla.model class Model(ivory.nnabla.model.Model): def __init__(self, hidden_sizes): super().__init__() self.hidden_sizes = hidden_sizes def forward(self, x): for k, hidden_size in enumerate(self.hidden_sizes): with nn.parameter_scope(f\"layer{k}\"): x = F.relu(PF.affine(x, hidden_size)) with nn.parameter_scope(f\"layer{k+1}\"): x = PF.affine(x, 1) return x File 18 A Model definition in PyTorch (rectangle/torch.py) import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] for in_features, out_features in zip([2] + hidden_sizes, hidden_sizes + [1]): layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) For simplicity, the TensorFlow model is defined by using the keras.Sequential() , so that we call the create_model() to get the model. Next, write parameter YAML files: File 19 A parameter YAML file for TensorFlow (tensorflow.yml) library: tensorflow extends: base model: call: rectangle.tensorflow.create_model optimizer: class: tensorflow.keras.optimizers.SGD File 20 A parameter YAML file for NNabla (nnabla.yml) library: nnabla extends: base model: class: rectangle.nnabla.Model optimizer: class: nnabla.solvers.Sgd File 21 A parameter YAML fine for PyTorch (torch2.yml) library: torch extends: base model: class: rectangle.torch.Model optimizer: class: torch.optim.SGD _: $.model.parameters() These YAML files are very similar. The only difference is that, in PyTorch, an optimizer needs model parameters at the time of instantiation. Note The model for TensorFlow is a function. A new call key is used. (But you can stil use class , or call for a class, vice versa, because both a class and function are callable .) Next, create three runs. import ivory client = ivory.create_client(\"examples\") run_tf = client.create_run('tensorflow') run_nn = client.create_run('nnabla') run_torch = client.create_run('torch2') [4] 2020-06-20 15:23:41 ( 1.64s ) python3 ( 10.3s ) [I 200620 15:23:41 tracker:48] A new experiment created with name: 'tensorflow' C:\\Users\\daizu\\miniconda3\\envs\\daizu\\lib\\site-packages\\nnabla\\parametric_functions.py:98: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly defaults + (None,)) C:\\Users\\daizu\\miniconda3\\envs\\daizu\\lib\\site-packages\\nnabla\\solvers.py:18: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly from .solver import * [I 200620 15:23:43 tracker:48] A new experiment created with name: 'nnabla' [I 200620 15:23:43 tracker:48] A new experiment created with name: 'torch2' For comparison, equalize initial parameters. import torch # These three lines are only needed for this example. run, trainer = run_nn, run_nn.trainer run.model.build(trainer.loss, run.datasets.train, trainer.batch_size) run.optimizer.set_parameters(run.model.parameters()) ws_tf = run_tf.model.weights ws_nn = run_nn.model.parameters().values() ws_torch = run_torch.model.parameters() for w_tf, w_nn, w_torch in zip(ws_tf, ws_nn, ws_torch): w_nn.data.data = w_tf.numpy() w_torch.data = torch.tensor(w_tf.numpy().T) [5] 2020-06-20 15:23:43 ( 14.0ms ) python3 ( 10.4s ) Then, start the runs. run_tf.start('both') # Slower due to usage of GPU for a simple network. [6] 2020-06-20 15:23:43 ( 1.89s ) python3 ( 12.2s ) C:\\Users\\daizu\\miniconda3\\envs\\daizu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py:1389: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working if isinstance(sample_weight_mode, collections.Mapping): C:\\Users\\daizu\\miniconda3\\envs\\daizu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py:544: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working if isinstance(inputs, collections.Sequence): [epoch#0] loss=10.74 val_loss=5.403 best [epoch#1] loss=5.6 val_loss=3.937 best [epoch#2] loss=4.053 val_loss=2.613 best [epoch#3] loss=2.655 val_loss=1.571 best [epoch#4] loss=1.63 val_loss=0.9675 best run_nn.start('both') [7] 2020-06-20 15:23:45 ( 842ms ) python3 ( 13.1s ) 2020-06-20 15:23:45,382 [nnabla][INFO]: DataSource with shuffle(False) 2020-06-20 15:23:45,383 [nnabla][INFO]: Using DataIterator 2020-06-20 15:23:45,385 [nnabla][INFO]: DataSource with shuffle(False) 2020-06-20 15:23:45,386 [nnabla][INFO]: Using DataIterator 2020-06-20 15:23:45,387 [nnabla][INFO]: DataSource with shuffle(False) 2020-06-20 15:23:45,388 [nnabla][INFO]: Using DataIterator [epoch#0] loss=10.74 val_loss=5.403 best [epoch#1] loss=5.6 val_loss=3.937 best [epoch#2] loss=4.053 val_loss=2.613 best [epoch#3] loss=2.655 val_loss=1.571 best [epoch#4] loss=1.63 val_loss=0.9675 best run_torch.start('both') [8] 2020-06-20 15:23:46 ( 812ms ) python3 ( 13.9s ) [epoch#0] loss=10.74 val_loss=5.403 lr=0.001 best [epoch#1] loss=5.6 val_loss=3.937 lr=0.001 best [epoch#2] loss=4.053 val_loss=2.613 lr=0.001 best [epoch#3] loss=2.655 val_loss=1.571 lr=0.001 best [epoch#4] loss=1.63 val_loss=0.9675 lr=0.001 best Metrics during training are almost same. Visualize the results: import matplotlib.pyplot as plt # A helper function def plot(run): dataset = run.results.val plt.scatter(dataset.target.reshape(-1), dataset.output.reshape(-1)) plt.xlim(0, 25) plt.ylim(0, 25) plt.xlabel('Target values') plt.ylabel('Predicted values') for run in [run_tf, run_nn, run_torch]: plot(run) [9] 2020-06-20 15:23:47 ( 95.1ms ) python3 ( 14.0s ) Actual outputs are like below: x = run_tf.datasets.val[:5][1] run_tf.model.predict(x) [10] 2020-06-20 15:23:47 ( 21.0ms ) python3 ( 14.0s ) array([[9.168745 ], [9.302884 ], [8.557519 ], [6.005666 ], [3.7306597]], dtype=float32) x = run_nn.datasets.val[:5][1] run_nn.model(x) [11] 2020-06-20 15:23:47 ( 4.00ms ) python3 ( 14.0s ) array([[9.168744 ], [9.302884 ], [8.557518 ], [6.005667 ], [3.7306597]], dtype=float32) x = run_torch.datasets.val[:5][1] run_torch.model(torch.tensor(x)) [12] 2020-06-20 15:23:47 ( 6.00ms ) python3 ( 14.0s ) tensor([[9.1687], [9.3029], [8.5575], [6.0057], [3.7307]], grad_fn=<AddmmBackward>) You can ensemble these results, although this is meaningless in this example. from ivory.callbacks.results import concatenate results = concatenate(run.results for run in [run_tf, run_nn, run_torch]) index = results.val.index.argsort() results.val.output[index[:15]] [13] 2020-06-20 15:23:47 ( 5.00ms ) python3 ( 14.0s ) array([[9.168744 ], [9.168744 ], [9.168745 ], [9.302884 ], [9.302884 ], [9.302884 ], [8.557519 ], [8.557518 ], [8.557519 ], [6.0056663], [6.005667 ], [6.0056663], [3.7306597], [3.7306602], [3.7306592]], dtype=float32) reduced_results = results.mean() reduced_results.val.output[:5] [14] 2020-06-20 15:23:47 ( 11.0ms ) python3 ( 14.0s ) array([[9.168744 ], [9.302884 ], [8.557519 ], [6.0056667], [3.7306597]], dtype=float32) Scikit-learn Ivory can optimize various scikit-learn 's estimators. Before showing some examples, we need reshape the target array. File 22 A base parameter YAML file for various estimators (data2.yml) extends: data datasets: dataset: transform: rectangle.data.transform The dataset has a transform argument. This function reshapes the target array to match the shape for scikit-learn estimators (1D from 2D). Code 3 rectangle.data.transform() TypeError: module, class, method, function, traceback, frame, or code object was expected, got list TypeError Traceback (most recent call last) <ipython-input-166-2e957216a06d> in <module> 28 source = f\"@dataclass{args}\\n{source}\" 29 return source ---> 30 getsource(_) <ipython-input-166-2e957216a06d> in getsource(obj) 4 else: 5 is_dataclass = False ----> 6 source = inspect.getsource(obj) 7 defaults = [('init', True), ('repr', True), ('eq', True), ('order', False), 8 ('unsafe_hash', False), ('frozen', False)] ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getsource(object) 971 or code object. The source code is returned as a single string. An 972 OSError is raised if the source code cannot be retrieved.\"\"\" --> 973 lines, lnum = getsourcelines(object) 974 return ''.join(lines) 975 ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getsourcelines(object) 953 raised if the source code cannot be retrieved.\"\"\" 954 object = unwrap(object) --> 955 lines, lnum = findsource(object) 956 957 if istraceback(object): ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in findsource(object) 766 is raised if the source code cannot be retrieved.\"\"\" 767 --> 768 file = getsourcefile(object) 769 if file: 770 # Invalidate cache if needed. ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getsourcefile(object) 682 Return None if no way can be identified to get the source. 683 \"\"\" --> 684 filename = getfile(object) 685 all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:] 686 all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:] ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getfile(object) 664 raise TypeError('module, class, method, function, traceback, frame, or ' 665 'code object was expected, got {}'.format( --> 666 type(object).__name__)) 667 668 def getmodulename(path): RandomForestRegressor File 23 A parameter YAML file for RandomForestRegressor (rfr.yml) library: sklearn extends: data2 estimator: model: sklearn.ensemble.RandomForestRegressor n_estimators: 5 max_depth: 3 results: metrics: There are nothing difference to start a run. run = client.create_run('rfr') run.start() [17] 2020-06-20 15:23:47 ( 380ms ) python3 ( 14.6s ) [I 200620 15:23:47 tracker:48] A new experiment created with name: 'rfr' [run#0] mse=2.635 Because RandomForestRegressor estimator has a criterion attribute, the metrics are automatically calculated. Take a look at the outputs. plot(run) [18] 2020-06-20 15:23:47 ( 76.0ms ) python3 ( 14.7s ) Ridge File 24 A parameter YAML file for Ridge (ridge.yml) library: sklearn extends: data2 estimator: model: sklearn.linear_model.Ridge results: metrics: mse: mse_2: rectangle.metrics.mean_squared_error Because Ridge estimator has no criterion attribute, you have to specify metrics if you need. A mse key has empty ( None ) value. In this case, the default function ( sklearn.metrics.mean_squared_error() ) is chosen. On the other hand, mse_2 's value is a custom function's name: Code 4 rectangle.metrics.mean_squared_error() TypeError: module, class, method, function, traceback, frame, or code object was expected, got list TypeError Traceback (most recent call last) <ipython-input-172-2e957216a06d> in <module> 28 source = f\"@dataclass{args}\\n{source}\" 29 return source ---> 30 getsource(_) <ipython-input-172-2e957216a06d> in getsource(obj) 4 else: 5 is_dataclass = False ----> 6 source = inspect.getsource(obj) 7 defaults = [('init', True), ('repr', True), ('eq', True), ('order', False), 8 ('unsafe_hash', False), ('frozen', False)] ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getsource(object) 971 or code object. The source code is returned as a single string. An 972 OSError is raised if the source code cannot be retrieved.\"\"\" --> 973 lines, lnum = getsourcelines(object) 974 return ''.join(lines) 975 ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getsourcelines(object) 953 raised if the source code cannot be retrieved.\"\"\" 954 object = unwrap(object) --> 955 lines, lnum = findsource(object) 956 957 if istraceback(object): ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in findsource(object) 766 is raised if the source code cannot be retrieved.\"\"\" 767 --> 768 file = getsourcefile(object) 769 if file: 770 # Invalidate cache if needed. ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getsourcefile(object) 682 Return None if no way can be identified to get the source. 683 \"\"\" --> 684 filename = getfile(object) 685 all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:] 686 all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:] ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getfile(object) 664 raise TypeError('module, class, method, function, traceback, frame, or ' 665 'code object was expected, got {}'.format( --> 666 type(object).__name__)) 667 668 def getmodulename(path): This functionality allows us to add arbitrary metrics as long as they can be calculated with true and pred arrays . run = client.create_run('ridge') run.start() # Both metrics would give the same values. [21] 2020-06-20 15:23:47 ( 374ms ) python3 ( 15.1s ) [I 200620 15:23:48 tracker:48] A new experiment created with name: 'ridge' [run#0] mse=1.77 mse_2=1.77 plot(run) [22] 2020-06-20 15:23:48 ( 80.7ms ) python3 ( 15.2s ) LightGBM For LightGBM , Ivory implements two estimators: ivory.lightgbm.estimator.Regressor ivory.lightgbm.estimator.Classifier File 25 A parameter YAML file for LightGBM (lgb.yml) extends: data2 estimator: class: ivory.lightgbm.estimator.Regressor boosting_type: gbdt num_leaves: 10 learning_rate: 0.1 max_depth: 4 num_boost_round: 10 verbose_eval: 2 results: metrics: mse: run = client.create_run('lgb') run.start() [23] 2020-06-20 15:23:48 ( 308ms ) python3 ( 15.5s ) [I 200620 15:23:48 tracker:48] A new experiment created with name: 'lgb' [2] training's l2: 16.6352 valid_1's l2: 15.7235 [4] training's l2: 11.526 valid_1's l2: 11.0195 [6] training's l2: 8.00956 valid_1's l2: 7.87861 [8] training's l2: 5.5925 valid_1's l2: 5.66989 [10] training's l2: 3.92488 valid_1's l2: 4.09496 [run#0] mse=4.095 plot(run) [24] 2020-06-20 15:23:48 ( 86.0ms ) python3 ( 15.6s )","title":"Library Comparison"},{"location":"tutorial/library/#library-comparison","text":"So far, we have used PyTorch in this tutorial, but Ivory can perform machine learning with other libraries.","title":"Library Comparison"},{"location":"tutorial/library/#base-parameter-file","text":"Before examples, we write two base or template parameter files, which are extended by other parameter files later. File 14 A base parameter YAML file for various libraries (data.yml) datasets: data: class: rectangle.data.Data n_splits: 4 dataset: fold: 0 File 15 A base parameter YAML file for various libraries (base.yml) extends: data model: hidden_sizes: [20, 30] optimizer: lr: 1e-3 results: metrics: monitor: metric: val_loss trainer: loss: mse batch_size: 5 epochs: 5 shuffle: false verbose: 2 In base.yml , the first line \" extends: data \" means that the file extends (or includes, in this case) data.yml .","title":"Base Parameter File"},{"location":"tutorial/library/#neural-network-libraries","text":"In this section we compare three neural network libraries ( TensorFlow , NNabla , and PyTorch ), and show that using different libraries on the same problem is straightforward. import tensorflow import nnabla import torch print(tensorflow.__version__) print(nnabla.__version__) print(torch.__version__) [3] 2020-06-20 15:23:40 ( 1.22s ) python3 ( 8.70s ) 2020-06-20 15:23:41,736 [nnabla][INFO]: Initializing CPU extension... C:\\Users\\daizu\\miniconda3\\envs\\daizu\\lib\\site-packages\\nnabla\\function_bases.py:58: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly spec.args[1:], spec.varargs, spec.keywords, defaults) 2.1.0 1.7.0 1.5.0+cu101 First define models: File 16 A Model definition in TensorFlow (rectangle/tensorflow.py) from tensorflow import keras from tensorflow.keras.layers import Dense def create_model(hidden_sizes): layers = [Dense(hidden_sizes[0], activation=\"relu\", input_shape=[2])] for hidden_size in hidden_sizes[1:]: layers.append(Dense(hidden_size, activation=\"relu\")) layers.append(Dense(1)) return keras.Sequential(layers) File 17 A Model definition in NNabla (rectangle/nnabla.py) import nnabla as nn import nnabla.functions as F import nnabla.parametric_functions as PF import ivory.nnabla.model class Model(ivory.nnabla.model.Model): def __init__(self, hidden_sizes): super().__init__() self.hidden_sizes = hidden_sizes def forward(self, x): for k, hidden_size in enumerate(self.hidden_sizes): with nn.parameter_scope(f\"layer{k}\"): x = F.relu(PF.affine(x, hidden_size)) with nn.parameter_scope(f\"layer{k+1}\"): x = PF.affine(x, 1) return x File 18 A Model definition in PyTorch (rectangle/torch.py) import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] for in_features, out_features in zip([2] + hidden_sizes, hidden_sizes + [1]): layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) For simplicity, the TensorFlow model is defined by using the keras.Sequential() , so that we call the create_model() to get the model. Next, write parameter YAML files: File 19 A parameter YAML file for TensorFlow (tensorflow.yml) library: tensorflow extends: base model: call: rectangle.tensorflow.create_model optimizer: class: tensorflow.keras.optimizers.SGD File 20 A parameter YAML file for NNabla (nnabla.yml) library: nnabla extends: base model: class: rectangle.nnabla.Model optimizer: class: nnabla.solvers.Sgd File 21 A parameter YAML fine for PyTorch (torch2.yml) library: torch extends: base model: class: rectangle.torch.Model optimizer: class: torch.optim.SGD _: $.model.parameters() These YAML files are very similar. The only difference is that, in PyTorch, an optimizer needs model parameters at the time of instantiation. Note The model for TensorFlow is a function. A new call key is used. (But you can stil use class , or call for a class, vice versa, because both a class and function are callable .) Next, create three runs. import ivory client = ivory.create_client(\"examples\") run_tf = client.create_run('tensorflow') run_nn = client.create_run('nnabla') run_torch = client.create_run('torch2') [4] 2020-06-20 15:23:41 ( 1.64s ) python3 ( 10.3s ) [I 200620 15:23:41 tracker:48] A new experiment created with name: 'tensorflow' C:\\Users\\daizu\\miniconda3\\envs\\daizu\\lib\\site-packages\\nnabla\\parametric_functions.py:98: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly defaults + (None,)) C:\\Users\\daizu\\miniconda3\\envs\\daizu\\lib\\site-packages\\nnabla\\solvers.py:18: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly from .solver import * [I 200620 15:23:43 tracker:48] A new experiment created with name: 'nnabla' [I 200620 15:23:43 tracker:48] A new experiment created with name: 'torch2' For comparison, equalize initial parameters. import torch # These three lines are only needed for this example. run, trainer = run_nn, run_nn.trainer run.model.build(trainer.loss, run.datasets.train, trainer.batch_size) run.optimizer.set_parameters(run.model.parameters()) ws_tf = run_tf.model.weights ws_nn = run_nn.model.parameters().values() ws_torch = run_torch.model.parameters() for w_tf, w_nn, w_torch in zip(ws_tf, ws_nn, ws_torch): w_nn.data.data = w_tf.numpy() w_torch.data = torch.tensor(w_tf.numpy().T) [5] 2020-06-20 15:23:43 ( 14.0ms ) python3 ( 10.4s ) Then, start the runs. run_tf.start('both') # Slower due to usage of GPU for a simple network. [6] 2020-06-20 15:23:43 ( 1.89s ) python3 ( 12.2s ) C:\\Users\\daizu\\miniconda3\\envs\\daizu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py:1389: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working if isinstance(sample_weight_mode, collections.Mapping): C:\\Users\\daizu\\miniconda3\\envs\\daizu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py:544: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working if isinstance(inputs, collections.Sequence): [epoch#0] loss=10.74 val_loss=5.403 best [epoch#1] loss=5.6 val_loss=3.937 best [epoch#2] loss=4.053 val_loss=2.613 best [epoch#3] loss=2.655 val_loss=1.571 best [epoch#4] loss=1.63 val_loss=0.9675 best run_nn.start('both') [7] 2020-06-20 15:23:45 ( 842ms ) python3 ( 13.1s ) 2020-06-20 15:23:45,382 [nnabla][INFO]: DataSource with shuffle(False) 2020-06-20 15:23:45,383 [nnabla][INFO]: Using DataIterator 2020-06-20 15:23:45,385 [nnabla][INFO]: DataSource with shuffle(False) 2020-06-20 15:23:45,386 [nnabla][INFO]: Using DataIterator 2020-06-20 15:23:45,387 [nnabla][INFO]: DataSource with shuffle(False) 2020-06-20 15:23:45,388 [nnabla][INFO]: Using DataIterator [epoch#0] loss=10.74 val_loss=5.403 best [epoch#1] loss=5.6 val_loss=3.937 best [epoch#2] loss=4.053 val_loss=2.613 best [epoch#3] loss=2.655 val_loss=1.571 best [epoch#4] loss=1.63 val_loss=0.9675 best run_torch.start('both') [8] 2020-06-20 15:23:46 ( 812ms ) python3 ( 13.9s ) [epoch#0] loss=10.74 val_loss=5.403 lr=0.001 best [epoch#1] loss=5.6 val_loss=3.937 lr=0.001 best [epoch#2] loss=4.053 val_loss=2.613 lr=0.001 best [epoch#3] loss=2.655 val_loss=1.571 lr=0.001 best [epoch#4] loss=1.63 val_loss=0.9675 lr=0.001 best Metrics during training are almost same. Visualize the results: import matplotlib.pyplot as plt # A helper function def plot(run): dataset = run.results.val plt.scatter(dataset.target.reshape(-1), dataset.output.reshape(-1)) plt.xlim(0, 25) plt.ylim(0, 25) plt.xlabel('Target values') plt.ylabel('Predicted values') for run in [run_tf, run_nn, run_torch]: plot(run) [9] 2020-06-20 15:23:47 ( 95.1ms ) python3 ( 14.0s ) Actual outputs are like below: x = run_tf.datasets.val[:5][1] run_tf.model.predict(x) [10] 2020-06-20 15:23:47 ( 21.0ms ) python3 ( 14.0s ) array([[9.168745 ], [9.302884 ], [8.557519 ], [6.005666 ], [3.7306597]], dtype=float32) x = run_nn.datasets.val[:5][1] run_nn.model(x) [11] 2020-06-20 15:23:47 ( 4.00ms ) python3 ( 14.0s ) array([[9.168744 ], [9.302884 ], [8.557518 ], [6.005667 ], [3.7306597]], dtype=float32) x = run_torch.datasets.val[:5][1] run_torch.model(torch.tensor(x)) [12] 2020-06-20 15:23:47 ( 6.00ms ) python3 ( 14.0s ) tensor([[9.1687], [9.3029], [8.5575], [6.0057], [3.7307]], grad_fn=<AddmmBackward>) You can ensemble these results, although this is meaningless in this example. from ivory.callbacks.results import concatenate results = concatenate(run.results for run in [run_tf, run_nn, run_torch]) index = results.val.index.argsort() results.val.output[index[:15]] [13] 2020-06-20 15:23:47 ( 5.00ms ) python3 ( 14.0s ) array([[9.168744 ], [9.168744 ], [9.168745 ], [9.302884 ], [9.302884 ], [9.302884 ], [8.557519 ], [8.557518 ], [8.557519 ], [6.0056663], [6.005667 ], [6.0056663], [3.7306597], [3.7306602], [3.7306592]], dtype=float32) reduced_results = results.mean() reduced_results.val.output[:5] [14] 2020-06-20 15:23:47 ( 11.0ms ) python3 ( 14.0s ) array([[9.168744 ], [9.302884 ], [8.557519 ], [6.0056667], [3.7306597]], dtype=float32)","title":"Neural Network Libraries"},{"location":"tutorial/library/#scikit-learn","text":"Ivory can optimize various scikit-learn 's estimators. Before showing some examples, we need reshape the target array. File 22 A base parameter YAML file for various estimators (data2.yml) extends: data datasets: dataset: transform: rectangle.data.transform The dataset has a transform argument. This function reshapes the target array to match the shape for scikit-learn estimators (1D from 2D). Code 3 rectangle.data.transform() TypeError: module, class, method, function, traceback, frame, or code object was expected, got list TypeError Traceback (most recent call last) <ipython-input-166-2e957216a06d> in <module> 28 source = f\"@dataclass{args}\\n{source}\" 29 return source ---> 30 getsource(_) <ipython-input-166-2e957216a06d> in getsource(obj) 4 else: 5 is_dataclass = False ----> 6 source = inspect.getsource(obj) 7 defaults = [('init', True), ('repr', True), ('eq', True), ('order', False), 8 ('unsafe_hash', False), ('frozen', False)] ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getsource(object) 971 or code object. The source code is returned as a single string. An 972 OSError is raised if the source code cannot be retrieved.\"\"\" --> 973 lines, lnum = getsourcelines(object) 974 return ''.join(lines) 975 ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getsourcelines(object) 953 raised if the source code cannot be retrieved.\"\"\" 954 object = unwrap(object) --> 955 lines, lnum = findsource(object) 956 957 if istraceback(object): ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in findsource(object) 766 is raised if the source code cannot be retrieved.\"\"\" 767 --> 768 file = getsourcefile(object) 769 if file: 770 # Invalidate cache if needed. ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getsourcefile(object) 682 Return None if no way can be identified to get the source. 683 \"\"\" --> 684 filename = getfile(object) 685 all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:] 686 all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:] ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getfile(object) 664 raise TypeError('module, class, method, function, traceback, frame, or ' 665 'code object was expected, got {}'.format( --> 666 type(object).__name__)) 667 668 def getmodulename(path):","title":"Scikit-learn"},{"location":"tutorial/library/#randomforestregressor","text":"File 23 A parameter YAML file for RandomForestRegressor (rfr.yml) library: sklearn extends: data2 estimator: model: sklearn.ensemble.RandomForestRegressor n_estimators: 5 max_depth: 3 results: metrics: There are nothing difference to start a run. run = client.create_run('rfr') run.start() [17] 2020-06-20 15:23:47 ( 380ms ) python3 ( 14.6s ) [I 200620 15:23:47 tracker:48] A new experiment created with name: 'rfr' [run#0] mse=2.635 Because RandomForestRegressor estimator has a criterion attribute, the metrics are automatically calculated. Take a look at the outputs. plot(run) [18] 2020-06-20 15:23:47 ( 76.0ms ) python3 ( 14.7s )","title":"RandomForestRegressor"},{"location":"tutorial/library/#ridge","text":"File 24 A parameter YAML file for Ridge (ridge.yml) library: sklearn extends: data2 estimator: model: sklearn.linear_model.Ridge results: metrics: mse: mse_2: rectangle.metrics.mean_squared_error Because Ridge estimator has no criterion attribute, you have to specify metrics if you need. A mse key has empty ( None ) value. In this case, the default function ( sklearn.metrics.mean_squared_error() ) is chosen. On the other hand, mse_2 's value is a custom function's name: Code 4 rectangle.metrics.mean_squared_error() TypeError: module, class, method, function, traceback, frame, or code object was expected, got list TypeError Traceback (most recent call last) <ipython-input-172-2e957216a06d> in <module> 28 source = f\"@dataclass{args}\\n{source}\" 29 return source ---> 30 getsource(_) <ipython-input-172-2e957216a06d> in getsource(obj) 4 else: 5 is_dataclass = False ----> 6 source = inspect.getsource(obj) 7 defaults = [('init', True), ('repr', True), ('eq', True), ('order', False), 8 ('unsafe_hash', False), ('frozen', False)] ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getsource(object) 971 or code object. The source code is returned as a single string. An 972 OSError is raised if the source code cannot be retrieved.\"\"\" --> 973 lines, lnum = getsourcelines(object) 974 return ''.join(lines) 975 ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getsourcelines(object) 953 raised if the source code cannot be retrieved.\"\"\" 954 object = unwrap(object) --> 955 lines, lnum = findsource(object) 956 957 if istraceback(object): ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in findsource(object) 766 is raised if the source code cannot be retrieved.\"\"\" 767 --> 768 file = getsourcefile(object) 769 if file: 770 # Invalidate cache if needed. ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getsourcefile(object) 682 Return None if no way can be identified to get the source. 683 \"\"\" --> 684 filename = getfile(object) 685 all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:] 686 all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:] ~\\miniconda3\\envs\\daizu\\lib\\inspect.py in getfile(object) 664 raise TypeError('module, class, method, function, traceback, frame, or ' 665 'code object was expected, got {}'.format( --> 666 type(object).__name__)) 667 668 def getmodulename(path): This functionality allows us to add arbitrary metrics as long as they can be calculated with true and pred arrays . run = client.create_run('ridge') run.start() # Both metrics would give the same values. [21] 2020-06-20 15:23:47 ( 374ms ) python3 ( 15.1s ) [I 200620 15:23:48 tracker:48] A new experiment created with name: 'ridge' [run#0] mse=1.77 mse_2=1.77 plot(run) [22] 2020-06-20 15:23:48 ( 80.7ms ) python3 ( 15.2s )","title":"Ridge"},{"location":"tutorial/library/#lightgbm","text":"For LightGBM , Ivory implements two estimators: ivory.lightgbm.estimator.Regressor ivory.lightgbm.estimator.Classifier File 25 A parameter YAML file for LightGBM (lgb.yml) extends: data2 estimator: class: ivory.lightgbm.estimator.Regressor boosting_type: gbdt num_leaves: 10 learning_rate: 0.1 max_depth: 4 num_boost_round: 10 verbose_eval: 2 results: metrics: mse: run = client.create_run('lgb') run.start() [23] 2020-06-20 15:23:48 ( 308ms ) python3 ( 15.5s ) [I 200620 15:23:48 tracker:48] A new experiment created with name: 'lgb' [2] training's l2: 16.6352 valid_1's l2: 15.7235 [4] training's l2: 11.526 valid_1's l2: 11.0195 [6] training's l2: 8.00956 valid_1's l2: 7.87861 [8] training's l2: 5.5925 valid_1's l2: 5.66989 [10] training's l2: 3.92488 valid_1's l2: 4.09496 [run#0] mse=4.095 plot(run) [24] 2020-06-20 15:23:48 ( 86.0ms ) python3 ( 15.6s )","title":"LightGBM"},{"location":"tutorial/model/","text":"Model Structure Model We have prepared a Datasets instance for PyTorch. Now define a MLP model that works with this Datasets . The model is defined in rectangle/torch.py File 6 rectangle/torch.py import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] for in_features, out_features in zip([2] + hidden_sizes, hidden_sizes + [1]): layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) We again use Ivory's instance creation system . import yaml # A helper function. def create(doc, name, **kwargs): params = yaml.safe_load(doc) return create_instance(params, name, **kwargs) doc = \"\"\" library: torch datasets: data: class: rectangle.data.Data n_splits: 5 dataset: fold: 0 model: class: rectangle.torch.Model hidden_sizes: [3, 4, 5] \"\"\" datasets = create(doc, 'datasets') model = create(doc, 'model') model [2] 2020-06-20 15:23:48 ( 9.00ms ) python3 ( 15.6s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=3, bias=True) (1): Linear(in_features=3, out_features=4, bias=True) (2): Linear(in_features=4, out_features=5, bias=True) (3): Linear(in_features=5, out_features=1, bias=True) ) ) We can uses this model as usual. import torch index, input, target = datasets.train[:5] input [3] 2020-06-20 15:23:48 ( 4.00ms ) python3 ( 15.6s ) array([[2.0772183, 4.9461417], [1.8550245, 4.6405816], [4.6277347, 4.6336703], [2.3827288, 1.1293393], [2.826326 , 2.8065689]], dtype=float32) model(torch.tensor(input)) [4] 2020-06-20 15:23:48 ( 6.00ms ) python3 ( 15.6s ) tensor([[0.0749], [0.0749], [0.0619], [0.0629], [0.0596]], grad_fn=<AddmmBackward>) Optimizer To train a model, we need an optimizer. For example import torch.optim optimizer = torch.optim.SGD(params=model.parameters(), lr=1e-3) optimizer [5] 2020-06-20 15:23:48 ( 4.00ms ) python3 ( 15.6s ) SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) Now try to describe this optimizer in a dictionary style. However, the first argument params is neigher a simple literal nor an other instance. It is an iterable of learnable parameters obtained from a model. Ivory provides \" $ -notation \" to tackle this problem. doc = \"\"\" optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 0.001 \"\"\" optimizer = create(doc, 'optimizer', globals={'model': model}) optimizer [6] 2020-06-20 15:23:48 ( 5.00ms ) python3 ( 15.6s ) SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) A \" $ \" is a starting point to refer other instance stored in the globals dictionary. In this case, $.model is replaced by the model instance in globals , then .parameters() invokes a call of Model.parameters() . Scheduler A scheduler controls the learning rate of an optimizer. doc = \"\"\" scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 \"\"\" scheduler = create(doc, 'scheduler', globals={'optimizer': optimizer}) scheduler [7] 2020-06-20 15:23:48 ( 5.00ms ) python3 ( 15.6s ) <torch.optim.lr_scheduler.ReduceLROnPlateau at 0x1420126fd88> If a $ -notation has no suffix, the value becomes its key itself. The following two examples are equivalent: optimizer: $ optimizer: $.optimizer Now we have had both data and a model.","title":"Model Structure"},{"location":"tutorial/model/#model-structure","text":"","title":"Model Structure"},{"location":"tutorial/model/#model","text":"We have prepared a Datasets instance for PyTorch. Now define a MLP model that works with this Datasets . The model is defined in rectangle/torch.py File 6 rectangle/torch.py import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self, hidden_sizes): super().__init__() layers = [] for in_features, out_features in zip([2] + hidden_sizes, hidden_sizes + [1]): layers.append(nn.Linear(in_features, out_features)) self.layers = nn.ModuleList(layers) def forward(self, x): for layer in self.layers[:-1]: x = F.relu(layer(x)) return self.layers[-1](x) We again use Ivory's instance creation system . import yaml # A helper function. def create(doc, name, **kwargs): params = yaml.safe_load(doc) return create_instance(params, name, **kwargs) doc = \"\"\" library: torch datasets: data: class: rectangle.data.Data n_splits: 5 dataset: fold: 0 model: class: rectangle.torch.Model hidden_sizes: [3, 4, 5] \"\"\" datasets = create(doc, 'datasets') model = create(doc, 'model') model [2] 2020-06-20 15:23:48 ( 9.00ms ) python3 ( 15.6s ) Model( (layers): ModuleList( (0): Linear(in_features=2, out_features=3, bias=True) (1): Linear(in_features=3, out_features=4, bias=True) (2): Linear(in_features=4, out_features=5, bias=True) (3): Linear(in_features=5, out_features=1, bias=True) ) ) We can uses this model as usual. import torch index, input, target = datasets.train[:5] input [3] 2020-06-20 15:23:48 ( 4.00ms ) python3 ( 15.6s ) array([[2.0772183, 4.9461417], [1.8550245, 4.6405816], [4.6277347, 4.6336703], [2.3827288, 1.1293393], [2.826326 , 2.8065689]], dtype=float32) model(torch.tensor(input)) [4] 2020-06-20 15:23:48 ( 6.00ms ) python3 ( 15.6s ) tensor([[0.0749], [0.0749], [0.0619], [0.0629], [0.0596]], grad_fn=<AddmmBackward>)","title":"Model"},{"location":"tutorial/model/#optimizer","text":"To train a model, we need an optimizer. For example import torch.optim optimizer = torch.optim.SGD(params=model.parameters(), lr=1e-3) optimizer [5] 2020-06-20 15:23:48 ( 4.00ms ) python3 ( 15.6s ) SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) Now try to describe this optimizer in a dictionary style. However, the first argument params is neigher a simple literal nor an other instance. It is an iterable of learnable parameters obtained from a model. Ivory provides \" $ -notation \" to tackle this problem. doc = \"\"\" optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 0.001 \"\"\" optimizer = create(doc, 'optimizer', globals={'model': model}) optimizer [6] 2020-06-20 15:23:48 ( 5.00ms ) python3 ( 15.6s ) SGD ( Parameter Group 0 dampening: 0 lr: 0.001 momentum: 0 nesterov: False weight_decay: 0 ) A \" $ \" is a starting point to refer other instance stored in the globals dictionary. In this case, $.model is replaced by the model instance in globals , then .parameters() invokes a call of Model.parameters() .","title":"Optimizer"},{"location":"tutorial/model/#scheduler","text":"A scheduler controls the learning rate of an optimizer. doc = \"\"\" scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 \"\"\" scheduler = create(doc, 'scheduler', globals={'optimizer': optimizer}) scheduler [7] 2020-06-20 15:23:48 ( 5.00ms ) python3 ( 15.6s ) <torch.optim.lr_scheduler.ReduceLROnPlateau at 0x1420126fd88> If a $ -notation has no suffix, the value becomes its key itself. The following two examples are equivalent: optimizer: $ optimizer: $.optimizer Now we have had both data and a model.","title":"Scheduler"},{"location":"tutorial/task/","text":"Multiple Runs Task Ivory implements a special run class Task that controls multiple nested runs. Task is useful for parameter search or cross validation. import ivory client = ivory.create_client(\"examples\") # Set the working directory task = client.create_task('torch') # Or, experiment.create_task() task [3] 2020-06-20 15:23:48 ( 45.0ms ) python3 ( 15.7s ) [I 200620 15:23:48 tracker:48] A new experiment created with name: 'torch' Task(id='ff42564ad6bf4384a93355441e17e3ae', name='task#0', num_instances=3) The Task class has two functions to generate multiple runs: Task.prodcut() and Task.chain() . These two function have the same functionality as itertools of Python starndard library. Product The Task.prodcut() makes an iterator that returns runs from cartesian product of input parameters. task = client.create_task('torch') # verbose=0: No progress bar. runs = task.product(fold=range(2), factor=[0.5, 0.7], verbose=0) runs [4] 2020-06-20 15:23:48 ( 38.0ms ) python3 ( 15.7s ) <generator object Task.product at 0x0000014201493248> for run in runs: pass # Do somthing, for example, run.start() [5] 2020-06-20 15:23:48 ( 514ms ) python3 ( 16.3s ) [run#0] fold=0 factor=0.5 [run#1] fold=0 factor=0.7 [run#2] fold=1 factor=0.5 [run#3] fold=1 factor=0.7 You can specify other parameters that don't change during iteration. task = client.create_task('torch') runs = task.product(fold=range(2), factor=[0.5, 0.7], lr=1e-4, verbose=0) for run in runs: pass # Do somthing, for example, run.start() [6] 2020-06-20 15:23:49 ( 607ms ) python3 ( 16.9s ) [run#4] lr=0.0001 fold=0 factor=0.5 [run#5] lr=0.0001 fold=0 factor=0.7 [run#6] lr=0.0001 fold=1 factor=0.5 [run#7] lr=0.0001 fold=1 factor=0.7 Chain The Task.chain() makes an iterator that returns runs from the first input paramter until it is exhausted, then proceeds to the next parameter, until all of the parameters are exhausted. Other parameters have default values if they don't be specified by additional key-value pairs. task = client.create_task('torch') runs = task.chain( fold=range(2), factor=[0.5, 0.7], lr=[1e-4, 1e-3], batch_size=32, use_best_param=False, verbose=0) runs [7] 2020-06-20 15:23:50 ( 65.0ms ) python3 ( 16.9s ) <generator object Task.chain at 0x0000014201493148> for run in runs: pass # Do somthing, for example, run.start() [8] 2020-06-20 15:23:50 ( 908ms ) python3 ( 17.8s ) [run#8] batch_size=32 fold=0 [run#9] batch_size=32 fold=1 [run#10] batch_size=32 factor=0.5 [run#11] batch_size=32 factor=0.7 [run#12] batch_size=32 lr=0.0001 [run#13] batch_size=32 lr=0.001 The use_best_param keyword argument is useful for dynamic updating of parameters. If True (default), the parameter that got the best score is used during the following iterations. task = client.create_task('torch') runs = task.chain( fold=range(2), factor=[0.5, 0.7], lr=[1e-4, 1e-3], use_best_param=True, verbose=0) for run in runs: pass # Do somthing, for example, run.start() # We do nothing, so the first values are used. [9] 2020-06-20 15:23:51 ( 1.07s ) python3 ( 18.9s ) [run#14] fold=0 [run#15] fold=1 [run#16] factor=0.5 fold=0 [run#17] factor=0.7 fold=0 [run#18] lr=0.0001 fold=0 factor=0.5 [run#19] lr=0.001 fold=0 factor=0.5 Range Ivory provides the ivory.utils.range.Range class for parameter ranging. This class can be used as the standard range , but more flexible, especially for the float type. from ivory.utils.range import Range list(Range(6)) # The stop value is included. [10] 2020-06-20 15:23:52 ( 5.00ms ) python3 ( 18.9s ) [0, 1, 2, 3, 4, 5, 6] list(Range(3, 6)) # Start and stop. [11] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [3, 4, 5, 6] list(Range(3, 10, 2)) # Step size. [12] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [3, 5, 7, 9] list(Range(3, 10, num=4)) # Sampling size. [13] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [3, 5, 8, 10] list(Range(0.0, 1.0, 0.25)) # float type. [14] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [0.0, 0.25, 0.5, 0.75, 1.0] list(Range(0.0, 1.0, num=5)) # Sampling size [15] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [0.0, 0.25, 0.5, 0.75, 1.0] list(Range(1e-3, 1e2, num=6, log=True)) # Log scale [16] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] A Range instance can be created from a string. list(Range('3-7')) # <start>-<stop> [17] 2020-06-20 15:23:52 ( 5.00ms ) python3 ( 18.9s ) [3, 4, 5, 6, 7] list(Range('3-7-2')) # <start>-<stop>-<step> [18] 2020-06-20 15:23:52 ( 5.00ms ) python3 ( 18.9s ) [3, 5, 7] list(Range('0.0-1.0:5')) # <start>-<stop>:<num> [19] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [0.0, 0.25, 0.5, 0.75, 1.0] list(Range('1e-3_1e2:6.log')) # '_' instead of '-', log scale [20] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 19.0s ) [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]","title":"Multiple Runs"},{"location":"tutorial/task/#multiple-runs","text":"","title":"Multiple Runs"},{"location":"tutorial/task/#task","text":"Ivory implements a special run class Task that controls multiple nested runs. Task is useful for parameter search or cross validation. import ivory client = ivory.create_client(\"examples\") # Set the working directory task = client.create_task('torch') # Or, experiment.create_task() task [3] 2020-06-20 15:23:48 ( 45.0ms ) python3 ( 15.7s ) [I 200620 15:23:48 tracker:48] A new experiment created with name: 'torch' Task(id='ff42564ad6bf4384a93355441e17e3ae', name='task#0', num_instances=3) The Task class has two functions to generate multiple runs: Task.prodcut() and Task.chain() . These two function have the same functionality as itertools of Python starndard library.","title":"Task"},{"location":"tutorial/task/#product","text":"The Task.prodcut() makes an iterator that returns runs from cartesian product of input parameters. task = client.create_task('torch') # verbose=0: No progress bar. runs = task.product(fold=range(2), factor=[0.5, 0.7], verbose=0) runs [4] 2020-06-20 15:23:48 ( 38.0ms ) python3 ( 15.7s ) <generator object Task.product at 0x0000014201493248> for run in runs: pass # Do somthing, for example, run.start() [5] 2020-06-20 15:23:48 ( 514ms ) python3 ( 16.3s ) [run#0] fold=0 factor=0.5 [run#1] fold=0 factor=0.7 [run#2] fold=1 factor=0.5 [run#3] fold=1 factor=0.7 You can specify other parameters that don't change during iteration. task = client.create_task('torch') runs = task.product(fold=range(2), factor=[0.5, 0.7], lr=1e-4, verbose=0) for run in runs: pass # Do somthing, for example, run.start() [6] 2020-06-20 15:23:49 ( 607ms ) python3 ( 16.9s ) [run#4] lr=0.0001 fold=0 factor=0.5 [run#5] lr=0.0001 fold=0 factor=0.7 [run#6] lr=0.0001 fold=1 factor=0.5 [run#7] lr=0.0001 fold=1 factor=0.7","title":"Product"},{"location":"tutorial/task/#chain","text":"The Task.chain() makes an iterator that returns runs from the first input paramter until it is exhausted, then proceeds to the next parameter, until all of the parameters are exhausted. Other parameters have default values if they don't be specified by additional key-value pairs. task = client.create_task('torch') runs = task.chain( fold=range(2), factor=[0.5, 0.7], lr=[1e-4, 1e-3], batch_size=32, use_best_param=False, verbose=0) runs [7] 2020-06-20 15:23:50 ( 65.0ms ) python3 ( 16.9s ) <generator object Task.chain at 0x0000014201493148> for run in runs: pass # Do somthing, for example, run.start() [8] 2020-06-20 15:23:50 ( 908ms ) python3 ( 17.8s ) [run#8] batch_size=32 fold=0 [run#9] batch_size=32 fold=1 [run#10] batch_size=32 factor=0.5 [run#11] batch_size=32 factor=0.7 [run#12] batch_size=32 lr=0.0001 [run#13] batch_size=32 lr=0.001 The use_best_param keyword argument is useful for dynamic updating of parameters. If True (default), the parameter that got the best score is used during the following iterations. task = client.create_task('torch') runs = task.chain( fold=range(2), factor=[0.5, 0.7], lr=[1e-4, 1e-3], use_best_param=True, verbose=0) for run in runs: pass # Do somthing, for example, run.start() # We do nothing, so the first values are used. [9] 2020-06-20 15:23:51 ( 1.07s ) python3 ( 18.9s ) [run#14] fold=0 [run#15] fold=1 [run#16] factor=0.5 fold=0 [run#17] factor=0.7 fold=0 [run#18] lr=0.0001 fold=0 factor=0.5 [run#19] lr=0.001 fold=0 factor=0.5","title":"Chain"},{"location":"tutorial/task/#range","text":"Ivory provides the ivory.utils.range.Range class for parameter ranging. This class can be used as the standard range , but more flexible, especially for the float type. from ivory.utils.range import Range list(Range(6)) # The stop value is included. [10] 2020-06-20 15:23:52 ( 5.00ms ) python3 ( 18.9s ) [0, 1, 2, 3, 4, 5, 6] list(Range(3, 6)) # Start and stop. [11] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [3, 4, 5, 6] list(Range(3, 10, 2)) # Step size. [12] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [3, 5, 7, 9] list(Range(3, 10, num=4)) # Sampling size. [13] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [3, 5, 8, 10] list(Range(0.0, 1.0, 0.25)) # float type. [14] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [0.0, 0.25, 0.5, 0.75, 1.0] list(Range(0.0, 1.0, num=5)) # Sampling size [15] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [0.0, 0.25, 0.5, 0.75, 1.0] list(Range(1e-3, 1e2, num=6, log=True)) # Log scale [16] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] A Range instance can be created from a string. list(Range('3-7')) # <start>-<stop> [17] 2020-06-20 15:23:52 ( 5.00ms ) python3 ( 18.9s ) [3, 4, 5, 6, 7] list(Range('3-7-2')) # <start>-<stop>-<step> [18] 2020-06-20 15:23:52 ( 5.00ms ) python3 ( 18.9s ) [3, 5, 7] list(Range('0.0-1.0:5')) # <start>-<stop>:<num> [19] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 18.9s ) [0.0, 0.25, 0.5, 0.75, 1.0] list(Range('1e-3_1e2:6.log')) # '_' instead of '-', log scale [20] 2020-06-20 15:23:52 ( 4.00ms ) python3 ( 19.0s ) [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]","title":"Range"},{"location":"tutorial/tracking/","text":"Tracking Runs with Ivory First create several runs for demonstration. import ivory client = ivory.create_client(\"examples\") run = client.create_run('torch', fold=2) run.start() [3] 2020-06-20 15:23:52 ( 1.21s ) python3 ( 20.2s ) [I 200620 15:23:52 tracker:48] A new experiment created with name: 'torch' [epoch#0] loss=21.27 val_loss=6.933 lr=0.001 best [epoch#1] loss=7.318 val_loss=5.955 lr=0.001 best [epoch#2] loss=6.176 val_loss=5.054 lr=0.001 best [epoch#3] loss=5.281 val_loss=4.591 lr=0.001 best [epoch#4] loss=4.411 val_loss=3.402 lr=0.001 best [epoch#5] loss=3.327 val_loss=2.703 lr=0.001 best [epoch#6] loss=2.485 val_loss=1.81 lr=0.001 best [epoch#7] loss=1.838 val_loss=1.274 lr=0.001 best [epoch#8] loss=1.276 val_loss=0.9247 lr=0.001 best [epoch#9] loss=0.8944 val_loss=0.6791 lr=0.001 best run = client.create_run('torch', fold=3) run.start('both') [4] 2020-06-20 15:23:53 ( 1.26s ) python3 ( 21.5s ) [epoch#0] loss=21.51 val_loss=9.308 lr=0.001 best [epoch#1] loss=7.742 val_loss=9.016 lr=0.001 best [epoch#2] loss=7.198 val_loss=7.626 lr=0.001 best [epoch#3] loss=6.383 val_loss=6.84 lr=0.001 best [epoch#4] loss=5.829 val_loss=6.014 lr=0.001 best [epoch#5] loss=5.034 val_loss=5.606 lr=0.001 best [epoch#6] loss=4.281 val_loss=4.397 lr=0.001 best [epoch#7] loss=3.631 val_loss=3.608 lr=0.001 best [epoch#8] loss=2.765 val_loss=2.891 lr=0.001 best [epoch#9] loss=2.236 val_loss=2.267 lr=0.001 best task = client.create_task('torch') runs = task.product(fold=range(3), verbose=0) for run in runs: pass # Do something [5] 2020-06-20 15:23:54 ( 447ms ) python3 ( 21.9s ) [run#2] fold=0 [run#3] fold=1 [run#4] fold=2 task = client.create_task('torch') runs = task.product(n_splits=[3, 4], verbose=0) for run in runs: pass # Do something [6] 2020-06-20 15:23:55 ( 322ms ) python3 ( 22.3s ) [run#5] n_splits=3 [run#6] n_splits=4 task = client.create_task('torch') runs = task.chain(lr=[1e-4, 1e-3], batch_size=[16, 32], verbose=0) for run in runs: pass # Do something [7] 2020-06-20 15:23:55 ( 641ms ) python3 ( 22.9s ) [run#7] lr=0.0001 [run#8] lr=0.001 [run#9] batch_size=16 lr=0.0001 [run#10] batch_size=32 lr=0.0001 Tracking Interface Search functions Client.search_run_ids() makes an iterator that returns Run IDs of runs. # A helper function def print_run_info(run_ids): for run_id in run_ids: print(run_id[:5], client.get_run_name(run_id)) run_ids = client.search_run_ids('torch') print_run_info(run_ids) [8] 2020-06-20 15:23:56 ( 100ms ) python3 ( 23.0s ) cb865 run#10 ee268 run#9 86d77 run#8 1e101 run#7 14c20 task#2 cc4d9 run#6 4543c run#5 01d94 task#1 facee run#4 635c5 run#3 732d1 run#2 eb3a8 task#0 84a48 run#1 ce78f run#0 You can filtering runs by passing keyword arguments. run_ids = client.search_run_ids('torch', lr=1e-4, batch_size=32) print_run_info(run_ids) [9] 2020-06-20 15:23:56 ( 211ms ) python3 ( 23.2s ) cb865 run#10 Client.search_nested_run_ids() makes an iterator that returns Run IDs of runs that have a parent run. Optionally, you can filter runs. run_ids = client.search_nested_run_ids('torch') print_run_info(run_ids) [10] 2020-06-20 15:23:56 ( 71.0ms ) python3 ( 23.3s ) cb865 run#10 ee268 run#9 86d77 run#8 1e101 run#7 cc4d9 run#6 4543c run#5 facee run#4 635c5 run#3 732d1 run#2 Note that the run#0 isn't returned because it was created by Client.create_run() directly. Client.search_parent_run_ids() makes an iterator that returns Run IDs of runs that have nested runs. In this case, parent runs are three tasks we made above. run_ids = client.search_parent_run_ids('torch') print_run_info(run_ids) [11] 2020-06-20 15:23:56 ( 46.0ms ) python3 ( 23.3s ) 14c20 task#2 01d94 task#1 eb3a8 task#0 Get functions Client.get_run_id() returns a Run ID of runs you select by run name. run_id = client.get_run_id('torch', run=0) print_run_info([run_id]) [12] 2020-06-20 15:23:56 ( 39.0ms ) python3 ( 23.4s ) ce78f run#0 Client.get_run_ids() makes an iterator that returns Run IDs of runs you select by run names. run_ids = client.get_run_ids('torch', task=range(1, 3)) print_run_info(run_ids) [13] 2020-06-20 15:23:56 ( 81.7ms ) python3 ( 23.4s ) 01d94 task#1 14c20 task#2 Client.get_nested_run_ids() makes an iterator that returns Run IDs of runs that have a parent you select by run names. run_ids = client.get_nested_run_ids('torch', task=range(2)) print_run_info(run_ids) [14] 2020-06-20 15:23:56 ( 152ms ) python3 ( 23.6s ) facee run#4 635c5 run#3 732d1 run#2 cc4d9 run#6 4543c run#5 Client.get_parent_run_id() returns a Run ID of a run that is refered by a nested run. run_id = client.get_parent_run_id('torch', run=5) print_run_info([run_id]) [15] 2020-06-20 15:23:56 ( 41.0ms ) python3 ( 23.6s ) 01d94 task#1 Set function Sometimes, you may want to change a parent for nested runs. Use Client.set_parent_run_id() . run_ids = client.get_nested_run_ids('torch', task=2) print_run_info(run_ids) [16] 2020-06-20 15:23:56 ( 84.0ms ) python3 ( 23.7s ) cb865 run#10 ee268 run#9 86d77 run#8 1e101 run#7 client.set_parent_run_id('torch', run=(0, 2, 3), task=2) run_ids = client.get_nested_run_ids('torch', task=2) print_run_info(run_ids) [17] 2020-06-20 15:23:57 ( 222ms ) python3 ( 23.9s ) cb865 run#10 ee268 run#9 86d77 run#8 1e101 run#7 635c5 run#3 732d1 run#2 ce78f run#0 Next Step Once you got Run ID(s), you can load a run, a member of a run, or collect results of multiple runs for an ensemble. See the quickstart .","title":"Tracking Runs with Ivory"},{"location":"tutorial/tracking/#tracking-runs-with-ivory","text":"First create several runs for demonstration. import ivory client = ivory.create_client(\"examples\") run = client.create_run('torch', fold=2) run.start() [3] 2020-06-20 15:23:52 ( 1.21s ) python3 ( 20.2s ) [I 200620 15:23:52 tracker:48] A new experiment created with name: 'torch' [epoch#0] loss=21.27 val_loss=6.933 lr=0.001 best [epoch#1] loss=7.318 val_loss=5.955 lr=0.001 best [epoch#2] loss=6.176 val_loss=5.054 lr=0.001 best [epoch#3] loss=5.281 val_loss=4.591 lr=0.001 best [epoch#4] loss=4.411 val_loss=3.402 lr=0.001 best [epoch#5] loss=3.327 val_loss=2.703 lr=0.001 best [epoch#6] loss=2.485 val_loss=1.81 lr=0.001 best [epoch#7] loss=1.838 val_loss=1.274 lr=0.001 best [epoch#8] loss=1.276 val_loss=0.9247 lr=0.001 best [epoch#9] loss=0.8944 val_loss=0.6791 lr=0.001 best run = client.create_run('torch', fold=3) run.start('both') [4] 2020-06-20 15:23:53 ( 1.26s ) python3 ( 21.5s ) [epoch#0] loss=21.51 val_loss=9.308 lr=0.001 best [epoch#1] loss=7.742 val_loss=9.016 lr=0.001 best [epoch#2] loss=7.198 val_loss=7.626 lr=0.001 best [epoch#3] loss=6.383 val_loss=6.84 lr=0.001 best [epoch#4] loss=5.829 val_loss=6.014 lr=0.001 best [epoch#5] loss=5.034 val_loss=5.606 lr=0.001 best [epoch#6] loss=4.281 val_loss=4.397 lr=0.001 best [epoch#7] loss=3.631 val_loss=3.608 lr=0.001 best [epoch#8] loss=2.765 val_loss=2.891 lr=0.001 best [epoch#9] loss=2.236 val_loss=2.267 lr=0.001 best task = client.create_task('torch') runs = task.product(fold=range(3), verbose=0) for run in runs: pass # Do something [5] 2020-06-20 15:23:54 ( 447ms ) python3 ( 21.9s ) [run#2] fold=0 [run#3] fold=1 [run#4] fold=2 task = client.create_task('torch') runs = task.product(n_splits=[3, 4], verbose=0) for run in runs: pass # Do something [6] 2020-06-20 15:23:55 ( 322ms ) python3 ( 22.3s ) [run#5] n_splits=3 [run#6] n_splits=4 task = client.create_task('torch') runs = task.chain(lr=[1e-4, 1e-3], batch_size=[16, 32], verbose=0) for run in runs: pass # Do something [7] 2020-06-20 15:23:55 ( 641ms ) python3 ( 22.9s ) [run#7] lr=0.0001 [run#8] lr=0.001 [run#9] batch_size=16 lr=0.0001 [run#10] batch_size=32 lr=0.0001","title":"Tracking Runs with Ivory"},{"location":"tutorial/tracking/#tracking-interface","text":"","title":"Tracking Interface"},{"location":"tutorial/tracking/#search-functions","text":"Client.search_run_ids() makes an iterator that returns Run IDs of runs. # A helper function def print_run_info(run_ids): for run_id in run_ids: print(run_id[:5], client.get_run_name(run_id)) run_ids = client.search_run_ids('torch') print_run_info(run_ids) [8] 2020-06-20 15:23:56 ( 100ms ) python3 ( 23.0s ) cb865 run#10 ee268 run#9 86d77 run#8 1e101 run#7 14c20 task#2 cc4d9 run#6 4543c run#5 01d94 task#1 facee run#4 635c5 run#3 732d1 run#2 eb3a8 task#0 84a48 run#1 ce78f run#0 You can filtering runs by passing keyword arguments. run_ids = client.search_run_ids('torch', lr=1e-4, batch_size=32) print_run_info(run_ids) [9] 2020-06-20 15:23:56 ( 211ms ) python3 ( 23.2s ) cb865 run#10 Client.search_nested_run_ids() makes an iterator that returns Run IDs of runs that have a parent run. Optionally, you can filter runs. run_ids = client.search_nested_run_ids('torch') print_run_info(run_ids) [10] 2020-06-20 15:23:56 ( 71.0ms ) python3 ( 23.3s ) cb865 run#10 ee268 run#9 86d77 run#8 1e101 run#7 cc4d9 run#6 4543c run#5 facee run#4 635c5 run#3 732d1 run#2 Note that the run#0 isn't returned because it was created by Client.create_run() directly. Client.search_parent_run_ids() makes an iterator that returns Run IDs of runs that have nested runs. In this case, parent runs are three tasks we made above. run_ids = client.search_parent_run_ids('torch') print_run_info(run_ids) [11] 2020-06-20 15:23:56 ( 46.0ms ) python3 ( 23.3s ) 14c20 task#2 01d94 task#1 eb3a8 task#0","title":"Search functions"},{"location":"tutorial/tracking/#get-functions","text":"Client.get_run_id() returns a Run ID of runs you select by run name. run_id = client.get_run_id('torch', run=0) print_run_info([run_id]) [12] 2020-06-20 15:23:56 ( 39.0ms ) python3 ( 23.4s ) ce78f run#0 Client.get_run_ids() makes an iterator that returns Run IDs of runs you select by run names. run_ids = client.get_run_ids('torch', task=range(1, 3)) print_run_info(run_ids) [13] 2020-06-20 15:23:56 ( 81.7ms ) python3 ( 23.4s ) 01d94 task#1 14c20 task#2 Client.get_nested_run_ids() makes an iterator that returns Run IDs of runs that have a parent you select by run names. run_ids = client.get_nested_run_ids('torch', task=range(2)) print_run_info(run_ids) [14] 2020-06-20 15:23:56 ( 152ms ) python3 ( 23.6s ) facee run#4 635c5 run#3 732d1 run#2 cc4d9 run#6 4543c run#5 Client.get_parent_run_id() returns a Run ID of a run that is refered by a nested run. run_id = client.get_parent_run_id('torch', run=5) print_run_info([run_id]) [15] 2020-06-20 15:23:56 ( 41.0ms ) python3 ( 23.6s ) 01d94 task#1","title":"Get functions"},{"location":"tutorial/tracking/#set-function","text":"Sometimes, you may want to change a parent for nested runs. Use Client.set_parent_run_id() . run_ids = client.get_nested_run_ids('torch', task=2) print_run_info(run_ids) [16] 2020-06-20 15:23:56 ( 84.0ms ) python3 ( 23.7s ) cb865 run#10 ee268 run#9 86d77 run#8 1e101 run#7 client.set_parent_run_id('torch', run=(0, 2, 3), task=2) run_ids = client.get_nested_run_ids('torch', task=2) print_run_info(run_ids) [17] 2020-06-20 15:23:57 ( 222ms ) python3 ( 23.9s ) cb865 run#10 ee268 run#9 86d77 run#8 1e101 run#7 635c5 run#3 732d1 run#2 ce78f run#0","title":"Set function"},{"location":"tutorial/tracking/#next-step","text":"Once you got Run ID(s), you can load a run, a member of a run, or collect results of multiple runs for an ensemble. See the quickstart .","title":"Next Step"},{"location":"tutorial/training/","text":"Training a Model First, create data and model set. For more details about the following code, see Creating Instance section . import yaml params = yaml.safe_load(\"\"\" library: torch run: datasets: data: class: rectangle.data.Data n_splits: 4 dataset: fold: 0 model: class: rectangle.torch.Model hidden_sizes: [100, 100] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 0.001 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 results: metrics: monitor: metric: val_loss early_stopping: patience: 10 trainer: loss: torch.nn.functional.mse_loss batch_size: 10 epochs: 10 verbose: 2 \"\"\") params [2] 2020-06-20 15:23:57 ( 9.00ms ) python3 ( 24.0s ) {'library': 'torch', 'run': {'datasets': {'data': {'class': 'rectangle.data.Data', 'n_splits': 4}, 'dataset': None, 'fold': 0}, 'model': {'class': 'rectangle.torch.Model', 'hidden_sizes': [100, 100]}, 'optimizer': {'class': 'torch.optim.SGD', 'params': '$.model.parameters()', 'lr': 0.001}, 'scheduler': {'class': 'torch.optim.lr_scheduler.ReduceLROnPlateau', 'optimizer': '$', 'factor': 0.5, 'patience': 4}, 'results': None, 'metrics': None, 'monitor': {'metric': 'val_loss'}, 'early_stopping': {'patience': 10}, 'trainer': {'loss': 'torch.nn.functional.mse_loss', 'batch_size': 10, 'epochs': 10, 'verbose': 2}}} Note Key-order in the params dictionary is meaningful, because the callback functions are called by this order. For example, Monitor uses the results of Metrics so that Monitor should appear later than Metrics . ivory.core.instance.create_base_instance() is more useful to create a run from a dictionary than the ivory.core.instance.create_instance() because it can create multiple objects by one step processing $ -notation properly. import ivory.core.instance run = ivory.core.instance.create_base_instance(params, 'run') list(run) [3] 2020-06-20 15:23:57 ( 8.00ms ) python3 ( 24.0s ) ['datasets', 'model', 'optimizer', 'scheduler', 'results', 'metrics', 'monitor', 'early_stopping', 'trainer'] Callbacks Check callbacks of the Run instance. import ivory.core.base # A helper function def print_callbacks(obj): for func in ivory.core.base.Callback.METHODS: if hasattr(obj, func) and callable(getattr(obj, func)): print(' ', func) for name, obj in run.items(): print(f'[{name}]') print_callbacks(obj) [4] 2020-06-20 15:23:57 ( 34.0ms ) python3 ( 24.0s ) [datasets] [model] [optimizer] [scheduler] [results] on_train_begin on_train_end on_val_end on_test_begin on_test_end [metrics] on_epoch_begin on_train_begin on_train_end on_val_begin on_val_end on_epoch_end [monitor] on_epoch_end [early_stopping] on_epoch_end [trainer] on_init_begin on_train_begin on_val_begin on_epoch_end on_test_begin Metrics The role of Metrics class is to record a set of metric for evaluation of model performance. The metirics are updated at each epoch end. run.metrics # Now, metrics are empty. [5] 2020-06-20 15:23:57 ( 4.00ms ) python3 ( 24.0s ) Metrics() Monitor The Monitor class is monitoring the most important metric to measure the model score or to determine the training logic (early stopping or pruning). run.monitor # Monitoring `val_loss`. Lower is better. [6] 2020-06-20 15:23:57 ( 3.00ms ) python3 ( 24.0s ) Monitor(metric='val_loss', mode='min') EarlyStopping The EarlyStopping class is to stop the training loop when a monitored metric has stopped improving. run.early_stopping # Early stopping occurs when `wait` > `patience`. [7] 2020-06-20 15:23:57 ( 3.00ms ) python3 ( 24.0s ) EarlyStopping(patience=10, wait=0) Trainer The Tainer class controls the model training. This is a callback, but at the same time, invokes callback functions at each step of training, validation, and test loop. run.trainer # Training hasn't started yet, so epoch = -1. [8] 2020-06-20 15:23:57 ( 3.00ms ) python3 ( 24.0s ) Trainer(epoch=-1, epochs=10, global_step=-1, batch_size=10, shuffle=True, dataloaders='ivory.torch.data.DataLoaders', verbose=2, loss=<function mse_loss at 0x000001404B1C93A8>, gpu=False, precision=32, amp_level='O1', scheduler_step_mode='epoch') Using a Trainer A Run instance invokes its trainer by Run.start() . run.start() # create_callbacks() is called automatically. [9] 2020-06-20 15:23:57 ( 523ms ) python3 ( 24.5s ) [epoch#0] loss=17.13 val_loss=5.569 lr=0.001 best [epoch#1] loss=5.436 val_loss=3.977 lr=0.001 best [epoch#2] loss=3.7 val_loss=2.625 lr=0.001 best [epoch#3] loss=2.331 val_loss=1.614 lr=0.001 best [epoch#4] loss=1.427 val_loss=0.905 lr=0.001 best [epoch#5] loss=0.9515 val_loss=0.6364 lr=0.001 best [epoch#6] loss=0.7028 val_loss=0.5842 lr=0.001 best [epoch#7] loss=0.6472 val_loss=0.5076 lr=0.001 best [epoch#8] loss=0.5725 val_loss=0.4287 lr=0.001 best [epoch#9] loss=0.5375 val_loss=0.4063 lr=0.001 best You can update attributes of run's objects at any time. run.trainer.epochs = 5 run.start() [10] 2020-06-20 15:23:57 ( 275ms ) python3 ( 24.8s ) [epoch#10] loss=0.465 val_loss=0.3766 lr=0.001 best [epoch#11] loss=0.4343 val_loss=0.3537 lr=0.001 best [epoch#12] loss=0.4087 val_loss=0.3376 lr=0.001 best [epoch#13] loss=0.4152 val_loss=0.3248 lr=0.001 best [epoch#14] loss=0.3732 val_loss=0.3337 lr=0.001 Note The Run.start() doesn't reset the trainer's epoch. Callbacks after Training After training, the callbacks changes their states. run.metrics # Show metrics at current epoch. [11] 2020-06-20 15:23:58 ( 4.00ms ) python3 ( 24.8s ) Metrics(loss=0.3732, val_loss=0.3337, lr=0.001) run.metrics.history.val_loss # Metrics history. [12] 2020-06-20 15:23:58 ( 4.00ms ) python3 ( 24.8s ) {0: 5.56868793964386, 1: 3.976963722705841, 2: 2.6250638723373414, 3: 1.6140344977378844, 4: 0.9049779623746872, 5: 0.6364109225571155, 6: 0.584185541048646, 7: 0.5075690947473049, 8: 0.42866935580968857, 9: 0.4063351653516293, 10: 0.37661438062787056, 11: 0.35370331779122355, 12: 0.33763624504208567, 13: 0.3248191948980093, 14: 0.333729387819767} run.monitor # Store the best score and its epoch. [13] 2020-06-20 15:23:58 ( 4.00ms ) python3 ( 24.8s ) Monitor(metric='val_loss', mode='min', best_score=0.325, best_epoch=13) run.early_stopping # Current `wait`. [14] 2020-06-20 15:23:58 ( 3.00ms ) python3 ( 24.8s ) EarlyStopping(patience=10, wait=1) run.trainer # Current epoch is 14 (0-indexed). [15] 2020-06-20 15:23:58 ( 3.00ms ) python3 ( 24.8s ) Trainer(epoch=14, epochs=5, global_step=899, batch_size=10, shuffle=True, dataloaders='ivory.torch.data.DataLoaders', verbose=2, loss=<function mse_loss at 0x000001404B1C93A8>, gpu=False, precision=32, amp_level='O1', scheduler_step_mode='epoch')","title":"Training a Model"},{"location":"tutorial/training/#training-a-model","text":"First, create data and model set. For more details about the following code, see Creating Instance section . import yaml params = yaml.safe_load(\"\"\" library: torch run: datasets: data: class: rectangle.data.Data n_splits: 4 dataset: fold: 0 model: class: rectangle.torch.Model hidden_sizes: [100, 100] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 0.001 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 results: metrics: monitor: metric: val_loss early_stopping: patience: 10 trainer: loss: torch.nn.functional.mse_loss batch_size: 10 epochs: 10 verbose: 2 \"\"\") params [2] 2020-06-20 15:23:57 ( 9.00ms ) python3 ( 24.0s ) {'library': 'torch', 'run': {'datasets': {'data': {'class': 'rectangle.data.Data', 'n_splits': 4}, 'dataset': None, 'fold': 0}, 'model': {'class': 'rectangle.torch.Model', 'hidden_sizes': [100, 100]}, 'optimizer': {'class': 'torch.optim.SGD', 'params': '$.model.parameters()', 'lr': 0.001}, 'scheduler': {'class': 'torch.optim.lr_scheduler.ReduceLROnPlateau', 'optimizer': '$', 'factor': 0.5, 'patience': 4}, 'results': None, 'metrics': None, 'monitor': {'metric': 'val_loss'}, 'early_stopping': {'patience': 10}, 'trainer': {'loss': 'torch.nn.functional.mse_loss', 'batch_size': 10, 'epochs': 10, 'verbose': 2}}} Note Key-order in the params dictionary is meaningful, because the callback functions are called by this order. For example, Monitor uses the results of Metrics so that Monitor should appear later than Metrics . ivory.core.instance.create_base_instance() is more useful to create a run from a dictionary than the ivory.core.instance.create_instance() because it can create multiple objects by one step processing $ -notation properly. import ivory.core.instance run = ivory.core.instance.create_base_instance(params, 'run') list(run) [3] 2020-06-20 15:23:57 ( 8.00ms ) python3 ( 24.0s ) ['datasets', 'model', 'optimizer', 'scheduler', 'results', 'metrics', 'monitor', 'early_stopping', 'trainer']","title":"Training a Model"},{"location":"tutorial/training/#callbacks","text":"Check callbacks of the Run instance. import ivory.core.base # A helper function def print_callbacks(obj): for func in ivory.core.base.Callback.METHODS: if hasattr(obj, func) and callable(getattr(obj, func)): print(' ', func) for name, obj in run.items(): print(f'[{name}]') print_callbacks(obj) [4] 2020-06-20 15:23:57 ( 34.0ms ) python3 ( 24.0s ) [datasets] [model] [optimizer] [scheduler] [results] on_train_begin on_train_end on_val_end on_test_begin on_test_end [metrics] on_epoch_begin on_train_begin on_train_end on_val_begin on_val_end on_epoch_end [monitor] on_epoch_end [early_stopping] on_epoch_end [trainer] on_init_begin on_train_begin on_val_begin on_epoch_end on_test_begin","title":"Callbacks"},{"location":"tutorial/training/#metrics","text":"The role of Metrics class is to record a set of metric for evaluation of model performance. The metirics are updated at each epoch end. run.metrics # Now, metrics are empty. [5] 2020-06-20 15:23:57 ( 4.00ms ) python3 ( 24.0s ) Metrics()","title":"Metrics"},{"location":"tutorial/training/#monitor","text":"The Monitor class is monitoring the most important metric to measure the model score or to determine the training logic (early stopping or pruning). run.monitor # Monitoring `val_loss`. Lower is better. [6] 2020-06-20 15:23:57 ( 3.00ms ) python3 ( 24.0s ) Monitor(metric='val_loss', mode='min')","title":"Monitor"},{"location":"tutorial/training/#earlystopping","text":"The EarlyStopping class is to stop the training loop when a monitored metric has stopped improving. run.early_stopping # Early stopping occurs when `wait` > `patience`. [7] 2020-06-20 15:23:57 ( 3.00ms ) python3 ( 24.0s ) EarlyStopping(patience=10, wait=0)","title":"EarlyStopping"},{"location":"tutorial/training/#trainer","text":"The Tainer class controls the model training. This is a callback, but at the same time, invokes callback functions at each step of training, validation, and test loop. run.trainer # Training hasn't started yet, so epoch = -1. [8] 2020-06-20 15:23:57 ( 3.00ms ) python3 ( 24.0s ) Trainer(epoch=-1, epochs=10, global_step=-1, batch_size=10, shuffle=True, dataloaders='ivory.torch.data.DataLoaders', verbose=2, loss=<function mse_loss at 0x000001404B1C93A8>, gpu=False, precision=32, amp_level='O1', scheduler_step_mode='epoch')","title":"Trainer"},{"location":"tutorial/training/#using-a-trainer","text":"A Run instance invokes its trainer by Run.start() . run.start() # create_callbacks() is called automatically. [9] 2020-06-20 15:23:57 ( 523ms ) python3 ( 24.5s ) [epoch#0] loss=17.13 val_loss=5.569 lr=0.001 best [epoch#1] loss=5.436 val_loss=3.977 lr=0.001 best [epoch#2] loss=3.7 val_loss=2.625 lr=0.001 best [epoch#3] loss=2.331 val_loss=1.614 lr=0.001 best [epoch#4] loss=1.427 val_loss=0.905 lr=0.001 best [epoch#5] loss=0.9515 val_loss=0.6364 lr=0.001 best [epoch#6] loss=0.7028 val_loss=0.5842 lr=0.001 best [epoch#7] loss=0.6472 val_loss=0.5076 lr=0.001 best [epoch#8] loss=0.5725 val_loss=0.4287 lr=0.001 best [epoch#9] loss=0.5375 val_loss=0.4063 lr=0.001 best You can update attributes of run's objects at any time. run.trainer.epochs = 5 run.start() [10] 2020-06-20 15:23:57 ( 275ms ) python3 ( 24.8s ) [epoch#10] loss=0.465 val_loss=0.3766 lr=0.001 best [epoch#11] loss=0.4343 val_loss=0.3537 lr=0.001 best [epoch#12] loss=0.4087 val_loss=0.3376 lr=0.001 best [epoch#13] loss=0.4152 val_loss=0.3248 lr=0.001 best [epoch#14] loss=0.3732 val_loss=0.3337 lr=0.001 Note The Run.start() doesn't reset the trainer's epoch.","title":"Using a Trainer"},{"location":"tutorial/training/#callbacks-after-training","text":"After training, the callbacks changes their states. run.metrics # Show metrics at current epoch. [11] 2020-06-20 15:23:58 ( 4.00ms ) python3 ( 24.8s ) Metrics(loss=0.3732, val_loss=0.3337, lr=0.001) run.metrics.history.val_loss # Metrics history. [12] 2020-06-20 15:23:58 ( 4.00ms ) python3 ( 24.8s ) {0: 5.56868793964386, 1: 3.976963722705841, 2: 2.6250638723373414, 3: 1.6140344977378844, 4: 0.9049779623746872, 5: 0.6364109225571155, 6: 0.584185541048646, 7: 0.5075690947473049, 8: 0.42866935580968857, 9: 0.4063351653516293, 10: 0.37661438062787056, 11: 0.35370331779122355, 12: 0.33763624504208567, 13: 0.3248191948980093, 14: 0.333729387819767} run.monitor # Store the best score and its epoch. [13] 2020-06-20 15:23:58 ( 4.00ms ) python3 ( 24.8s ) Monitor(metric='val_loss', mode='min', best_score=0.325, best_epoch=13) run.early_stopping # Current `wait`. [14] 2020-06-20 15:23:58 ( 3.00ms ) python3 ( 24.8s ) EarlyStopping(patience=10, wait=1) run.trainer # Current epoch is 14 (0-indexed). [15] 2020-06-20 15:23:58 ( 3.00ms ) python3 ( 24.8s ) Trainer(epoch=14, epochs=5, global_step=899, batch_size=10, shuffle=True, dataloaders='ivory.torch.data.DataLoaders', verbose=2, loss=<function mse_loss at 0x000001404B1C93A8>, gpu=False, precision=32, amp_level='O1', scheduler_step_mode='epoch')","title":"Callbacks after Training"},{"location":"tutorial/tuning/","text":"Hyperparameter Tuning Suggest Function To optimize a set of hyperparameters, define a suggest function . Here are example functions. File 9 rectangle/suggest.py def suggest_lr(trial, min=1e-5, max=1e-3): trial.suggest_loguniform(\"lr\", min, max) def suggest_hidden_sizes(trial, max_num_layers, min_size=10, max_size=30): num_layers = trial.suggest_int(\"num_layers\", 2, max_num_layers) for k in range(num_layers): trial.suggest_int(f\"hidden_sizes:{k}\", min_size, max_size) A suggest function must take a trial (an instance of Trial ) as the first argument but you can add arbitrary arguments if you need. For more details about what the Trial can do, see the offical Optuna documentation . Note In the suggest_hidden_sizes() , we use 0-indexed colon-notation , because Optuna doesn't suggest a list itself but its element. These suggest functions don't return any parameters. The only work of suggest functions is to make the Trial instance suggest parameters. Suggested parameters are stored in the Trial instance, so that nothing is needed from suggest functions. Note that an objective function in Optuna has only one trial argument, so that we have to use the functools.partial() to make a pure suggest function. from functools import partial from rectangle.suggest import suggest_lr, suggest_hidden_sizes lr = partial(suggest_lr, min=1e-5, max=1e-2) hidden_sizes = partial(suggest_hidden_sizes, max_num_layers=3) [3] 2020-06-20 15:23:58 ( 6.00ms ) python3 ( 24.9s ) Study Ivory implements a special run class Study that controls hyperparameter tuning using Optuna. import ivory client = ivory.create_client(\"examples\") # Set the working directory study_lr = client.create_study('torch', lr=lr) study_hs = client.create_study('torch', hidden_sizes=hidden_sizes) study_lr [4] 2020-06-20 15:23:58 ( 91.0ms ) python3 ( 25.0s ) [I 200620 15:23:58 tracker:48] A new experiment created with name: 'torch' Study(id='48460cd925134cbeb5033cfb2e91ede7', name='study#0', num_instances=5) In the Client.create_study() , you can pass a keyword argument in which the key is a suggest name and the value is a pure suggest function. Objective The ivory.core.objective.Objective class provides objective functions that return a score to minimize or maximize. But you don't need to know about the Objective class in details. Ivory builds an objective function from a suggest function and sends it to Optuna so that Optuna can optimize the parameters. A Study instance has an Objective instance. study_lr.objective [5] 2020-06-20 15:23:58 ( 4.00ms ) python3 ( 25.0s ) Objective(['lr']) study_hs.objective [6] 2020-06-20 15:23:58 ( 4.00ms ) python3 ( 25.0s ) Objective(['hidden_sizes']) Optimization Then \"optimize\" the learning rate and hidden sizes just for fun. optuna_study_lr = study_lr.optimize(n_trials=3, fold=3, epochs=3) [7] 2020-06-20 15:23:58 ( 1.94s ) python3 ( 26.9s ) [I 2020-06-20 15:23:58,368] A new study created with name: torch.lr.study#0 [run#0] lr=2.473e-05 fold=3 epochs=3 [epoch#0] loss=84.9 val_loss=81.53 lr=2.473e-05 best [epoch#1] loss=75.93 val_loss=72.07 lr=2.473e-05 best [epoch#2] loss=65.7 val_loss=61.3 lr=2.473e-05 best [I 2020-06-20 15:23:59,020] Finished trial#0 with value: 61.3017183303833 with parameters: {'lr': 2.4732972291277312e-05}. Best is trial#0 with value: 61.3017183303833. [run#1] lr=0.004571 fold=3 epochs=3 [epoch#0] loss=13.09 val_loss=5.294 lr=0.004571 best [epoch#1] loss=7.122 val_loss=2.763 lr=0.004571 best [epoch#2] loss=5.946 val_loss=1.33 lr=0.004571 best [I 2020-06-20 15:23:59,638] Finished trial#1 with value: 1.3301505535840987 with parameters: {'lr': 0.0045710953332283675}. Best is trial#1 with value: 1.3301505535840987. [run#2] lr=0.0003579 fold=3 epochs=3 [epoch#0] loss=81.91 val_loss=37.03 lr=0.0003579 best [epoch#1] loss=12.24 val_loss=9.582 lr=0.0003579 best [epoch#2] loss=7.788 val_loss=9.338 lr=0.0003579 best [I 2020-06-20 15:24:00,266] Finished trial#2 with value: 9.337713980674744 with parameters: {'lr': 0.00035786096146409664}. Best is trial#1 with value: 1.3301505535840987. optuna_study_hs = study_hs.optimize(n_trials=3, epochs=3) [8] 2020-06-20 15:24:00 ( 2.02s ) python3 ( 29.0s ) [I 2020-06-20 15:24:00,298] A new study created with name: torch.hidden_sizes.study#1 [run#3] hidden_sizes:0=16 hidden_sizes:1=28 hidden_sizes:2=23 num_layers=3 epochs=3 [epoch#0] loss=37.55 val_loss=5.88 lr=0.001 best [epoch#1] loss=6.224 val_loss=5.166 lr=0.001 best [epoch#2] loss=5.125 val_loss=3.951 lr=0.001 best [I 2020-06-20 15:24:00,974] Finished trial#0 with value: 3.9506407141685487 with parameters: {'hidden_sizes:0': 16, 'hidden_sizes:1': 28, 'hidden_sizes:2': 23, 'num_layers': 3}. Best is trial#0 with value: 3.9506407141685487. [run#4] hidden_sizes:0=24 hidden_sizes:1=24 num_layers=2 epochs=3 [epoch#0] loss=23.84 val_loss=6.409 lr=0.001 best [epoch#1] loss=6.523 val_loss=5.263 lr=0.001 best [epoch#2] loss=5.656 val_loss=4.537 lr=0.001 best [I 2020-06-20 15:24:01,622] Finished trial#1 with value: 4.536559498310089 with parameters: {'hidden_sizes:0': 24, 'hidden_sizes:1': 24, 'num_layers': 2}. Best is trial#0 with value: 3.9506407141685487. [run#5] hidden_sizes:0=22 hidden_sizes:1=25 hidden_sizes:2=27 num_layers=3 epochs=3 [epoch#0] loss=26.73 val_loss=8.15 lr=0.001 best [epoch#1] loss=7.776 val_loss=6.177 lr=0.001 best [epoch#2] loss=6.573 val_loss=5.906 lr=0.001 best [I 2020-06-20 15:24:02,293] Finished trial#2 with value: 5.905864572525024 with parameters: {'hidden_sizes:0': 22, 'hidden_sizes:1': 25, 'hidden_sizes:2': 27, 'num_layers': 3}. Best is trial#0 with value: 3.9506407141685487. Note By cliking an icon ( ) in the above cells, you can see the Optuna's log. The returned value of the Study.optimize() is an Optuna's Study instance (not Ivory's one). optuna_study_lr [9] 2020-06-20 15:24:02 ( 4.00ms ) python3 ( 29.0s ) <optuna.study.Study at 0x1420ecf1548> The Study instance is named after the experiment name, suggest name, and run name. optuna_study_lr.study_name [10] 2020-06-20 15:24:02 ( 4.00ms ) python3 ( 29.0s ) 'torch.lr.study#0' In user attributes that Optuna's Study and Trial instances provide, Run ID is saved. optuna_study_lr.user_attrs [11] 2020-06-20 15:24:02 ( 6.00ms ) python3 ( 29.0s ) {'run_id': '48460cd925134cbeb5033cfb2e91ede7'} optuna_study_lr.trials[0].user_attrs [12] 2020-06-20 15:24:02 ( 6.00ms ) python3 ( 29.0s ) {'run_id': '123700db98b144bb8f0f8a5a59b502a9'} On the other hand, MLFlow Tracking's run (not Ivory's one) has a tag to refer Optuna's study and trial. mlflow_client = client.tracker.client mlflow_client [13] 2020-06-20 15:24:02 ( 4.00ms ) python3 ( 29.0s ) <mlflow.tracking.client.MlflowClient at 0x1420144b988> run_id = optuna_study_lr.user_attrs['run_id'] run = mlflow_client.get_run(run_id) run.data.tags['study_name'] [14] 2020-06-20 15:24:02 ( 8.00ms ) python3 ( 29.0s ) 'torch.lr.study#0' run_id = optuna_study_lr.trials[0].user_attrs['run_id'] run = mlflow_client.get_run(run_id) run.data.tags['trial_number'] [15] 2020-06-20 15:24:02 ( 10.0ms ) python3 ( 29.0s ) '0' You may have a question. How does Optuna optimize the parameters without any score? The answer is the Monitor instance. An Objective instance gets the monitoring score from run.monitor and sends it to Optuna so that Optuna can determine the next suggestion. All you need is to make your Run instance have a Monitor instance. Check the YAML parameter file: File 10 torch.yml library: torch datasets: data: class: rectangle.data.Data n_splits: 4 dataset: fold: 0 model: class: rectangle.torch.Model hidden_sizes: [20, 30] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 results: metrics: monitor: metric: val_loss early_stopping: patience: 10 trainer: loss: mse batch_size: 10 epochs: 10 shuffle: true verbose: 2 The Monitor instance monitors val_loss (actually this is the default value, so that you can delete this line) and the default mode is min (smaller is better). If your monitor is accuracy, for example, set the monitor like this: monitor: metric: accuracy mode: max Parametric Optimization Again read the suggest functions. File 11 rectangle/suggest.py def suggest_lr(trial, min=1e-5, max=1e-3): trial.suggest_loguniform(\"lr\", min, max) def suggest_hidden_sizes(trial, max_num_layers, min_size=10, max_size=30): num_layers = trial.suggest_int(\"num_layers\", 2, max_num_layers) for k in range(num_layers): trial.suggest_int(f\"hidden_sizes:{k}\", min_size, max_size) The suggest_hidden_sizes() has some logic but the code of the suggest_lr() is too simple to define a function. You may not want to write such a function. Ivory can do that for you. You can pass key-iterable pairs to the client.create_study() instead of key-callable pairs. tuple, range, Range A tuple, range, or Range instance represents parameter range. study = client.create_study('torch', lr=(1e-3, 1e-2)) _ = study.optimize(n_trials=5, epochs=1, verbose=0) [16] 2020-06-20 15:24:02 ( 2.18s ) python3 ( 31.2s ) [I 2020-06-20 15:24:02,434] A new study created with name: torch.lr.study#2 [run#6] lr=0.004785 epochs=1 [I 2020-06-20 15:24:02,870] Finished trial#0 with value: 7.505557787418366 with parameters: {'lr': 0.0047846720934690504}. Best is trial#0 with value: 7.505557787418366. [run#7] lr=0.007948 epochs=1 [I 2020-06-20 15:24:03,276] Finished trial#1 with value: 16.9930198431015 with parameters: {'lr': 0.007948219494249426}. Best is trial#0 with value: 7.505557787418366. [run#8] lr=0.008386 epochs=1 [I 2020-06-20 15:24:03,701] Finished trial#2 with value: 21.089563941955568 with parameters: {'lr': 0.008385933151314614}. Best is trial#0 with value: 7.505557787418366. [run#9] lr=0.009467 epochs=1 [I 2020-06-20 15:24:04,116] Finished trial#3 with value: 24.760809683799742 with parameters: {'lr': 0.009466755734626534}. Best is trial#0 with value: 7.505557787418366. [run#10] lr=0.006797 epochs=1 [I 2020-06-20 15:24:04,534] Finished trial#4 with value: 63.910634422302245 with parameters: {'lr': 0.006797194491767582}. Best is trial#0 with value: 7.505557787418366. In the above cell, lr=Range(1e-3, 1e-2) also works. For integer parameters, you can use normal range as well as tuple or Range . params = {'hidden_sizes.0': range(10, 21)} # Range(10, 20), or (10, 20) study = client.create_study('torch', params) _ = study.optimize(n_trials=5, epochs=1, verbose=0) [17] 2020-06-20 15:24:04 ( 2.24s ) python3 ( 33.4s ) [I 2020-06-20 15:24:04,641] A new study created with name: torch.hidden_sizes.0.study#3 [run#11] hidden_sizes.0=10 epochs=1 [I 2020-06-20 15:24:05,062] Finished trial#0 with value: 7.2038188695907595 with parameters: {'hidden_sizes.0': 10}. Best is trial#0 with value: 7.2038188695907595. [run#12] hidden_sizes.0=19 epochs=1 [I 2020-06-20 15:24:05,489] Finished trial#1 with value: 7.636005270481109 with parameters: {'hidden_sizes.0': 19}. Best is trial#0 with value: 7.2038188695907595. [run#13] hidden_sizes.0=11 epochs=1 [I 2020-06-20 15:24:05,913] Finished trial#2 with value: 7.774815452098847 with parameters: {'hidden_sizes.0': 11}. Best is trial#0 with value: 7.2038188695907595. [run#14] hidden_sizes.0=19 epochs=1 [I 2020-06-20 15:24:06,341] Finished trial#3 with value: 6.721940863132477 with parameters: {'hidden_sizes.0': 19}. Best is trial#3 with value: 6.721940863132477. [run#15] hidden_sizes.0=11 epochs=1 [I 2020-06-20 15:24:06,777] Finished trial#4 with value: 7.97547961473465 with parameters: {'hidden_sizes.0': 11}. Best is trial#3 with value: 6.721940863132477. You can specify a step params = {'hidden_sizes.0': range(10, 21, 3)} study = client.create_study('torch', params) _ = study.optimize(n_trials=5, epochs=1, verbose=0) [18] 2020-06-20 15:24:06 ( 2.34s ) python3 ( 35.8s ) [I 2020-06-20 15:24:06,903] A new study created with name: torch.hidden_sizes.0.study#4 [run#16] hidden_sizes.0=19 epochs=1 [I 2020-06-20 15:24:07,351] Finished trial#0 with value: 6.568264007568359 with parameters: {'hidden_sizes.0': 19}. Best is trial#0 with value: 6.568264007568359. [run#17] hidden_sizes.0=10 epochs=1 [I 2020-06-20 15:24:07,789] Finished trial#1 with value: 8.529261600971221 with parameters: {'hidden_sizes.0': 10}. Best is trial#0 with value: 6.568264007568359. [run#18] hidden_sizes.0=16 epochs=1 [I 2020-06-20 15:24:08,230] Finished trial#2 with value: 7.528908741474152 with parameters: {'hidden_sizes.0': 16}. Best is trial#0 with value: 6.568264007568359. [run#19] hidden_sizes.0=19 epochs=1 [I 2020-06-20 15:24:08,667] Finished trial#3 with value: 7.347911989688873 with parameters: {'hidden_sizes.0': 19}. Best is trial#0 with value: 6.568264007568359. [run#20] hidden_sizes.0=10 epochs=1 [I 2020-06-20 15:24:09,116] Finished trial#4 with value: 6.824846786260605 with parameters: {'hidden_sizes.0': 10}. Best is trial#0 with value: 6.568264007568359. If you need sampling in log scale, use Range with log=True . from ivory.utils.range import Range study = client.create_study('torch', lr=Range(1e-3, 1e-2, log=True)) _ = study.optimize(n_trials=5, epochs=1, verbose=0) [19] 2020-06-20 15:24:09 ( 2.45s ) python3 ( 38.2s ) [I 2020-06-20 15:24:09,246] A new study created with name: torch.lr.study#5 [run#21] lr=0.006629 epochs=1 [I 2020-06-20 15:24:09,725] Finished trial#0 with value: 23.470214128494263 with parameters: {'lr': 0.0066287900160612}. Best is trial#0 with value: 23.470214128494263. [run#22] lr=0.003144 epochs=1 [I 2020-06-20 15:24:10,181] Finished trial#1 with value: 6.392648601531983 with parameters: {'lr': 0.0031441144268029536}. Best is trial#1 with value: 6.392648601531983. [run#23] lr=0.005022 epochs=1 [I 2020-06-20 15:24:10,651] Finished trial#2 with value: 4.59294992685318 with parameters: {'lr': 0.005022326394376906}. Best is trial#2 with value: 4.59294992685318. [run#24] lr=0.003322 epochs=1 [I 2020-06-20 15:24:11,107] Finished trial#3 with value: 5.0058059871196745 with parameters: {'lr': 0.00332243814575827}. Best is trial#2 with value: 4.59294992685318. [run#25] lr=0.001459 epochs=1 [I 2020-06-20 15:24:11,567] Finished trial#4 with value: 5.597119307518005 with parameters: {'lr': 0.0014591650124987697}. Best is trial#2 with value: 4.59294992685318. list A list represents parameter choice. params = {'hidden_sizes.0': [10, 20, 30]} study = client.create_study('torch', params) _ = study.optimize(n_trials=5, epochs=1, verbose=0) [20] 2020-06-20 15:24:11 ( 2.54s ) python3 ( 40.7s ) [I 2020-06-20 15:24:11,709] A new study created with name: torch.hidden_sizes.0.study#6 [run#26] hidden_sizes.0=30 epochs=1 [I 2020-06-20 15:24:12,212] Finished trial#0 with value: 6.970041018724442 with parameters: {'hidden_sizes.0': 30}. Best is trial#0 with value: 6.970041018724442. [run#27] hidden_sizes.0=20 epochs=1 [I 2020-06-20 15:24:12,679] Finished trial#1 with value: 6.162596261501312 with parameters: {'hidden_sizes.0': 20}. Best is trial#1 with value: 6.162596261501312. [run#28] hidden_sizes.0=30 epochs=1 [I 2020-06-20 15:24:13,161] Finished trial#2 with value: 7.544045042991638 with parameters: {'hidden_sizes.0': 30}. Best is trial#1 with value: 6.162596261501312. [run#29] hidden_sizes.0=30 epochs=1 [I 2020-06-20 15:24:13,631] Finished trial#3 with value: 8.475840830802918 with parameters: {'hidden_sizes.0': 30}. Best is trial#1 with value: 6.162596261501312. [run#30] hidden_sizes.0=30 epochs=1 [I 2020-06-20 15:24:14,105] Finished trial#4 with value: 6.80062427520752 with parameters: {'hidden_sizes.0': 30}. Best is trial#1 with value: 6.162596261501312. Product If a key and value are tuples, the entry means cartesian product of suggest functions like Task.product() . params = {('hidden_sizes', 'lr'): (hidden_sizes, Range(1e-4, 1e-3))} study = client.create_study('torch', params) optuna_study = study.optimize(n_trials=10, epochs=1, verbose=0) [21] 2020-06-20 15:24:14 ( 5.46s ) python3 ( 46.2s ) [I 2020-06-20 15:24:14,276] A new study created with name: torch.hidden_sizes.lr.study#7 [run#31] hidden_sizes:0=23 hidden_sizes:1=26 hidden_sizes:2=30 lr=0.000285 num_layers=3 epochs=1 [I 2020-06-20 15:24:14,829] Finished trial#0 with value: 73.63323173522949 with parameters: {'hidden_sizes:0': 23, 'hidden_sizes:1': 26, 'hidden_sizes:2': 30, 'lr': 0.00028500875002022773, 'num_layers': 3}. Best is trial#0 with value: 73.63323173522949. [run#32] hidden_sizes:0=15 hidden_sizes:1=23 lr=0.0003589 num_layers=2 epochs=1 [I 2020-06-20 15:24:15,349] Finished trial#1 with value: 8.391229891777039 with parameters: {'hidden_sizes:0': 15, 'hidden_sizes:1': 23, 'lr': 0.00035887166285874963, 'num_layers': 2}. Best is trial#1 with value: 8.391229891777039. [run#33] hidden_sizes:0=10 hidden_sizes:1=16 lr=0.0009096 num_layers=2 epochs=1 [I 2020-06-20 15:24:15,838] Finished trial#2 with value: 6.040180587768555 with parameters: {'hidden_sizes:0': 10, 'hidden_sizes:1': 16, 'lr': 0.0009095927923320399, 'num_layers': 2}. Best is trial#2 with value: 6.040180587768555. [run#34] hidden_sizes:0=16 hidden_sizes:1=27 lr=0.0003067 num_layers=2 epochs=1 [I 2020-06-20 15:24:16,357] Finished trial#3 with value: 9.55086771249771 with parameters: {'hidden_sizes:0': 16, 'hidden_sizes:1': 27, 'lr': 0.0003067305802012419, 'num_layers': 2}. Best is trial#2 with value: 6.040180587768555. [run#35] hidden_sizes:0=21 hidden_sizes:1=13 lr=0.0005063 num_layers=2 epochs=1 [I 2020-06-20 15:24:16,879] Finished trial#4 with value: 7.796209609508514 with parameters: {'hidden_sizes:0': 21, 'hidden_sizes:1': 13, 'lr': 0.0005063167017779712, 'num_layers': 2}. Best is trial#2 with value: 6.040180587768555. [run#36] hidden_sizes:0=13 hidden_sizes:1=14 hidden_sizes:2=11 lr=0.0007649 num_layers=3 epochs=1 [I 2020-06-20 15:24:17,384] Finished trial#5 with value: 6.721876096725464 with parameters: {'hidden_sizes:0': 13, 'hidden_sizes:1': 14, 'hidden_sizes:2': 11, 'lr': 0.0007648866552945914, 'num_layers': 3}. Best is trial#2 with value: 6.040180587768555. [run#37] hidden_sizes:0=12 hidden_sizes:1=28 lr=0.0003215 num_layers=2 epochs=1 [I 2020-06-20 15:24:17,921] Finished trial#6 with value: 10.44865968823433 with parameters: {'hidden_sizes:0': 12, 'hidden_sizes:1': 28, 'lr': 0.00032149466296796944, 'num_layers': 2}. Best is trial#2 with value: 6.040180587768555. [run#38] hidden_sizes:0=23 hidden_sizes:1=20 lr=0.0006336 num_layers=2 epochs=1 [I 2020-06-20 15:24:18,457] Finished trial#7 with value: 7.703984320163727 with parameters: {'hidden_sizes:0': 23, 'hidden_sizes:1': 20, 'lr': 0.0006336399565950085, 'num_layers': 2}. Best is trial#2 with value: 6.040180587768555. [run#39] hidden_sizes:0=17 hidden_sizes:1=27 hidden_sizes:2=22 lr=0.0006164 num_layers=3 epochs=1 [I 2020-06-20 15:24:19,014] Finished trial#8 with value: 9.060218453407288 with parameters: {'hidden_sizes:0': 17, 'hidden_sizes:1': 27, 'hidden_sizes:2': 22, 'lr': 0.0006163710231144914, 'num_layers': 3}. Best is trial#2 with value: 6.040180587768555. [run#40] hidden_sizes:0=15 hidden_sizes:1=26 lr=0.0001284 num_layers=2 epochs=1 [I 2020-06-20 15:24:19,563] Finished trial#9 with value: 59.055799674987796 with parameters: {'hidden_sizes:0': 15, 'hidden_sizes:1': 26, 'lr': 0.0001283989499444509, 'num_layers': 2}. Best is trial#2 with value: 6.040180587768555. Note You can mix suggest funtions and parametric optimization. Note You may feel that \" params = {'hidden_sizes.1': hidden_sizes, 'lr': Range(1e-4, 1e-3)} \" is better, but the above style is intentional. In parametric optimization, the name of Optuna's Study instance is dot-joint style : optuna_study.study_name [22] 2020-06-20 15:24:19 ( 4.00ms ) python3 ( 46.2s ) 'torch.hidden_sizes.lr.study#7' Study from YAML file As a normal Run , a Study instance also can be created from a YAML file. Pass an extra keyword argument to the client.create_experiment() . The key is the instance name (in this case study ) and value is a YAML file name without its extension. experiment = client.create_experiment('torch', study='study') experiment [23] 2020-06-20 15:24:19 ( 11.0ms ) python3 ( 46.2s ) Experiment(id='1', name='torch', num_instances=1) Here is the contents of study.yml file. File 12 study.yml objective: lr: rectangle.suggest.suggest_lr hidden_sizes: def: rectangle.suggest.suggest_hidden_sizes max_num_layers: 3 min_size: __default__ max_size: __default__ Suggest functions should be callable, hidden_sizes uses a def key to create a callable. On the other hand, lr is just one line. If a suggest funtion can be called without additional arguments, you can omit the def key. Using this experiment, we can create Study instances with a suggest function. study_lr = client.create_study('torch', 'lr') study_lr.objective [24] 2020-06-20 15:24:19 ( 174ms ) python3 ( 46.4s ) Objective(['lr']) study_hs = client.create_study('torch', 'hidden_sizes') study_hs.objective [25] 2020-06-20 15:24:19 ( 178ms ) python3 ( 46.6s ) Objective(['hidden_sizes']) study_hs.objective.hidden_sizes [26] 2020-06-20 15:24:19 ( 3.00ms ) python3 ( 46.6s ) functools.partial(<function suggest_hidden_sizes at 0x000001420C0508B8>, max_num_layers=3, min_size=10, max_size=30) For min_size and max_size , default values are inspected from the signature. study_lr.optimize(n_trials=3, epochs=3, verbose=0) [27] 2020-06-20 15:24:19 ( 2.32s ) python3 ( 48.9s ) [I 2020-06-20 15:24:19,976] A new study created with name: torch.lr.study#8 [run#41] lr=8.41e-05 epochs=3 [I 2020-06-20 15:24:20,729] Finished trial#0 with value: 9.582427203655243 with parameters: {'lr': 8.409509940148645e-05}. Best is trial#0 with value: 9.582427203655243. [run#42] lr=3.765e-05 epochs=3 [I 2020-06-20 15:24:21,499] Finished trial#1 with value: 73.85470199584961 with parameters: {'lr': 3.7651669961268316e-05}. Best is trial#0 with value: 9.582427203655243. [run#43] lr=1.924e-05 epochs=3 [I 2020-06-20 15:24:22,268] Finished trial#2 with value: 93.58401222229004 with parameters: {'lr': 1.924316867807553e-05}. Best is trial#0 with value: 9.582427203655243. <optuna.study.Study at 0x14212124e48> Pruning Optuna provides the pruning functionality . Ivory can uses this feature seamlessly. Here is updated contents of study.yml file. File 13 study.yml tuner: pruner: class: optuna.pruners.MedianPruner objective: lr: rectangle.suggest.suggest_lr hidden_sizes: def: rectangle.suggest.suggest_hidden_sizes max_num_layers: 3 min_size: __default__ max_size: __default__ The Tuner instance has Optuna's MedianPruner . (Off course, you can use other pruners .) A Study instance give an ivory.callbacks.Pruning instance to a run when the run is created, then with Ivory's callback system , the Pruning instance communicates with Optuna in order to determine the step of pruning. Note Pruning is supported for PyTorch and TensorFlow now.","title":"Hyperparameter Tuning"},{"location":"tutorial/tuning/#hyperparameter-tuning","text":"","title":"Hyperparameter Tuning"},{"location":"tutorial/tuning/#suggest-function","text":"To optimize a set of hyperparameters, define a suggest function . Here are example functions. File 9 rectangle/suggest.py def suggest_lr(trial, min=1e-5, max=1e-3): trial.suggest_loguniform(\"lr\", min, max) def suggest_hidden_sizes(trial, max_num_layers, min_size=10, max_size=30): num_layers = trial.suggest_int(\"num_layers\", 2, max_num_layers) for k in range(num_layers): trial.suggest_int(f\"hidden_sizes:{k}\", min_size, max_size) A suggest function must take a trial (an instance of Trial ) as the first argument but you can add arbitrary arguments if you need. For more details about what the Trial can do, see the offical Optuna documentation . Note In the suggest_hidden_sizes() , we use 0-indexed colon-notation , because Optuna doesn't suggest a list itself but its element. These suggest functions don't return any parameters. The only work of suggest functions is to make the Trial instance suggest parameters. Suggested parameters are stored in the Trial instance, so that nothing is needed from suggest functions. Note that an objective function in Optuna has only one trial argument, so that we have to use the functools.partial() to make a pure suggest function. from functools import partial from rectangle.suggest import suggest_lr, suggest_hidden_sizes lr = partial(suggest_lr, min=1e-5, max=1e-2) hidden_sizes = partial(suggest_hidden_sizes, max_num_layers=3) [3] 2020-06-20 15:23:58 ( 6.00ms ) python3 ( 24.9s )","title":"Suggest Function"},{"location":"tutorial/tuning/#study","text":"Ivory implements a special run class Study that controls hyperparameter tuning using Optuna. import ivory client = ivory.create_client(\"examples\") # Set the working directory study_lr = client.create_study('torch', lr=lr) study_hs = client.create_study('torch', hidden_sizes=hidden_sizes) study_lr [4] 2020-06-20 15:23:58 ( 91.0ms ) python3 ( 25.0s ) [I 200620 15:23:58 tracker:48] A new experiment created with name: 'torch' Study(id='48460cd925134cbeb5033cfb2e91ede7', name='study#0', num_instances=5) In the Client.create_study() , you can pass a keyword argument in which the key is a suggest name and the value is a pure suggest function.","title":"Study"},{"location":"tutorial/tuning/#objective","text":"The ivory.core.objective.Objective class provides objective functions that return a score to minimize or maximize. But you don't need to know about the Objective class in details. Ivory builds an objective function from a suggest function and sends it to Optuna so that Optuna can optimize the parameters. A Study instance has an Objective instance. study_lr.objective [5] 2020-06-20 15:23:58 ( 4.00ms ) python3 ( 25.0s ) Objective(['lr']) study_hs.objective [6] 2020-06-20 15:23:58 ( 4.00ms ) python3 ( 25.0s ) Objective(['hidden_sizes'])","title":"Objective"},{"location":"tutorial/tuning/#optimization","text":"Then \"optimize\" the learning rate and hidden sizes just for fun. optuna_study_lr = study_lr.optimize(n_trials=3, fold=3, epochs=3) [7] 2020-06-20 15:23:58 ( 1.94s ) python3 ( 26.9s ) [I 2020-06-20 15:23:58,368] A new study created with name: torch.lr.study#0 [run#0] lr=2.473e-05 fold=3 epochs=3 [epoch#0] loss=84.9 val_loss=81.53 lr=2.473e-05 best [epoch#1] loss=75.93 val_loss=72.07 lr=2.473e-05 best [epoch#2] loss=65.7 val_loss=61.3 lr=2.473e-05 best [I 2020-06-20 15:23:59,020] Finished trial#0 with value: 61.3017183303833 with parameters: {'lr': 2.4732972291277312e-05}. Best is trial#0 with value: 61.3017183303833. [run#1] lr=0.004571 fold=3 epochs=3 [epoch#0] loss=13.09 val_loss=5.294 lr=0.004571 best [epoch#1] loss=7.122 val_loss=2.763 lr=0.004571 best [epoch#2] loss=5.946 val_loss=1.33 lr=0.004571 best [I 2020-06-20 15:23:59,638] Finished trial#1 with value: 1.3301505535840987 with parameters: {'lr': 0.0045710953332283675}. Best is trial#1 with value: 1.3301505535840987. [run#2] lr=0.0003579 fold=3 epochs=3 [epoch#0] loss=81.91 val_loss=37.03 lr=0.0003579 best [epoch#1] loss=12.24 val_loss=9.582 lr=0.0003579 best [epoch#2] loss=7.788 val_loss=9.338 lr=0.0003579 best [I 2020-06-20 15:24:00,266] Finished trial#2 with value: 9.337713980674744 with parameters: {'lr': 0.00035786096146409664}. Best is trial#1 with value: 1.3301505535840987. optuna_study_hs = study_hs.optimize(n_trials=3, epochs=3) [8] 2020-06-20 15:24:00 ( 2.02s ) python3 ( 29.0s ) [I 2020-06-20 15:24:00,298] A new study created with name: torch.hidden_sizes.study#1 [run#3] hidden_sizes:0=16 hidden_sizes:1=28 hidden_sizes:2=23 num_layers=3 epochs=3 [epoch#0] loss=37.55 val_loss=5.88 lr=0.001 best [epoch#1] loss=6.224 val_loss=5.166 lr=0.001 best [epoch#2] loss=5.125 val_loss=3.951 lr=0.001 best [I 2020-06-20 15:24:00,974] Finished trial#0 with value: 3.9506407141685487 with parameters: {'hidden_sizes:0': 16, 'hidden_sizes:1': 28, 'hidden_sizes:2': 23, 'num_layers': 3}. Best is trial#0 with value: 3.9506407141685487. [run#4] hidden_sizes:0=24 hidden_sizes:1=24 num_layers=2 epochs=3 [epoch#0] loss=23.84 val_loss=6.409 lr=0.001 best [epoch#1] loss=6.523 val_loss=5.263 lr=0.001 best [epoch#2] loss=5.656 val_loss=4.537 lr=0.001 best [I 2020-06-20 15:24:01,622] Finished trial#1 with value: 4.536559498310089 with parameters: {'hidden_sizes:0': 24, 'hidden_sizes:1': 24, 'num_layers': 2}. Best is trial#0 with value: 3.9506407141685487. [run#5] hidden_sizes:0=22 hidden_sizes:1=25 hidden_sizes:2=27 num_layers=3 epochs=3 [epoch#0] loss=26.73 val_loss=8.15 lr=0.001 best [epoch#1] loss=7.776 val_loss=6.177 lr=0.001 best [epoch#2] loss=6.573 val_loss=5.906 lr=0.001 best [I 2020-06-20 15:24:02,293] Finished trial#2 with value: 5.905864572525024 with parameters: {'hidden_sizes:0': 22, 'hidden_sizes:1': 25, 'hidden_sizes:2': 27, 'num_layers': 3}. Best is trial#0 with value: 3.9506407141685487. Note By cliking an icon ( ) in the above cells, you can see the Optuna's log. The returned value of the Study.optimize() is an Optuna's Study instance (not Ivory's one). optuna_study_lr [9] 2020-06-20 15:24:02 ( 4.00ms ) python3 ( 29.0s ) <optuna.study.Study at 0x1420ecf1548> The Study instance is named after the experiment name, suggest name, and run name. optuna_study_lr.study_name [10] 2020-06-20 15:24:02 ( 4.00ms ) python3 ( 29.0s ) 'torch.lr.study#0' In user attributes that Optuna's Study and Trial instances provide, Run ID is saved. optuna_study_lr.user_attrs [11] 2020-06-20 15:24:02 ( 6.00ms ) python3 ( 29.0s ) {'run_id': '48460cd925134cbeb5033cfb2e91ede7'} optuna_study_lr.trials[0].user_attrs [12] 2020-06-20 15:24:02 ( 6.00ms ) python3 ( 29.0s ) {'run_id': '123700db98b144bb8f0f8a5a59b502a9'} On the other hand, MLFlow Tracking's run (not Ivory's one) has a tag to refer Optuna's study and trial. mlflow_client = client.tracker.client mlflow_client [13] 2020-06-20 15:24:02 ( 4.00ms ) python3 ( 29.0s ) <mlflow.tracking.client.MlflowClient at 0x1420144b988> run_id = optuna_study_lr.user_attrs['run_id'] run = mlflow_client.get_run(run_id) run.data.tags['study_name'] [14] 2020-06-20 15:24:02 ( 8.00ms ) python3 ( 29.0s ) 'torch.lr.study#0' run_id = optuna_study_lr.trials[0].user_attrs['run_id'] run = mlflow_client.get_run(run_id) run.data.tags['trial_number'] [15] 2020-06-20 15:24:02 ( 10.0ms ) python3 ( 29.0s ) '0' You may have a question. How does Optuna optimize the parameters without any score? The answer is the Monitor instance. An Objective instance gets the monitoring score from run.monitor and sends it to Optuna so that Optuna can determine the next suggestion. All you need is to make your Run instance have a Monitor instance. Check the YAML parameter file: File 10 torch.yml library: torch datasets: data: class: rectangle.data.Data n_splits: 4 dataset: fold: 0 model: class: rectangle.torch.Model hidden_sizes: [20, 30] optimizer: class: torch.optim.SGD params: $.model.parameters() lr: 1e-3 scheduler: class: torch.optim.lr_scheduler.ReduceLROnPlateau optimizer: $ factor: 0.5 patience: 4 results: metrics: monitor: metric: val_loss early_stopping: patience: 10 trainer: loss: mse batch_size: 10 epochs: 10 shuffle: true verbose: 2 The Monitor instance monitors val_loss (actually this is the default value, so that you can delete this line) and the default mode is min (smaller is better). If your monitor is accuracy, for example, set the monitor like this: monitor: metric: accuracy mode: max","title":"Optimization"},{"location":"tutorial/tuning/#parametric-optimization","text":"Again read the suggest functions. File 11 rectangle/suggest.py def suggest_lr(trial, min=1e-5, max=1e-3): trial.suggest_loguniform(\"lr\", min, max) def suggest_hidden_sizes(trial, max_num_layers, min_size=10, max_size=30): num_layers = trial.suggest_int(\"num_layers\", 2, max_num_layers) for k in range(num_layers): trial.suggest_int(f\"hidden_sizes:{k}\", min_size, max_size) The suggest_hidden_sizes() has some logic but the code of the suggest_lr() is too simple to define a function. You may not want to write such a function. Ivory can do that for you. You can pass key-iterable pairs to the client.create_study() instead of key-callable pairs.","title":"Parametric Optimization"},{"location":"tutorial/tuning/#tuple-range-range","text":"A tuple, range, or Range instance represents parameter range. study = client.create_study('torch', lr=(1e-3, 1e-2)) _ = study.optimize(n_trials=5, epochs=1, verbose=0) [16] 2020-06-20 15:24:02 ( 2.18s ) python3 ( 31.2s ) [I 2020-06-20 15:24:02,434] A new study created with name: torch.lr.study#2 [run#6] lr=0.004785 epochs=1 [I 2020-06-20 15:24:02,870] Finished trial#0 with value: 7.505557787418366 with parameters: {'lr': 0.0047846720934690504}. Best is trial#0 with value: 7.505557787418366. [run#7] lr=0.007948 epochs=1 [I 2020-06-20 15:24:03,276] Finished trial#1 with value: 16.9930198431015 with parameters: {'lr': 0.007948219494249426}. Best is trial#0 with value: 7.505557787418366. [run#8] lr=0.008386 epochs=1 [I 2020-06-20 15:24:03,701] Finished trial#2 with value: 21.089563941955568 with parameters: {'lr': 0.008385933151314614}. Best is trial#0 with value: 7.505557787418366. [run#9] lr=0.009467 epochs=1 [I 2020-06-20 15:24:04,116] Finished trial#3 with value: 24.760809683799742 with parameters: {'lr': 0.009466755734626534}. Best is trial#0 with value: 7.505557787418366. [run#10] lr=0.006797 epochs=1 [I 2020-06-20 15:24:04,534] Finished trial#4 with value: 63.910634422302245 with parameters: {'lr': 0.006797194491767582}. Best is trial#0 with value: 7.505557787418366. In the above cell, lr=Range(1e-3, 1e-2) also works. For integer parameters, you can use normal range as well as tuple or Range . params = {'hidden_sizes.0': range(10, 21)} # Range(10, 20), or (10, 20) study = client.create_study('torch', params) _ = study.optimize(n_trials=5, epochs=1, verbose=0) [17] 2020-06-20 15:24:04 ( 2.24s ) python3 ( 33.4s ) [I 2020-06-20 15:24:04,641] A new study created with name: torch.hidden_sizes.0.study#3 [run#11] hidden_sizes.0=10 epochs=1 [I 2020-06-20 15:24:05,062] Finished trial#0 with value: 7.2038188695907595 with parameters: {'hidden_sizes.0': 10}. Best is trial#0 with value: 7.2038188695907595. [run#12] hidden_sizes.0=19 epochs=1 [I 2020-06-20 15:24:05,489] Finished trial#1 with value: 7.636005270481109 with parameters: {'hidden_sizes.0': 19}. Best is trial#0 with value: 7.2038188695907595. [run#13] hidden_sizes.0=11 epochs=1 [I 2020-06-20 15:24:05,913] Finished trial#2 with value: 7.774815452098847 with parameters: {'hidden_sizes.0': 11}. Best is trial#0 with value: 7.2038188695907595. [run#14] hidden_sizes.0=19 epochs=1 [I 2020-06-20 15:24:06,341] Finished trial#3 with value: 6.721940863132477 with parameters: {'hidden_sizes.0': 19}. Best is trial#3 with value: 6.721940863132477. [run#15] hidden_sizes.0=11 epochs=1 [I 2020-06-20 15:24:06,777] Finished trial#4 with value: 7.97547961473465 with parameters: {'hidden_sizes.0': 11}. Best is trial#3 with value: 6.721940863132477. You can specify a step params = {'hidden_sizes.0': range(10, 21, 3)} study = client.create_study('torch', params) _ = study.optimize(n_trials=5, epochs=1, verbose=0) [18] 2020-06-20 15:24:06 ( 2.34s ) python3 ( 35.8s ) [I 2020-06-20 15:24:06,903] A new study created with name: torch.hidden_sizes.0.study#4 [run#16] hidden_sizes.0=19 epochs=1 [I 2020-06-20 15:24:07,351] Finished trial#0 with value: 6.568264007568359 with parameters: {'hidden_sizes.0': 19}. Best is trial#0 with value: 6.568264007568359. [run#17] hidden_sizes.0=10 epochs=1 [I 2020-06-20 15:24:07,789] Finished trial#1 with value: 8.529261600971221 with parameters: {'hidden_sizes.0': 10}. Best is trial#0 with value: 6.568264007568359. [run#18] hidden_sizes.0=16 epochs=1 [I 2020-06-20 15:24:08,230] Finished trial#2 with value: 7.528908741474152 with parameters: {'hidden_sizes.0': 16}. Best is trial#0 with value: 6.568264007568359. [run#19] hidden_sizes.0=19 epochs=1 [I 2020-06-20 15:24:08,667] Finished trial#3 with value: 7.347911989688873 with parameters: {'hidden_sizes.0': 19}. Best is trial#0 with value: 6.568264007568359. [run#20] hidden_sizes.0=10 epochs=1 [I 2020-06-20 15:24:09,116] Finished trial#4 with value: 6.824846786260605 with parameters: {'hidden_sizes.0': 10}. Best is trial#0 with value: 6.568264007568359. If you need sampling in log scale, use Range with log=True . from ivory.utils.range import Range study = client.create_study('torch', lr=Range(1e-3, 1e-2, log=True)) _ = study.optimize(n_trials=5, epochs=1, verbose=0) [19] 2020-06-20 15:24:09 ( 2.45s ) python3 ( 38.2s ) [I 2020-06-20 15:24:09,246] A new study created with name: torch.lr.study#5 [run#21] lr=0.006629 epochs=1 [I 2020-06-20 15:24:09,725] Finished trial#0 with value: 23.470214128494263 with parameters: {'lr': 0.0066287900160612}. Best is trial#0 with value: 23.470214128494263. [run#22] lr=0.003144 epochs=1 [I 2020-06-20 15:24:10,181] Finished trial#1 with value: 6.392648601531983 with parameters: {'lr': 0.0031441144268029536}. Best is trial#1 with value: 6.392648601531983. [run#23] lr=0.005022 epochs=1 [I 2020-06-20 15:24:10,651] Finished trial#2 with value: 4.59294992685318 with parameters: {'lr': 0.005022326394376906}. Best is trial#2 with value: 4.59294992685318. [run#24] lr=0.003322 epochs=1 [I 2020-06-20 15:24:11,107] Finished trial#3 with value: 5.0058059871196745 with parameters: {'lr': 0.00332243814575827}. Best is trial#2 with value: 4.59294992685318. [run#25] lr=0.001459 epochs=1 [I 2020-06-20 15:24:11,567] Finished trial#4 with value: 5.597119307518005 with parameters: {'lr': 0.0014591650124987697}. Best is trial#2 with value: 4.59294992685318.","title":"tuple, range, Range"},{"location":"tutorial/tuning/#list","text":"A list represents parameter choice. params = {'hidden_sizes.0': [10, 20, 30]} study = client.create_study('torch', params) _ = study.optimize(n_trials=5, epochs=1, verbose=0) [20] 2020-06-20 15:24:11 ( 2.54s ) python3 ( 40.7s ) [I 2020-06-20 15:24:11,709] A new study created with name: torch.hidden_sizes.0.study#6 [run#26] hidden_sizes.0=30 epochs=1 [I 2020-06-20 15:24:12,212] Finished trial#0 with value: 6.970041018724442 with parameters: {'hidden_sizes.0': 30}. Best is trial#0 with value: 6.970041018724442. [run#27] hidden_sizes.0=20 epochs=1 [I 2020-06-20 15:24:12,679] Finished trial#1 with value: 6.162596261501312 with parameters: {'hidden_sizes.0': 20}. Best is trial#1 with value: 6.162596261501312. [run#28] hidden_sizes.0=30 epochs=1 [I 2020-06-20 15:24:13,161] Finished trial#2 with value: 7.544045042991638 with parameters: {'hidden_sizes.0': 30}. Best is trial#1 with value: 6.162596261501312. [run#29] hidden_sizes.0=30 epochs=1 [I 2020-06-20 15:24:13,631] Finished trial#3 with value: 8.475840830802918 with parameters: {'hidden_sizes.0': 30}. Best is trial#1 with value: 6.162596261501312. [run#30] hidden_sizes.0=30 epochs=1 [I 2020-06-20 15:24:14,105] Finished trial#4 with value: 6.80062427520752 with parameters: {'hidden_sizes.0': 30}. Best is trial#1 with value: 6.162596261501312.","title":"list"},{"location":"tutorial/tuning/#product","text":"If a key and value are tuples, the entry means cartesian product of suggest functions like Task.product() . params = {('hidden_sizes', 'lr'): (hidden_sizes, Range(1e-4, 1e-3))} study = client.create_study('torch', params) optuna_study = study.optimize(n_trials=10, epochs=1, verbose=0) [21] 2020-06-20 15:24:14 ( 5.46s ) python3 ( 46.2s ) [I 2020-06-20 15:24:14,276] A new study created with name: torch.hidden_sizes.lr.study#7 [run#31] hidden_sizes:0=23 hidden_sizes:1=26 hidden_sizes:2=30 lr=0.000285 num_layers=3 epochs=1 [I 2020-06-20 15:24:14,829] Finished trial#0 with value: 73.63323173522949 with parameters: {'hidden_sizes:0': 23, 'hidden_sizes:1': 26, 'hidden_sizes:2': 30, 'lr': 0.00028500875002022773, 'num_layers': 3}. Best is trial#0 with value: 73.63323173522949. [run#32] hidden_sizes:0=15 hidden_sizes:1=23 lr=0.0003589 num_layers=2 epochs=1 [I 2020-06-20 15:24:15,349] Finished trial#1 with value: 8.391229891777039 with parameters: {'hidden_sizes:0': 15, 'hidden_sizes:1': 23, 'lr': 0.00035887166285874963, 'num_layers': 2}. Best is trial#1 with value: 8.391229891777039. [run#33] hidden_sizes:0=10 hidden_sizes:1=16 lr=0.0009096 num_layers=2 epochs=1 [I 2020-06-20 15:24:15,838] Finished trial#2 with value: 6.040180587768555 with parameters: {'hidden_sizes:0': 10, 'hidden_sizes:1': 16, 'lr': 0.0009095927923320399, 'num_layers': 2}. Best is trial#2 with value: 6.040180587768555. [run#34] hidden_sizes:0=16 hidden_sizes:1=27 lr=0.0003067 num_layers=2 epochs=1 [I 2020-06-20 15:24:16,357] Finished trial#3 with value: 9.55086771249771 with parameters: {'hidden_sizes:0': 16, 'hidden_sizes:1': 27, 'lr': 0.0003067305802012419, 'num_layers': 2}. Best is trial#2 with value: 6.040180587768555. [run#35] hidden_sizes:0=21 hidden_sizes:1=13 lr=0.0005063 num_layers=2 epochs=1 [I 2020-06-20 15:24:16,879] Finished trial#4 with value: 7.796209609508514 with parameters: {'hidden_sizes:0': 21, 'hidden_sizes:1': 13, 'lr': 0.0005063167017779712, 'num_layers': 2}. Best is trial#2 with value: 6.040180587768555. [run#36] hidden_sizes:0=13 hidden_sizes:1=14 hidden_sizes:2=11 lr=0.0007649 num_layers=3 epochs=1 [I 2020-06-20 15:24:17,384] Finished trial#5 with value: 6.721876096725464 with parameters: {'hidden_sizes:0': 13, 'hidden_sizes:1': 14, 'hidden_sizes:2': 11, 'lr': 0.0007648866552945914, 'num_layers': 3}. Best is trial#2 with value: 6.040180587768555. [run#37] hidden_sizes:0=12 hidden_sizes:1=28 lr=0.0003215 num_layers=2 epochs=1 [I 2020-06-20 15:24:17,921] Finished trial#6 with value: 10.44865968823433 with parameters: {'hidden_sizes:0': 12, 'hidden_sizes:1': 28, 'lr': 0.00032149466296796944, 'num_layers': 2}. Best is trial#2 with value: 6.040180587768555. [run#38] hidden_sizes:0=23 hidden_sizes:1=20 lr=0.0006336 num_layers=2 epochs=1 [I 2020-06-20 15:24:18,457] Finished trial#7 with value: 7.703984320163727 with parameters: {'hidden_sizes:0': 23, 'hidden_sizes:1': 20, 'lr': 0.0006336399565950085, 'num_layers': 2}. Best is trial#2 with value: 6.040180587768555. [run#39] hidden_sizes:0=17 hidden_sizes:1=27 hidden_sizes:2=22 lr=0.0006164 num_layers=3 epochs=1 [I 2020-06-20 15:24:19,014] Finished trial#8 with value: 9.060218453407288 with parameters: {'hidden_sizes:0': 17, 'hidden_sizes:1': 27, 'hidden_sizes:2': 22, 'lr': 0.0006163710231144914, 'num_layers': 3}. Best is trial#2 with value: 6.040180587768555. [run#40] hidden_sizes:0=15 hidden_sizes:1=26 lr=0.0001284 num_layers=2 epochs=1 [I 2020-06-20 15:24:19,563] Finished trial#9 with value: 59.055799674987796 with parameters: {'hidden_sizes:0': 15, 'hidden_sizes:1': 26, 'lr': 0.0001283989499444509, 'num_layers': 2}. Best is trial#2 with value: 6.040180587768555. Note You can mix suggest funtions and parametric optimization. Note You may feel that \" params = {'hidden_sizes.1': hidden_sizes, 'lr': Range(1e-4, 1e-3)} \" is better, but the above style is intentional. In parametric optimization, the name of Optuna's Study instance is dot-joint style : optuna_study.study_name [22] 2020-06-20 15:24:19 ( 4.00ms ) python3 ( 46.2s ) 'torch.hidden_sizes.lr.study#7'","title":"Product"},{"location":"tutorial/tuning/#study-from-yaml-file","text":"As a normal Run , a Study instance also can be created from a YAML file. Pass an extra keyword argument to the client.create_experiment() . The key is the instance name (in this case study ) and value is a YAML file name without its extension. experiment = client.create_experiment('torch', study='study') experiment [23] 2020-06-20 15:24:19 ( 11.0ms ) python3 ( 46.2s ) Experiment(id='1', name='torch', num_instances=1) Here is the contents of study.yml file. File 12 study.yml objective: lr: rectangle.suggest.suggest_lr hidden_sizes: def: rectangle.suggest.suggest_hidden_sizes max_num_layers: 3 min_size: __default__ max_size: __default__ Suggest functions should be callable, hidden_sizes uses a def key to create a callable. On the other hand, lr is just one line. If a suggest funtion can be called without additional arguments, you can omit the def key. Using this experiment, we can create Study instances with a suggest function. study_lr = client.create_study('torch', 'lr') study_lr.objective [24] 2020-06-20 15:24:19 ( 174ms ) python3 ( 46.4s ) Objective(['lr']) study_hs = client.create_study('torch', 'hidden_sizes') study_hs.objective [25] 2020-06-20 15:24:19 ( 178ms ) python3 ( 46.6s ) Objective(['hidden_sizes']) study_hs.objective.hidden_sizes [26] 2020-06-20 15:24:19 ( 3.00ms ) python3 ( 46.6s ) functools.partial(<function suggest_hidden_sizes at 0x000001420C0508B8>, max_num_layers=3, min_size=10, max_size=30) For min_size and max_size , default values are inspected from the signature. study_lr.optimize(n_trials=3, epochs=3, verbose=0) [27] 2020-06-20 15:24:19 ( 2.32s ) python3 ( 48.9s ) [I 2020-06-20 15:24:19,976] A new study created with name: torch.lr.study#8 [run#41] lr=8.41e-05 epochs=3 [I 2020-06-20 15:24:20,729] Finished trial#0 with value: 9.582427203655243 with parameters: {'lr': 8.409509940148645e-05}. Best is trial#0 with value: 9.582427203655243. [run#42] lr=3.765e-05 epochs=3 [I 2020-06-20 15:24:21,499] Finished trial#1 with value: 73.85470199584961 with parameters: {'lr': 3.7651669961268316e-05}. Best is trial#0 with value: 9.582427203655243. [run#43] lr=1.924e-05 epochs=3 [I 2020-06-20 15:24:22,268] Finished trial#2 with value: 93.58401222229004 with parameters: {'lr': 1.924316867807553e-05}. Best is trial#0 with value: 9.582427203655243. <optuna.study.Study at 0x14212124e48>","title":"Study from YAML file"},{"location":"tutorial/tuning/#pruning","text":"Optuna provides the pruning functionality . Ivory can uses this feature seamlessly. Here is updated contents of study.yml file. File 13 study.yml tuner: pruner: class: optuna.pruners.MedianPruner objective: lr: rectangle.suggest.suggest_lr hidden_sizes: def: rectangle.suggest.suggest_hidden_sizes max_num_layers: 3 min_size: __default__ max_size: __default__ The Tuner instance has Optuna's MedianPruner . (Off course, you can use other pruners .) A Study instance give an ivory.callbacks.Pruning instance to a run when the run is created, then with Ivory's callback system , the Pruning instance communicates with Optuna in order to determine the step of pruning. Note Pruning is supported for PyTorch and TensorFlow now.","title":"Pruning"},{"location":"tutorial/ui/","text":"Tracking UI Ivory uses MLFlow Tracking for the workflow tracking and model saving. For this feature, the Client instace has to have a Tracker instance. First create several runs for demonstration. import ivory client = ivory.create_client(\"examples\") run = client.create_run('torch') run.start('both') [3] 2020-06-20 15:24:22 ( 1.26s ) python3 ( 50.4s ) [I 200620 15:24:22 tracker:48] A new experiment created with name: 'torch' [epoch#0] loss=19.56 val_loss=7.696 lr=0.001 best [epoch#1] loss=7.47 val_loss=6.247 lr=0.001 best [epoch#2] loss=6.753 val_loss=6.538 lr=0.001 [epoch#3] loss=5.735 val_loss=4.631 lr=0.001 best [epoch#4] loss=4.799 val_loss=3.765 lr=0.001 best [epoch#5] loss=4.023 val_loss=2.892 lr=0.001 best [epoch#6] loss=2.992 val_loss=2.08 lr=0.001 best [epoch#7] loss=2.187 val_loss=1.411 lr=0.001 best [epoch#8] loss=1.551 val_loss=1.592 lr=0.001 [epoch#9] loss=1.024 val_loss=0.9122 lr=0.001 best task = client.create_task('torch') runs = task.product(fold=range(3), verbose=0) for run in runs: run.start('both') [4] 2020-06-20 15:24:23 ( 4.06s ) python3 ( 54.4s ) [run#1] fold=0 [run#2] fold=1 [run#3] fold=2 task = client.create_task('torch') runs = task.chain(lr=[1e-4, 1e-3], batch_size=[16, 32], verbose=0) for run in runs: run.start('both') [5] 2020-06-20 15:24:27 ( 5.18s ) python3 ( 59.6s ) [run#4] lr=0.0001 [run#5] lr=0.001 [run#6] batch_size=16 lr=0.001 [run#7] batch_size=32 lr=0.001 from ivory.utils.range import Range study = client.create_study('torch', lr=Range(1e-5, 1e-3, log=True)) study.optimize(n_trials=5, verbose=0) [6] 2020-06-20 15:24:33 ( 7.22s ) python3 ( 1min7s ) [I 2020-06-20 15:24:33,098] A new study created with name: torch.lr.study#0 [run#8] lr=0.0005617 [I 2020-06-20 15:24:34,544] Finished trial#0 with value: 4.446846199035645 with parameters: {'lr': 0.000561694309300103}. Best is trial#0 with value: 4.446846199035645. [run#9] lr=2.332e-05 [I 2020-06-20 15:24:35,950] Finished trial#1 with value: 14.365483665466309 with parameters: {'lr': 2.3319404904620393e-05}. Best is trial#0 with value: 4.446846199035645. [run#10] lr=0.0001296 [I 2020-06-20 15:24:37,379] Finished trial#2 with value: 7.584907138347626 with parameters: {'lr': 0.00012963450909320425}. Best is trial#0 with value: 4.446846199035645. [run#11] lr=4.874e-05 [I 2020-06-20 15:24:38,817] Finished trial#3 with value: 6.94333416223526 with parameters: {'lr': 4.874482477479321e-05}. Best is trial#0 with value: 4.446846199035645. [run#12] lr=0.0005352 [I 2020-06-20 15:24:40,234] Finished trial#4 with value: 4.568214583396911 with parameters: {'lr': 0.0005352429859687107}. Best is trial#0 with value: 4.446846199035645. <optuna.study.Study at 0x142121106c8> Tracking UI Optionally, you can update missing parameters: client.update_params('torch') [7] 2020-06-20 15:24:40 ( 357ms ) python3 ( 1min7s ) In a terminal, move to the working directory ( examples ), then run $ ivory ui You can view the UI using URL http://localhost:5000 in your browser. Table 1 A collection of runs. Parameters, metrics, tags are logged. You can compare the training results among runs. Figure 1 Comparison of training curves See also the official MLFlow documentation .","title":"Tracking UI"},{"location":"tutorial/ui/#tracking-ui","text":"Ivory uses MLFlow Tracking for the workflow tracking and model saving. For this feature, the Client instace has to have a Tracker instance. First create several runs for demonstration. import ivory client = ivory.create_client(\"examples\") run = client.create_run('torch') run.start('both') [3] 2020-06-20 15:24:22 ( 1.26s ) python3 ( 50.4s ) [I 200620 15:24:22 tracker:48] A new experiment created with name: 'torch' [epoch#0] loss=19.56 val_loss=7.696 lr=0.001 best [epoch#1] loss=7.47 val_loss=6.247 lr=0.001 best [epoch#2] loss=6.753 val_loss=6.538 lr=0.001 [epoch#3] loss=5.735 val_loss=4.631 lr=0.001 best [epoch#4] loss=4.799 val_loss=3.765 lr=0.001 best [epoch#5] loss=4.023 val_loss=2.892 lr=0.001 best [epoch#6] loss=2.992 val_loss=2.08 lr=0.001 best [epoch#7] loss=2.187 val_loss=1.411 lr=0.001 best [epoch#8] loss=1.551 val_loss=1.592 lr=0.001 [epoch#9] loss=1.024 val_loss=0.9122 lr=0.001 best task = client.create_task('torch') runs = task.product(fold=range(3), verbose=0) for run in runs: run.start('both') [4] 2020-06-20 15:24:23 ( 4.06s ) python3 ( 54.4s ) [run#1] fold=0 [run#2] fold=1 [run#3] fold=2 task = client.create_task('torch') runs = task.chain(lr=[1e-4, 1e-3], batch_size=[16, 32], verbose=0) for run in runs: run.start('both') [5] 2020-06-20 15:24:27 ( 5.18s ) python3 ( 59.6s ) [run#4] lr=0.0001 [run#5] lr=0.001 [run#6] batch_size=16 lr=0.001 [run#7] batch_size=32 lr=0.001 from ivory.utils.range import Range study = client.create_study('torch', lr=Range(1e-5, 1e-3, log=True)) study.optimize(n_trials=5, verbose=0) [6] 2020-06-20 15:24:33 ( 7.22s ) python3 ( 1min7s ) [I 2020-06-20 15:24:33,098] A new study created with name: torch.lr.study#0 [run#8] lr=0.0005617 [I 2020-06-20 15:24:34,544] Finished trial#0 with value: 4.446846199035645 with parameters: {'lr': 0.000561694309300103}. Best is trial#0 with value: 4.446846199035645. [run#9] lr=2.332e-05 [I 2020-06-20 15:24:35,950] Finished trial#1 with value: 14.365483665466309 with parameters: {'lr': 2.3319404904620393e-05}. Best is trial#0 with value: 4.446846199035645. [run#10] lr=0.0001296 [I 2020-06-20 15:24:37,379] Finished trial#2 with value: 7.584907138347626 with parameters: {'lr': 0.00012963450909320425}. Best is trial#0 with value: 4.446846199035645. [run#11] lr=4.874e-05 [I 2020-06-20 15:24:38,817] Finished trial#3 with value: 6.94333416223526 with parameters: {'lr': 4.874482477479321e-05}. Best is trial#0 with value: 4.446846199035645. [run#12] lr=0.0005352 [I 2020-06-20 15:24:40,234] Finished trial#4 with value: 4.568214583396911 with parameters: {'lr': 0.0005352429859687107}. Best is trial#0 with value: 4.446846199035645. <optuna.study.Study at 0x142121106c8>","title":"Tracking UI"},{"location":"tutorial/ui/#tracking-ui_1","text":"Optionally, you can update missing parameters: client.update_params('torch') [7] 2020-06-20 15:24:40 ( 357ms ) python3 ( 1min7s ) In a terminal, move to the working directory ( examples ), then run $ ivory ui You can view the UI using URL http://localhost:5000 in your browser. Table 1 A collection of runs. Parameters, metrics, tags are logged. You can compare the training results among runs. Figure 1 Comparison of training curves See also the official MLFlow documentation .","title":"Tracking UI"}]}